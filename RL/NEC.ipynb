{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class HashDND(object):\n",
    "    \"\"\"differentiable neural dictionary, using LSH for approximate\n",
    "    nearest neighbour lookup. Assumes keys are vectors. Also assumes we only\n",
    "    use float32 and doesn't handle batched operations :(\"\"\"\n",
    "\n",
    "    sentinel_value = np.inf\n",
    "\n",
    "    @classmethod\n",
    "    def _setup_variables(cls, hash_bits, max_neighbours, key_size,\n",
    "                         value_shapes):\n",
    "        \"\"\"setup variables with appropriate initializers given the shapes\"\"\"\n",
    "        init = tf.constant_initializer(cls.sentinel_value)\n",
    "\n",
    "        keys = tf.get_variable(name='keys',\n",
    "                               shape=[2**hash_bits * max_neighbours, key_size],\n",
    "                               initializer=init)\n",
    "\n",
    "        values = []\n",
    "        for i, shape in enumerate(value_shapes):\n",
    "            var_shape = [2**hash_bits * max_neighbours] + shape\n",
    "            var = tf.get_variable(name='value_{}'.format(i),\n",
    "                                  shape=var_shape,\n",
    "                                  initializer=init)\n",
    "            values.append(var)\n",
    "        return keys, values\n",
    "\n",
    "    def __init__(self, hash_bits, max_neighbours, key_size, value_shapes,\n",
    "                 similarity_measure=None, name='dnd'):\n",
    "        \"\"\"Set up the dnd.\n",
    "\n",
    "        Args:\n",
    "            hash_bits (int): how many bits for the hash. There will be\n",
    "                `2**num_bits` individual buckets.\n",
    "            max_neighbours (int): how many entries to store in each bucket.\n",
    "                This controls the number of neighbours we check against.\n",
    "                Operations will be linear in this value and it will likely\n",
    "                effect learning performance significantly as well.\n",
    "            key_size (int): size of the key vectors. We use the unhashed key\n",
    "                vectors to compute similarities between keys we find from the\n",
    "                nearest neighbour lookup.\n",
    "            value_shapes (list): list of shapes for the values stored in the\n",
    "                dictionary.\n",
    "            similarity_measure (Optional[callable]): function which adds ops\n",
    "                to compare a query key with all of the other keys in the\n",
    "                bucket. If unspecified, the cosine similarity is used. Should\n",
    "                be a callable which takes two input tensors: the query key\n",
    "                (shaped `[key_size]`) and a  `[max_neighbours, key_size]`\n",
    "                tensor  of keys to compare against. Should return a\n",
    "                `[max_neighbours]` tensor of similarities, between 0 and 1\n",
    "                where 1 means the two keys were identical.\n",
    "            name (Optional[str]): a name under which to group ops and\n",
    "                variables. Defaults to `dnd`.\n",
    "        \"\"\"\n",
    "        self._name = name\n",
    "        self._hash_size = hash_bits\n",
    "        self._key_size = key_size\n",
    "        self._bucket_size = max_neighbours\n",
    "        with tf.variable_scope(self._name):\n",
    "            self._keys, self._values = HashDND._setup_variables(hash_bits,\n",
    "                                                                max_neighbours,\n",
    "                                                                key_size,\n",
    "                                                                value_shapes)\n",
    "        self._hash_config = get_simhash_config(self._key_size,\n",
    "                                               self._hash_size)\n",
    "\n",
    "        if not similarity_measure:\n",
    "            similarity_measure = cosine_similarity\n",
    "        self._similarity_measure = similarity_measure\n",
    "\n",
    "    def _get_bucket(self, key):\n",
    "        \"\"\"look up the contents of a bucket by hash. Also return the bucket\n",
    "        index so we can create updates to the storage variables.\"\"\"\n",
    "        idx = simhash(key, self._hash_config)\n",
    "        bucket_start = idx * self._bucket_size\n",
    "        bucket_end = (idx + 1) * self._bucket_size\n",
    "        keys = self._keys[bucket_start:bucket_end, ...]\n",
    "        values = [val[bucket_start:bucket_end, ...] for val in self._values]\n",
    "        return keys, values, idx\n",
    "\n",
    "    def store(self, key, value):\n",
    "        \"\"\"Gets an op which will store the key-value pair. This involves the\n",
    "        following process:\n",
    "            - compute hash of `key`\n",
    "            - lookup all keys and values with matching hash\n",
    "            - if the bucket isn't full\n",
    "                - assign the values to the values of the next empty position\n",
    "            - else (if the bucket is full)\n",
    "                - update, according to some update rule which may be\n",
    "                  application specific.\n",
    "                - TODO: figure this out (LRU?)\n",
    "\n",
    "        Args:\n",
    "            key (tensor): `[key_size]` key to store\n",
    "            value (list of tensors): `[???]` tensors to\n",
    "                be stored.\n",
    "\n",
    "        Returns:\n",
    "            op: an op which carries out the above steps.\n",
    "        \"\"\"\n",
    "        with tf.name_scope(self._name + '/store'):\n",
    "            bucket_keys, bucket_values, idx = self._get_bucket(key)\n",
    "            # is there space?\n",
    "            can_store = tf.reduce_any(tf.equal(bucket_keys[:, 0],\n",
    "                                               self.sentinel_value))\n",
    "\n",
    "            def _empty_store():\n",
    "                return self._get_store_op_empty(key, value, idx, bucket_keys)\n",
    "\n",
    "            def _full_store():\n",
    "                return self._get_store_op_full(key, value, idx, bucket_keys)\n",
    "\n",
    "            store_op = tf.cond(can_store, _empty_store, _full_store)\n",
    "        return store_op\n",
    "\n",
    "    def _flatten_index(self, index, bucket_index):\n",
    "        \"\"\"turn a bucket-level index into a global index\"\"\"\n",
    "        return index + (bucket_index * self._bucket_size)\n",
    "\n",
    "    def _update_at_index(self, index, new_key, new_vals):\n",
    "        \"\"\"make update ops to insert at the appropriate (flattened) index\"\"\"\n",
    "        # update the keys\n",
    "        key_update = tf.scatter_update(self._keys, [index],\n",
    "                                       tf.expand_dims(new_key, 0))\n",
    "        # and update the values\n",
    "        value_updates = []\n",
    "        for value_var, new_val in zip(self._values, new_vals):\n",
    "            val_update = tf.scatter_update(value_var, [index],\n",
    "                                           tf.expand_dims(new_val, 0))\n",
    "            value_updates.append(val_update)\n",
    "\n",
    "        # make sure they all happen at once\n",
    "        return tf.group(key_update, *value_updates)\n",
    "\n",
    "    def _get_store_op_empty(self, store_key, store_vals, bucket_index,\n",
    "                            bucket_keys):\n",
    "        \"\"\"get an op to store given key and values in the first empty space.\n",
    "\n",
    "        Returns an op with no output that will run all of the required updates.\n",
    "        \"\"\"\n",
    "        # first find the first empty spot (assuming there is one)\n",
    "        with tf.name_scope('empty_store'):\n",
    "            empty_indices = tf.where(tf.equal(bucket_keys[:, 0],\n",
    "                                              self.sentinel_value))\n",
    "            empty_indices = tf.cast(empty_indices, tf.int32)\n",
    "            store_idx = self._flatten_index(empty_indices[0, 0], bucket_index)\n",
    "            return self._update_at_index(store_idx, store_key, store_vals)\n",
    "\n",
    "    def _get_store_op_full(self, store_key, store_vals, bucket_index,\n",
    "                           bucket_keys):\n",
    "        \"\"\"get an op to store given keys and values when there are no empty\n",
    "        slots.\n",
    "\n",
    "        Returns an op with no output that will run all of the require updates.\n",
    "        \"\"\"\n",
    "        # TODO: what should this do? LRU? Need some accounting for that,\n",
    "        # otherwise some kind of interpolation for lossily storing it in the\n",
    "        # bucket?\n",
    "        # for now we are just going to choose at random, which is surely a\n",
    "        # terrible strategy, but at least it will run\n",
    "        with tf.name_scope('store_full'):\n",
    "            idx = tf.random_uniform([], minval=0, maxval=self._bucket_size,\n",
    "                                    dtype=tf.int32)\n",
    "            store_idx = self._flatten_index(idx, bucket_index)\n",
    "            return self._update_at_index(store_idx, store_key, store_vals)\n",
    "\n",
    "    def _get_averaged_value(self, values, similarities):\n",
    "        \"\"\"get a weighted sum of values.\"\"\"\n",
    "        weighted_values = tf.expand_dims(similarities, 1) * values\n",
    "        all_values = tf.reduce_sum(weighted_values, axis=0)\n",
    "        return all_values\n",
    "\n",
    "    def get(self, key):\n",
    "        \"\"\"Get the values in the dictionary corresponding to a particular key,\n",
    "        or zeros if the key is not present.\n",
    "\n",
    "        The process is as follows:\n",
    "            - compute hash of `key`\n",
    "            - lookup all keys and values with matching hash\n",
    "            - compute similarities between all matching keys and `key`\n",
    "            - return average of all matching values, weighted by similarities.\n",
    "\n",
    "        The default similarity is the cosine distance.\n",
    "\n",
    "        Args:\n",
    "            key (tensor): `[key_size]` batch of keys to look up.\n",
    "\n",
    "        Returns:\n",
    "            value (tuple): associated values.\n",
    "        \"\"\"\n",
    "        # TODO: what to return when the bucket is empty?\n",
    "        # at the moment it is all zeros\n",
    "        with tf.name_scope(self._name + '/get'):\n",
    "            bucket_keys, bucket_values, _ = self._get_bucket(key)\n",
    "            # compute similarities\n",
    "            similarities = self._similarity_measure(key, bucket_keys)\n",
    "            # where the keys are sentinel, mask it out\n",
    "            used_positions = tf.not_equal(bucket_keys[:, 0],\n",
    "                                          self.sentinel_value)\n",
    "            values = [tf.boolean_mask(val, used_positions)\n",
    "                      for val in bucket_values]\n",
    "            similarities = tf.boolean_mask(similarities, used_positions)\n",
    "            # normalise them to sum to one, and maybe give them a kick\n",
    "            similarities /= tf.reduce_sum(similarities)\n",
    "\n",
    "            results = tuple(self._get_averaged_value(val, similarities)\n",
    "                            for val in values)\n",
    "        return results\n",
    "    \n",
    "    \n",
    "def cosine_similarity(query, bucket):\n",
    "    \"\"\"Cosine similarity: the cosine of the angle between two vectors.\n",
    "    Also the dot product, if the vectors are normalised in the l2 norm,\n",
    "    which is how it is implemented here.\"\"\"\n",
    "    query = tf.expand_dims(query, 1)\n",
    "    query = tf.nn.l2_normalize(query, dim=0)\n",
    "    bucket = tf.nn.l2_normalize(bucket, dim=1)\n",
    "    return tf.squeeze(tf.matmul(bucket, query), 1, name='cos_sim')\n",
    "\n",
    "\n",
    "def get_simhash_config(input_size, hash_bits):\n",
    "    \"\"\"Gets any necessary configuration and data structures necessary for\n",
    "    consistent hashing.\n",
    "\n",
    "    For simhash, this just corresponds to the random matrix used to project the\n",
    "    input down, but we also store some values used in the conversion from\n",
    "    binary to integers.\n",
    "\n",
    "    This function should be run once and the result stored and passed in to all\n",
    "    subsequent calls to `simhash`, so that we use the same matrix every time.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): size of the inputs we are going to hash.\n",
    "        hash_bits (int): the number of bits we output.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with two keys: \"matrix\" corresponding to a variable\n",
    "            used for the random projection and \"bases\" used in the conversion\n",
    "            to integers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('simhash_config'):\n",
    "        mat = tf.get_variable(\n",
    "            'projection_matrix',\n",
    "            shape=[input_size, hash_bits],\n",
    "            initializer=tf.random_normal_initializer())\n",
    "        bases = tf.expand_dims(2 ** tf.range(hash_bits), 0)\n",
    "        return {'matrix': mat,\n",
    "                'bases': bases}\n",
    "\n",
    "\n",
    "def simhash(inputs, config):\n",
    "    \"\"\"SimHash the inputs into an integer with `num_bits` used bits.\n",
    "    The process is:\n",
    "        - flatten inputs into `[batch_size, ?]`\n",
    "        - multiply by a (fixed) random gaussian matrix.\n",
    "        - convert to bits with the sign function (and appropriate shifting and\n",
    "          scaling)\n",
    "        - convert from a `[batch_size, num_bits]` matrix to a `[batch_size, 1]`\n",
    "          vector of integers.\n",
    "\n",
    "    The standard process requires sign(0) = 1, as usual. However, most maths\n",
    "    libraries define sign(0) = 0. At this stage we ignore it because it is\n",
    "    highly unlikely, but if the inputs are sparse it's a possibility and should\n",
    "    probably be adressed (it's not going to break anything, but it will lead to\n",
    "    some potentially unexpected hashes which _might_ break the locality\n",
    "    sensitive property).\n",
    "\n",
    "    If inputs is a vector, then we will return a scalar, otherwise batches of\n",
    "    inputs will produce batches of outputs.\n",
    "\n",
    "    Args:\n",
    "        inputs (tensor): tensor of whatever shape, with the batch on the first\n",
    "            axis. Apart from the batch size, the shape does need to be defined.\n",
    "        config (dict): the result of `get_simhash_config`.\n",
    "\n",
    "    Returns:\n",
    "        tensor: `[batch_size, 1]` integer tensor.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('simhash'):\n",
    "        if len(inputs.get_shape()) == 1:\n",
    "            inputs = tf.expand_dims(inputs, 0)\n",
    "            squeeze_output = True\n",
    "        else:\n",
    "            squeeze_output = False\n",
    "        projected = tf.matmul(inputs, config['matrix'])\n",
    "        bits = tf.sign(projected) * 0.5 + 0.5\n",
    "        # return bits\n",
    "        bits = tf.cast(bits, tf.int32)\n",
    "        # convert to single int\n",
    "        index = tf.reduce_sum(bits * config['bases'],\n",
    "                              axis=1, keep_dims=True)\n",
    "        if squeeze_output:\n",
    "            index = tf.squeeze(index, [0])\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseDifferentiableMemory(object):\n",
    "    def __init__(self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from logging import getLogger\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from rltensor.agents.agent import Agent\n",
    "from rltensor.utils import get_shape\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class NEC(Agent):\n",
    "    def __init__(self, env, conf, controller_cls, memory_cls, default_conf=None, sess=None):\n",
    "        self.controller_cls = controller_cls\n",
    "        self.memory_cls = memory_cls\n",
    "        self.key_dim = conf[\"key_dim\"]\n",
    "        self.delay = conf[\"delay\"]\n",
    "        self.recent_rewards = deque(maxlen=self.delay)\n",
    "        self.recent_terminals = deque(maxlen=self.delay)\n",
    "        super(NEC, self).__init__(env, conf, default_conf, sess)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        # state shape has to be (batch, length,) + input_dim\n",
    "        self.state = tf.placeholder(tf.float32,\n",
    "                                     get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                     name='state')\n",
    "        _state = self.processor.tensor_process(self.state)\n",
    "        # Employ maximal strategy\n",
    "        self.controller = self.controller_cls(self.key_dim, self.conf[\"controller\"], scope_name=\"controller\")\n",
    "        self.diff_memory = self.diff_memory_cls(self.action_dim, self.conf[\"diff_memory\"])\n",
    "        query = self.controller(_state, self.training)\n",
    "        self.q_val = self.diff_memory.get_q(query)\n",
    "        self.max_action = tf.argmax(self.q_val, dimension=1)\n",
    "        self.max_q_val = tf.reduce_max(self.q_val, dimension=1)\n",
    "        # Build action graph\n",
    "        self.action = tf.placeholder(tf.int32, (None,), name='action')\n",
    "        action_one_hot = tf.one_hot(self.action, depth=self.action_dim)\n",
    "        self.action_q_val = tf.reduce_sum(self.q_val * action_one_hot, axis=1)\n",
    "        # Build target\n",
    "        self.target = tf.placeholder(tf.float32, (None,), name=\"target\")\n",
    "        self.terminal = tf.placeholder(tf.bool, (None,), name=\"terminal\")\n",
    "        # Clip error to stabilize learning\n",
    "        self.error = self.target - self.action_q_val\n",
    "        clipped_error = tf.where(tf.abs(self.error) < self.error_clip,\n",
    "                                    0.5 * tf.square(self.error),\n",
    "                                    tf.abs(self.error), name='clipped_error')\n",
    "        self.weights = tf.placeholder(tf.float32, (None,), name=\"importance_weights\")\n",
    "        self.loss = tf.reduce_mean(self.weights * clipped_error, name='loss')\n",
    "        # Build optimization\n",
    "        self.update_op = self._get_update_op()\n",
    "        self.optimizer = self._get_optimizer(self.optimizer_name, self.learning_rate_op, self.optimizer_conf)\n",
    "        grads_vars = self.optimizer.compute_gradients(self.loss)\n",
    "        if \"grad_clip\" in self.conf and self.conf[\"grad_clip\"] is not None:\n",
    "            grads_vars = [\n",
    "                (tf.clip_by_norm(gv[0], clip_norm=self.conf[\"grad_clip\"]), gv[1]) \n",
    "                    for gv in grads_vars]\n",
    "        self.q_optim = self.optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    def observe(self, observation, action, reward, terminal, training):\n",
    "        # clip reward into  (min_r, max_r)\n",
    "        reward = max(self.min_r, min(self.max_r, reward))\n",
    "        assert len(self.memory.observations) == len(self.recent_rewards)\n",
    "        # We always keep data\n",
    "        self.recent_observations.append(observation)\n",
    "        self.recent_rewards.append(reward)\n",
    "        self.recent_terminals.append(terminal)\n",
    "        target_val = self._calc_target(observation)\n",
    "        # we keep target value instead of reward directly\n",
    "        self.memory.append(observation, action, target_val, terminal, is_store=True)\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        if (step + 1) % self.t_train_freq:\n",
    "            is_update = True\n",
    "        else:\n",
    "            is_update = False\n",
    "        if training:\n",
    "            self.memory.add_weights()\n",
    "            weights = self.memory.get_weights()\n",
    "            experiences = self.memory.sample(self.batch_size, weights)\n",
    "            weights = self.memory.get_importance_weights()\n",
    "            if weights is None:\n",
    "                weights = np.ones(self.batch_size)\n",
    "            result = self.q_learning_minibatch(experiences, weights, is_update)\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def q_learning_minibatch(self, experiences, batch_weights, is_update=True):\n",
    "        feed_dict = {\n",
    "            self.state: [experience.state for experience in experiences],\n",
    "            self.target: [experience.q_val for experience in experiences],\n",
    "            self.action: [experience.action for experience in experiences],\n",
    "            self.weights: batch_weights,\n",
    "            self.training: True,\n",
    "        }\n",
    "        if is_update:\n",
    "            self.sess.run(self.q_optim, feed_dict=feed_dict);\n",
    "        q_t, loss, error = self.sess.run([self.action_q_val, self.loss, self.error],\n",
    "                                     feed_dict=feed_dict)\n",
    "        return q_t, loss, error, is_update\n",
    "    \n",
    "    def predict(self, state, ep=None):\n",
    "        if ep is None:\n",
    "            ep = self.epsilon.eval(session=self.sess)\n",
    "        if random.random() < ep:\n",
    "            action = np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            action = self.sess.run(self.max_action, \n",
    "                                   feed_dict={self.state: [state],\n",
    "                                              self.training: False})[0]\n",
    "        return action\n",
    "    \n",
    "    def _calc_target(self, observation):\n",
    "        target_val = 0\n",
    "        backward = self.delay - 1\n",
    "        for i in range(len(self.recent_rewards)):\n",
    "            target_val += (self.gamma) ** i * self.recent_rewards[i]\n",
    "            backward -= 1\n",
    "            if recent_terminals[i]:\n",
    "                break\n",
    "        state = self.memory.get_delay_state(observation, backward)\n",
    "        feed_dict = {\n",
    "            self.state: [state],\n",
    "            self.training: False}\n",
    "        max_q_val = self.sess.run(self.max_q_val, feed_dict=feed_dict)[0]\n",
    "        target_val += self.gamma**(self.delay - backward) * max_q_val\n",
    "        return target_val\n",
    "    \n",
    "    def update_target_q_network(self):\n",
    "        # We have no operations for updating target network\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from rltensor.memories import SequentialMemory\n",
    "\n",
    "QExperience = namedtuple('QExperience', 'state, action, q_val')\n",
    "\n",
    "class QMemory(SequentialMemory):\n",
    "    def __init__(self, delay, window_length, limit, *args, **kwargs):\n",
    "        self.delay = delay\n",
    "        # Take more observations to make state\n",
    "        self.dleay_observations = deque(maxlen=self.delay+window_length)\n",
    "        self.delay_actions = deque(maxlen=self.delay)\n",
    "        self.delay_terminals = deque(maxlen=self.delay)\n",
    "        super(DelayMemory, self).__init__(window_length, limit, *args, **kwargs)\n",
    "        \n",
    "    def sample(self, batch_size, weights=None, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            if weights is None:\n",
    "                weights = None\n",
    "            # Draw random indexes such that we have at least a single entry before each\n",
    "            # index. Thus, draw samples from [1, self.nb_entries)\n",
    "            batch_idxs = self._sample_batch_indexes(0, self.nb_entries, batch_size, weights)\n",
    "        assert np.min(batch_idxs) >= 0\n",
    "        assert np.max(batch_idxs) < self.nb_entries\n",
    "        assert len(batch_idxs) == batch_size\n",
    "\n",
    "        # Create experiences\n",
    "        experiences = []\n",
    "        # Each idx is index for state1\n",
    "        for i, idx in enumerate(batch_idxs):\n",
    "            state = [self.observations[idx],]\n",
    "            for offset in xrange(1, self.window_length):\n",
    "                current_idx = idx - offset\n",
    "                current_terminal = self.terminals[current_idx] if current_idx >= 0 else False\n",
    "                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n",
    "                    # The previously handled observation was terminal, don't add the current one.\n",
    "                    # Otherwise we would leak into a different episode.\n",
    "                    break\n",
    "                state.insert(0, self.observations[current_idx])\n",
    "            # Complete unobserved state with 0\n",
    "            while len(state) < self.window_length:\n",
    "                state.insert(0, np.zeros_like(state[0]))\n",
    "            action = self.actions[idx]\n",
    "            q_val = self.rewards[idx]\n",
    "            assert len(state0) == self.window_length\n",
    "            experiences.append(QExperience(state=state, action=action, q_val=q_val))\n",
    "        assert len(experiences) == batch_size\n",
    "        # Keep sampled sampled idx for prioritized sampling\n",
    "        self.sampled_idx = batch_idxs\n",
    "        return experiences\n",
    "    \n",
    "    def append(self, observation, action, reward, terminal, is_store=True):\n",
    "        # Reward means Q value just for keeping compatibility\n",
    "        super(QMemory).append(observation, action, reward, terminal)\n",
    "        self.delay_observations.append(observation)\n",
    "        self.delay_actions.append(action)\n",
    "        self.delay_terminals.append(terminal)\n",
    "        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n",
    "        # and weather the next state is `terminal` or not.\n",
    "        if is_store:\n",
    "            self.observations.append(self.delay_observations[0])\n",
    "            self.actions.append(self.delay_actions[0])\n",
    "            self.rewards.append(reward)\n",
    "            self.terminals.append(self.delay_terminals[0])\n",
    "            \n",
    "    \n",
    "    def get_delay_state(self, observation=None, backward=0):\n",
    "        _observations = deepcopy(self.delay_observations)\n",
    "        if observation is not None:\n",
    "            _observations.append(observation)\n",
    "        if backward > 1:\n",
    "            _observation = _observation[:-backward]\n",
    "        while len(_observations) < self.window_length:\n",
    "            _observations.insert(0, np.zeros_like(self.recent_observations[0]))\n",
    "        # Make sure window length observations\n",
    "        return np.array(_observations)[-self.window_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    i+= 1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-11 02:42:15,224] Making new env: Breakout-v0\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [512] and type float\n\t [[Node: q_network/feature_network/layer_3/fully_connected/biases/RMSProp/Initializer/ones = Const[_class=[\"loc:@q_network/feature_network/layer_3/fully_connected/biases\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [512] values: 1 1 1...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'q_network/feature_network/layer_3/fully_connected/biases/RMSProp/Initializer/ones', defined at:\n  File \"/home/tomoaki/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-8ac51d688646>\", line 61, in <module>\n    dqn = DQN(env, conf, q_network_cls=DuelingModel)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/agents/deep_q.py\", line 21, in __init__\n    super(DQN, self).__init__(env, conf, default_conf, sess)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/agents/agent.py\", line 69, in __init__\n    self._build_graph()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/agents/deep_q.py\", line 78, in _build_graph\n    self.q_optim = self.optimizer.apply_gradients(grads_vars)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py\", line 100, in _create_slots\n    v.dtype, \"rms\", self._name)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 749, in _get_or_make_slot_with_initializer\n    var, initializer, shape, dtype, op_name)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 146, in create_slot_with_initializer\n    dtype)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 66, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 200, in __init__\n    expected_shape=expected_shape)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 278, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 701, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 108, in __call__\n    return array_ops.ones(shape, dtype)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1492, in ones\n    output = constant(one, shape=shape, dtype=dtype, name=name)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 106, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [512] and type float\n\t [[Node: q_network/feature_network/layer_3/fully_connected/biases/RMSProp/Initializer/ones = Const[_class=[\"loc:@q_network/feature_network/layer_3/fully_connected/biases\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [512] values: 1 1 1...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [512] and type float\n\t [[Node: q_network/feature_network/layer_3/fully_connected/biases/RMSProp/Initializer/ones = Const[_class=[\"loc:@q_network/feature_network/layer_3/fully_connected/biases\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [512] values: 1 1 1...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8ac51d688646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_network_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDuelingModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_video_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./videos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/agents/agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, t_max, num_max_start_steps, save_file_path, load_file_path, save_video_path, overwrite, render_freq)\u001b[0m\n\u001b[1;32m     87\u001b[0m     def train(self, t_max, num_max_start_steps=0, save_file_path=None, \n\u001b[1;32m     88\u001b[0m               load_file_path=None, save_video_path=None, overwrite=True, render_freq=None):\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_file_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \"\"\"\n\u001b[0;32m-> 1706\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3961\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3962\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3963\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [512] and type float\n\t [[Node: q_network/feature_network/layer_3/fully_connected/biases/RMSProp/Initializer/ones = Const[_class=[\"loc:@q_network/feature_network/layer_3/fully_connected/biases\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [512] values: 1 1 1...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'q_network/feature_network/layer_3/fully_connected/biases/RMSProp/Initializer/ones', defined at:\n  File \"/home/tomoaki/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-8ac51d688646>\", line 61, in <module>\n    dqn = DQN(env, conf, q_network_cls=DuelingModel)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/agents/deep_q.py\", line 21, in __init__\n    super(DQN, self).__init__(env, conf, default_conf, sess)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/agents/agent.py\", line 69, in __init__\n    self._build_graph()\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/agents/deep_q.py\", line 78, in _build_graph\n    self.q_optim = self.optimizer.apply_gradients(grads_vars)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 446, in apply_gradients\n    self._create_slots([_get_variable_for(v) for v in var_list])\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py\", line 100, in _create_slots\n    v.dtype, \"rms\", self._name)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 749, in _get_or_make_slot_with_initializer\n    var, initializer, shape, dtype, op_name)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 146, in create_slot_with_initializer\n    dtype)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 66, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 200, in __init__\n    expected_shape=expected_shape)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 278, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 701, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 108, in __call__\n    return array_ops.ones(shape, dtype)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1492, in ones\n    output = constant(one, shape=shape, dtype=dtype, name=name)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 106, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [512] and type float\n\t [[Node: q_network/feature_network/layer_3/fully_connected/biases/RMSProp/Initializer/ones = Const[_class=[\"loc:@q_network/feature_network/layer_3/fully_connected/biases\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [512] values: 1 1 1...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from rltensor.agents import DQN\n",
    "from rltensor.processors import AtariProcessor\n",
    "from rltensor.networks import DuelingModel\n",
    "\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":True, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        'double_q': True,\n",
    "        \"memory_limit\": 100000,\n",
    "        \"window_length\": 4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"learning_rate_minimum\": 2.5e-4,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 100,\n",
    "        \"ep\": 1e-3,\n",
    "        \"min_r\": -np.inf,\n",
    "        \"max_r\": np.inf,\n",
    "        \"batch_size\": 32,\n",
    "        \"error_clip\": 1.0,\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "        \"t_learn_start\": 100,\n",
    "        \"t_train_freq\": 4,\n",
    "        \"t_target_q_update_freq\": 10000,\n",
    "        \"ep_start\": 1.0,\n",
    "        \"ep_end\": 0.1,\n",
    "        \"t_ep_end\": int(1e6),\n",
    "        \"model_dir\": \"./dqn_logs\",\n",
    "        \"log_freq\": 1000,\n",
    "        \"avg_length\": 10000,\n",
    "        \"env_name\": 'DemonAttack-v0',\n",
    "        \"prioritized\": True,\n",
    "}\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":False, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        \"env_name\": 'Breakout-v0',\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "}\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "with tf.device('/gpu:1'):\n",
    "    tf.reset_default_graph()\n",
    "    dqn = DQN(env, conf, q_network_cls=DuelingModel)\n",
    "    dqn.train(int(1e7), render_freq=None, save_video_path=\"./videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:02,794] Making new env: Breakout-v0\n",
      "[2017-07-30 21:57:03,411] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/tomoaki/work/Development/RL/breakout_videos')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./breakout_dqn_params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:03,497] Restoring parameters from ./breakout_dqn_params/model.ckpt\n",
      "[2017-07-30 21:57:03,531] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-07-30 21:57:03,538] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:35,712] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000001.mp4\n",
      "\n",
      "[2017-07-30 22:01:21,034] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000008.mp4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from rltensor.agents import DQN\n",
    "from rltensor.processors import AtariProcessor\n",
    "from rltensor.networks import DuelingModel\n",
    "\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":False, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        \"env_name\": 'Breakout-v0',\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "}\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(env, conf, q_network_cls=DuelingModel)\n",
    "dqn.play(num_episode=10, ep=0.05, load_file_path=\"./breakout_dqn_params/model.ckpt\",\n",
    "         save_video_path=\"./breakout_videos\", render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if [1, 2, 3]:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 4\n",
    "while count < 5:\n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"%s\" % True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.insert(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = deque([1, 2, 3], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.append(3)\n",
    "x.append(3)\n",
    "x.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f829244df969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_false\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'select'"
     ]
    }
   ],
   "source": [
    " result = tf.select(pred, val_if_true, val_if_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.bool, (None,))\n",
    "y = tf.cast(x, tf.int32)\n",
    "z = tf.one_hot(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "print(y.eval(feed_dict={x:[True, False, True]}))\n",
    "print(z.eval(feed_dict={x:[True, False, True]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.arange(10).astype(int)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.random.choice(range(0, 5), 3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1, 2, 3, 4], 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "np.append(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
