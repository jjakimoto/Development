{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rltensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'rbg2gray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-77b61d6f0e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrbg2gray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'rbg2gray'"
     ]
    }
   ],
   "source": [
    "from skimage.color import rbg2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_data(data, width, height, c_dim=3, is_color=True):\n",
    "    \"\"\"resize data for trainining dcgan\n",
    "    Args:\n",
    "        data: list of image data, each of which has a shape,\n",
    "            (width, height, color_dim) if is_color==True\n",
    "            (width, height) otherwisei\n",
    "    \"\"\"\n",
    "    if is_color:\n",
    "        converted_data = np.array([imresize(d, [width, height]) for d in data\n",
    "                                if (len(d.shape)==3 and d.shape[-1] == c_dim)])\n",
    "    else:\n",
    "        # gray scale data\n",
    "        converted_data = np.array([imresize(d, [width, height]) for d in data\n",
    "                                if (len(d.shape)==2)])\n",
    "    return converted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import join, isfile, isdir\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "class DefaultProcessor(object):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        return np.array(x)\n",
    "    \n",
    "    def tensor_process(self, x):\n",
    "        return x\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return self.input_shape\n",
    "    \n",
    "class AtariProcessor(DefaultProcessor):\n",
    "    def __init__(self, height, width):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.input_shape = (height, width)\n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        x = resize_data([x], self.height, self.width)[0]\n",
    "        x = rgb2gray(x)\n",
    "        return x\n",
    "    \n",
    "    def tensor_process(self, x):\n",
    "        # change to (batch, width, hight, window_length)\n",
    "        return tf.transpose(x, [0, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "f = Image.open(\"/home/tomoaki/work/Development/web_scraping/idle_images/cont_img20130_464.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9949701960784314"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = AtariProcessor(84, 84)\n",
    "\n",
    "np.max(processor.preprocess(np.array(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.get_input_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from logging import getLogger\n",
    "import time\n",
    "from six.moves import xrange\n",
    "\n",
    "from rltensor.memories import SequentialMemory\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, env, conf, sess=None):\n",
    "        if sess is None:\n",
    "            sess = tf.Session()\n",
    "        self.sess = sess\n",
    "        self.conf = conf\n",
    "        self.limit = conf[\"memory_limit\"]\n",
    "        self.window_length = conf[\"window_length\"]\n",
    "        self.memory = self._get_memory(self.window_length, self.limit)\n",
    "        self.gamma = conf[\"gamma\"]\n",
    "        self.error_clip = conf[\"error_clip\"]\n",
    "        self.processor = conf[\"processor\"]\n",
    "        # Get input and action dim info from env\n",
    "        self.env = env\n",
    "        self.state_dim = processor.get_input_shape()\n",
    "        self.action_dim = env.action_space.n\n",
    "        # configure for learning schedule \n",
    "        self.learning_rate = conf[\"learning_rate\"]\n",
    "        self.learning_rate_minimum = conf[\"learning_rate_minimum\"]\n",
    "        self.learning_rate_decay = conf[\"learning_rate_decay\"]\n",
    "        self.learning_rate_decay_step = conf[\"learning_rate_decay_step\"]\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        # reward is in (min_r, max_r)\n",
    "        self.min_r = conf[\"min_r\"]\n",
    "        self.max_r = conf[\"max_r\"]\n",
    "        self.batch_size = conf[\"batch_size\"]\n",
    "        # Build tensorflow network\n",
    "        st = time.time()\n",
    "        logger.debug(\"Building tensorflow graph...\")\n",
    "        with self.sess.as_default():\n",
    "            self._build_graph()\n",
    "            self.saver = tf.train.Saver()\n",
    "        logger.debug(\"Finished building tensorflow graph, spent time:\", time.time() - st)\n",
    "        if \"load_file_path\" in conf:\n",
    "            self.load_params(conf[\"load_file_path\"])\n",
    "        \n",
    "    def _get_memory(self, window_length, limit):\n",
    "        return SequentialMemory(window_length, limit)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, t_max):\n",
    "        tf.global_variables_initializer().run(session=self.sess)\n",
    "        # initialize target netwoork\n",
    "        self.update_target_q_network()\n",
    "\n",
    "        observation = self.env.reset()\n",
    "        for t in tqdm(xrange(t_max)):\n",
    "            # 1. predict\n",
    "            if t > 0:\n",
    "                state = self.memory.get_recent_state()\n",
    "                action = self.predict(state)\n",
    "            else:\n",
    "                action = self.random_action()\n",
    "            # 2. act\n",
    "            observation, reward, terminal, info = self.env.step(action)\n",
    "            observation = self.processor.preprocess(observation)\n",
    "            # 3. store data and train network\n",
    "            if t < 10:\n",
    "                result = self.observe(observation, reward, action, terminal, False)\n",
    "                continue\n",
    "            else:\n",
    "                result = self.observe(observation, reward, action, terminal, True)\n",
    "            q, loss, is_update = result\n",
    "            logger.debug(\"a: %d, r:%f, t:%s, q:%.4f, l: %.4f\" % \\\n",
    "                (action, reward, terminal, np.mean(q), loss))\n",
    "\n",
    "    def predict(self, s_t, ep):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_target_q_network(self):\n",
    "        self.sess.run(self.update_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from logging import getLogger\n",
    "import random\n",
    "\n",
    "# from .agent import Agent\n",
    "from rltensor.utils import get_shape\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class DQN(Agent):\n",
    "    def __init__(self, env, conf, q_network_cls, sess=None):\n",
    "        self.q_network_cls = q_network_cls\n",
    "        super(DQN, self).__init__(env, conf, sess)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        # training flag\n",
    "        self.training = tf.placeholder(tf.bool, name=\"training\")\n",
    "        # state shape has to be (batch, length,) + input_dim\n",
    "        self.state = tf.placeholder(tf.float32,\n",
    "                                     get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                     name='state')\n",
    "        _state = self.processor.tensor_process(self.state)\n",
    "        self.target_state = tf.placeholder(tf.float32,\n",
    "                                            get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                            name='target_state')\n",
    "        _target_state = self.processor.tensor_process(self.target_state)\n",
    "        # Employ maximal strategy\n",
    "        self.q_network = self.q_network_cls(self.conf[\"q_conf\"], scope_name=\"q_network\")\n",
    "        self.q_val = self.q_network(_state, self.training)\n",
    "        assert self.q_val.get_shape().as_list()[-1] == self.action_dim\n",
    "        self.max_action = tf.argmax(self.q_val, dimension=1)\n",
    "        # Build action graph\n",
    "        self.action = tf.placeholder(tf.int32, (None,), name='action')\n",
    "        action_one_hot = tf.one_hot(self.action, depth=self.action_dim)\n",
    "        self.action_q_val = tf.reduce_sum(self.q_val * action_one_hot, axis=1)\n",
    "        # Build target network\n",
    "        self.target_q_network = self.q_network_cls(self.conf[\"q_conf\"], scope_name=\"target_q_network\")\n",
    "        target_q_val = self.target_q_network(_target_state, self.training)\n",
    "        self.reward = tf.placeholder(tf.float32, (None,), name='reward')\n",
    "        max_one_hot = tf.one_hot(self.max_action, depth=self.action_dim)\n",
    "        max_q_val = tf.reduce_sum(self.q_val * max_one_hot, axis=1)\n",
    "        target_val = self.reward  + self.gamma * max_q_val\n",
    "        # Clip error to stabilize learning\n",
    "        delta = target_val - self.action_q_val\n",
    "        clipped_error = tf.where(tf.abs(delta) < self.error_clip,\n",
    "                                    0.5 * tf.square(delta),\n",
    "                                    tf.abs(delta), name='clipped_error')\n",
    "        self.loss = tf.reduce_mean(clipped_error, name='loss')\n",
    "        # Build optimization\n",
    "        self.learning_rate_op = tf.maximum(self.learning_rate_minimum,\n",
    "          tf.train.exponential_decay(\n",
    "              self.learning_rate,\n",
    "              self.global_step,\n",
    "              self.learning_rate_decay_step,\n",
    "              self.learning_rate_decay,\n",
    "              staircase=True))\n",
    "        self.loss = tf.reduce_mean(tf.square(target_val - self.action_q_val), name='loss')\n",
    "        self.q_optim = tf.train.AdamOptimizer(self.learning_rate_op)\\\n",
    "            .minimize(self.loss, var_list=self.q_network.variables)\n",
    "        self.update_op = self._get_update_op()\n",
    "\n",
    "    def observe(self, observation, reward, action, terminal, training):\n",
    "        # clip reward into  (min_r, max_r)\n",
    "        reward = max(self.min_r, min(self.max_r, reward))\n",
    "        # We always keep data\n",
    "        self.memory.append(observation, action, reward, terminal, training=True)\n",
    "        if training:\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            result = self.q_learning_minibatch(experiences)\n",
    "            self.sess.run(tf.assign(self.global_step, self.global_step + 1))\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def q_learning_minibatch(self, experiences):\n",
    "        feed_dict = {\n",
    "            self.state: [experience.state0 for experience in experiences],\n",
    "            self.target_state: [experience.state1 for experience in experiences],\n",
    "            self.reward: [experience.reward for experience in experiences],\n",
    "            self.action: [experience.action for experience in experiences],\n",
    "            self.training: True,\n",
    "        }\n",
    "        _, q_t, loss = self.sess.run([self.q_optim, self.action_q_val, self.loss],\n",
    "                                     feed_dict=feed_dict)\n",
    "        return q_t, loss, True\n",
    "    \n",
    "    def predict(self, state, ep=1e-3):\n",
    "        if random.random() < ep:\n",
    "            action = random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            action = self.sess.run(self.max_action, \n",
    "                                   feed_dict={self.state: [state],\n",
    "                                              self.training: False})[0]\n",
    "        return action\n",
    "    \n",
    "    def _get_update_op(self):\n",
    "        update_op = []\n",
    "        for target_var, var in zip(self.target_q_network.variables, self.q_network.variables):\n",
    "            update_op.append(tf.assign(target_var, var))\n",
    "        return update_op\n",
    "    \n",
    "    def random_action(self):\n",
    "        return np.random.randint(0, self.action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-10 01:51:16,331] Making new env: DemonAttack-v0\n",
      "  0%|          | 2/1000 [00:00<04:17,  3.87it/s]/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/memories/utils.py:19: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "100%|██████████| 1000/1000 [00:48<00:00, 20.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from rltensor.networks import FeedForward\n",
    "import gym\n",
    "env = gym.make('DemonAttack-v0')\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":True, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", 'num_hidden': 6, 'activation': tf.nn.softmax}\n",
    "        ],\n",
    "        \"memory_limit\": 10000,\n",
    "        \"window_length\": 4,\n",
    "        \"gamma\": 0.9,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"learning_rate_minimum\": 1e-3,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 100,\n",
    "        \"ep\": 1e-3,\n",
    "        \"min_r\": -1,\n",
    "        \"max_r\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"error_clip\": 1.0,\n",
    "        \"processor\": AtariProcessor(84, 84)\n",
    "}\n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(env, conf, q_network_cls=FeedForward)\n",
    "dqn.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.memory.nb_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_shape(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "x = tf.Variable(0)\n",
    "y = x + 1\n",
    "# tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "if [1, 2, 3]:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "count = 4\n",
    "while count < 5:\n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%s\" % True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.insert(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
