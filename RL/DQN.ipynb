{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomoaki/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "from six.moves import xrange\n",
    "from gym import wrappers\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Runner(object):\n",
    "    def __init__(self, agent, env, env_name=\"env\",\n",
    "                 tensorboard_dir=\"./logs\", scalar_summary_tags=None,\n",
    "                 histogram_summary_tags=None, load_file_path=None,\n",
    "                *args, **kwargs):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "\n",
    "        if scalar_summary_tags is None:\n",
    "            scalar_summary_tags = [\n",
    "                'average.reward', 'average.loss', 'average.q',\n",
    "                'episode.cumulative_reward', 'episode.max_reward',\n",
    "                'episode.min_reward', 'episode.avg_reward',\n",
    "                'episode.num_of_game', 'training.epsilon',\n",
    "                'training.learning_rate',\n",
    "                'training.num_step_per_sec', 'training.time']\n",
    "        self.scalar_summary_tags = scalar_summary_tags\n",
    "\n",
    "        if histogram_summary_tags is None:\n",
    "            histogram_summary_tags = ['episode.rewards', 'episode.actions', 'episode.errors',\n",
    "                                      'episode_test.cumulative_rewards', 'episode_test.max_rewards',\n",
    "                                      'episode_test.min_rewards', 'episode_test.avg_rewards']\n",
    "        self.histogram_summary_tags = histogram_summary_tags\n",
    "        # Initialization\n",
    "        self.load_file_path = load_file_path\n",
    "        self._initialization()\n",
    "        self.num_ep = 1\n",
    "        self.st = time.time()\n",
    "\n",
    "    def fit(self, t_max, num_max_start_steps=0,\n",
    "            save_file_path=None,\n",
    "            save_video_path=None, overwrite=True,\n",
    "            render_freq=None,\n",
    "            log_freq=1000,\n",
    "            avg_length=1000):\n",
    "        # Save Model\n",
    "        self.agent.save_params(save_file_path, overwrite)\n",
    "        # Record Viodeo\n",
    "        if save_video_path is not None:\n",
    "            self.env = wrappers.Monitor(self.env,\n",
    "                                        save_video_path,\n",
    "                                        force=overwrite)\n",
    "        # initialize target netwoork\n",
    "        self.agent.init_update()\n",
    "        \n",
    "        # accumulate results\n",
    "        total_reward = deque(maxlen=avg_length)\n",
    "        total_loss = deque(maxlen=avg_length)\n",
    "        total_q_val = deque(maxlen=avg_length)\n",
    "        max_q_val = deque(maxlen=avg_length)\n",
    "        ep_rewards = []\n",
    "        ep_losses = []\n",
    "        ep_q_vals = []\n",
    "        ep_actions = []\n",
    "        ep_errors = []\n",
    "        step = self.agent.global_step\n",
    "        _st = self.st\n",
    "        \n",
    "        # initialize enviroment\n",
    "        observation = self.env.reset()\n",
    "        # Determine if it has to be randomly initialized\n",
    "        init_flag = True\n",
    "        # Start from the middle of training\n",
    "        t_max = t_max - step\n",
    "        try:\n",
    "            for t in tqdm(xrange(t_max)):\n",
    "                if init_flag:\n",
    "                    init_flag = False\n",
    "                    action = self.env.action_space.sample()\n",
    "                    terminal = False\n",
    "                    reward = 0\n",
    "                    self.agent.observe(observation, action, reward, terminal,\n",
    "                                       training=False, is_store=False)\n",
    "                    if num_max_start_steps == 0:\n",
    "                        num_random_start_steps = 0\n",
    "                    else:\n",
    "                        num_random_start_steps = np.random.randint(num_max_start_steps)\n",
    "                    for _ in xrange(num_random_start_steps):\n",
    "                        action = self.env.action_space.sample()\n",
    "                        observation, reward, terminal, info = self.env.step(action)\n",
    "                        observation = deepcopy(observation)\n",
    "                        if terminal:\n",
    "                            # Reset stored current states\n",
    "                            self.agent.memory.reset()\n",
    "                            observation = self.env.reset()\n",
    "                        self.agent.observe(observation, action, reward, terminal,\n",
    "                                           training=False, is_store=False)\n",
    "                # Update step\n",
    "                self.agent.update_step()\n",
    "                step = self.agent.global_step\n",
    "                # 1. predict\n",
    "                state = self.agent.get_recent_state()\n",
    "                action = self.agent.predict(state)\n",
    "                # 2. act\n",
    "                observation, reward, terminal, info = self.env.step(action)\n",
    "                # 3. store data and train network\n",
    "                if step < self.agent.t_learn_start:\n",
    "                    response = self.agent.observe(observation, action, reward,\n",
    "                                                  terminal, training=False,\n",
    "                                                  is_store=True)\n",
    "                    if terminal:\n",
    "                        observation = self.env.reset()\n",
    "                else:\n",
    "                    response = self.agent.observe(observation, action, reward,\n",
    "                                                  terminal, training=True,\n",
    "                                                  is_store=True)\n",
    "                    q, loss, error, is_update = response\n",
    "                    step = self.agent.global_step\n",
    "                    # update statistics\n",
    "                    total_reward.append(reward)\n",
    "                    total_loss.append(loss)\n",
    "                    total_q_val.append(np.mean(q))\n",
    "                    ep_actions.append(action)\n",
    "                    ep_errors.append(error)\n",
    "                    ep_rewards.append(reward)\n",
    "                    ep_losses.append(loss)\n",
    "                    ep_q_vals.append(np.mean(q))\n",
    "                    # Visualize reuslts\n",
    "                    if render_freq is not None:\n",
    "                        if step % render_freq == 0:\n",
    "                            self.env.render()\n",
    "                    # Write summary\n",
    "                    if log_freq is not None and step % log_freq == 0:\n",
    "                        num_per_sec = log_freq / (time.time() - _st)\n",
    "                        _st = time.time()\n",
    "                        epsilon = self.agent.epsilon\n",
    "                        learning_rate = self.agent.learning_rate\n",
    "                        avg_r = np.mean(total_reward)\n",
    "                        avg_loss = np.mean(total_loss)\n",
    "                        avg_q_val = np.mean(total_q_val)\n",
    "                        tag_dict = {'episode.num_of_game': self.num_ep,\n",
    "                                    'average.reward': avg_r,\n",
    "                                    'average.loss': avg_loss,\n",
    "                                    'average.q': avg_q_val,\n",
    "                                    'training.epsilon': epsilon,\n",
    "                                    'training.learning_rate': learning_rate,\n",
    "                                    'training.num_step_per_sec': num_per_sec,\n",
    "                                    'training.time': time.time() - self.st}\n",
    "                        self._inject_summary(tag_dict, step)\n",
    "                    if terminal:\n",
    "                        try:\n",
    "                            cum_ep_reward = np.sum(ep_rewards)\n",
    "                            max_ep_reward = np.max(ep_rewards)\n",
    "                            min_ep_reward = np.min(ep_rewards)\n",
    "                            avg_ep_reward = np.mean(ep_rewards)\n",
    "                        except:\n",
    "                            cum_ep_reward = 0\n",
    "                            max_ep_reward = 0\n",
    "                            min_ep_reward = 0\n",
    "                            avg_ep_reward = 0\n",
    "\n",
    "                        tag_dict = {'episode.cumulative_reward': cum_ep_reward,\n",
    "                                    'episode.max_reward': max_ep_reward,\n",
    "                                    'episode.min_reward': min_ep_reward,\n",
    "                                    'episode.avg_reward': avg_ep_reward,\n",
    "                                    'episode.rewards': ep_rewards,\n",
    "                                    'episode.actions': ep_actions,\n",
    "                                    'episode.errors': ep_errors}\n",
    "                        self._inject_summary(tag_dict, self.num_ep)\n",
    "                        # Reset stored current states\n",
    "                        self.agent.memory.reset()\n",
    "                        observation = self.env.reset()\n",
    "                        response = self.agent.observe(observation, None, 0, False,\n",
    "                                                      training=False,\n",
    "                                                      is_store=False)\n",
    "                        ep_rewards = []\n",
    "                        ep_losses = []\n",
    "                        ep_q_vals = []\n",
    "                        ep_actions = []\n",
    "                        ep_errors = []\n",
    "                        self.num_ep += 1\n",
    "                        init_flag = True\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        # Update parameters before finishing\n",
    "        self.agent.save_params(save_file_path, True)\n",
    "\n",
    "    def play(self, num_episode=1, ep=0.05, overwrite=True,\n",
    "             load_file_path=None, save_video_path=None, render_freq=None,\n",
    "             log_freq=10, avg_length=1000):\n",
    "        if load_file_path is not None:\n",
    "            tf.global_variables_initializer().run(session=self.agent.sess);\n",
    "            self.load_params(load_file_path)\n",
    "        # Record Viodeo\n",
    "        if save_video_path is not None:\n",
    "            self.env = wrappers.Monitor(self.env, save_video_path, force=overwrite)\n",
    "        for num_ep in range(1, num_episode + 1):\n",
    "            # initialize enviroment\n",
    "            self.agent.memory.reset()\n",
    "            observation = self.env.reset()\n",
    "            action = self.env.action_space.sample()\n",
    "            reward = 0\n",
    "            terminal = False\n",
    "            self.agent.observe(observation, action, reward, terminal,\n",
    "                               training=False, is_store=False)\n",
    "            ep_rewards = []\n",
    "            step = 1\n",
    "            cum_ep_rewards = []\n",
    "            max_ep_rewards = []\n",
    "            min_ep_rewards = []\n",
    "            avg_ep_rewards = []\n",
    "            while not terminal:\n",
    "                # 1. predict\n",
    "                state = self.agent.get_recent_state()\n",
    "                action = self.agent.predict(state, ep)\n",
    "                # 2. act\n",
    "                observation, reward, terminal, info = self.env.step(action)\n",
    "                self.agent.observe(observation, action, reward, terminal,\n",
    "                                   training=False, is_store=False)\n",
    "                # accumulate results\n",
    "                ep_rewards.append(reward)\n",
    "                # Visualize reuslts\n",
    "                if render_freq is not None:\n",
    "                    if step % render_freq == 0:\n",
    "                        self.env.render()\n",
    "                if terminal:\n",
    "                    try:\n",
    "                        cum_ep_reward = np.sum(ep_rewards)\n",
    "                        max_ep_reward = np.max(ep_rewards)\n",
    "                        min_ep_reward = np.min(ep_rewards)\n",
    "                        avg_ep_reward = np.mean(ep_rewards)\n",
    "                    except:\n",
    "                        cum_ep_reward, max_ep_reward, min_ep_reward, avg_ep_reward = 0, 0, 0, 0\n",
    "                        cum_ep_rewards.append(cum_ep_reward)\n",
    "                        max_ep_rewards.append(max_ep_reward)\n",
    "                        min_ep_rewards.append(min_ep_reward)\n",
    "                        avg_ep_rewards.append(avg_ep_reward)\n",
    "                step += 1\n",
    "            tag_dict = {'episode_test.cumulative_rewards': cum_ep_reward,\n",
    "                        'episode_test.max_rewards': max_ep_reward,\n",
    "                        'episode_test.min_rewards': min_ep_reward,\n",
    "                        'episode_test.avg_rewards': avg_ep_reward,}\n",
    "            self._inject_summary(tag_dict, self.num_ep)\n",
    "\n",
    "    def _initialization(self):\n",
    "        # Initialize graph\n",
    "        with self.agent.sess.as_default():\n",
    "            with tf.name_scope(\"summary\"):\n",
    "                self._build_summaries()\n",
    "            tf.global_variables_initializer().run()\n",
    "            if self.load_file_path is not None:\n",
    "                self.agent.load_params(self.load_file_path)\n",
    "\n",
    "    def _build_summaries(self):\n",
    "        self.writer = tf.summary.FileWriter(\n",
    "            self.tensorboard_dir, self.agent.sess.graph)\n",
    "        self.summary_placeholders = {}\n",
    "        self.summary_ops = {}\n",
    "        for tag in self.scalar_summary_tags:\n",
    "            self.summary_placeholders[tag] =\\\n",
    "                tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag] =\\\n",
    "                tf.summary.scalar(\"%s/%s\" % (self.env_name, tag),\n",
    "                                  self.summary_placeholders[tag])\n",
    "\n",
    "        for tag in self.histogram_summary_tags:\n",
    "            self.summary_placeholders[tag] =\\\n",
    "                tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag] = tf.summary.histogram(\n",
    "                tag,\n",
    "                self.summary_placeholders[tag])\n",
    "\n",
    "    def _inject_summary(self, tag_dict, step):\n",
    "        summary_str_lists = self.agent.sess.run(\n",
    "            [self.summary_ops[tag] for tag in tag_dict.keys()],\n",
    "            {self.summary_placeholders[tag]: value for tag, value in tag_dict.items()})\n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-20 02:51:38,519] Making new env: Breakout-v0\n",
      "[2017-12-20 02:51:38,651] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tensorflow graph...\n",
      "Finished building tensorflow graph, spent time: 0.5835573673248291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 36/50000000 [00:00<38:55:58, 356.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 670753/50000000 [1:51:41<143:57:10, 95.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from rltensor.agents import DQN\n",
    "from rltensor.processors import AtariProcessor\n",
    "# from rltensor.executions import Runner\n",
    "from rltensor.configs import dqn_config, fit_config\n",
    "\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "conf = dict(\n",
    "    action_spec={\"type\": \"int\", \"shape\": env.action_space.n},\n",
    ")\n",
    "default_config = dqn_config()\n",
    "conf.update(default_config)\n",
    "\n",
    "_fit_config = fit_config()\n",
    "# conf[\"t_learn_start\"] = 10\n",
    "env_name = 'Breakout-v0'\n",
    "env = gym.make(env_name)\n",
    "save_file_path = \"./params/model.ckpt\"\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(**conf)\n",
    "runner = Runner(agent=dqn, env=env, env_name=env_name,\n",
    "                tensorboard_dir=\"./logs\")\n",
    "runner.fit(save_video_path=None, **_fit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file_path = \"./params/model.ckpt\"\n",
    "# runner = Runner(agent=dqn, env=env, env_name=env_name,\n",
    "#                 load_file_path=load_file_path, tensorboard_dir=\"./test_logs\")\n",
    "runner.play(5, render_freq=2, ep=0.05, log_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/48475068 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 8/48475068 [00:00<173:29:42, 77.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "  0%|          | 17/48475068 [00:00<167:53:33, 80.20it/s]\u001b[A\n",
      "  0%|          | 26/48475068 [00:00<162:50:59, 82.69it/s]\u001b[A\n",
      "  0%|          | 35/48475068 [00:00<162:21:10, 82.94it/s]\u001b[A\n",
      "  0%|          | 45/48475068 [00:00<158:31:54, 84.94it/s]\u001b[A\n",
      "  0%|          | 55/48475068 [00:00<157:44:56, 85.36it/s]\u001b[A\n",
      "  0%|          | 65/48475068 [00:00<152:35:30, 88.24it/s]\u001b[A\n",
      "  0%|          | 75/48475068 [00:00<152:50:28, 88.10it/s]\u001b[A\n",
      "  0%|          | 85/48475068 [00:00<149:13:45, 90.23it/s]\u001b[A\n",
      "  0%|          | 95/48475068 [00:01<150:43:17, 89.34it/s]\u001b[A\n",
      "  0%|          | 105/48475068 [00:01<148:21:35, 90.76it/s]\u001b[A\n",
      "  0%|          | 115/48475068 [00:01<150:11:32, 89.65it/s]\u001b[A\n",
      "  0%|          | 125/48475068 [00:01<150:24:02, 89.53it/s]\u001b[A\n",
      "  0%|          | 135/48475068 [00:01<153:35:53, 87.67it/s]\u001b[A\n",
      "  0%|          | 145/48475068 [00:01<149:38:16, 89.99it/s]\u001b[A\n",
      "  0%|          | 155/48475068 [00:01<151:23:56, 88.94it/s]\u001b[A\n",
      "  0%|          | 165/48475068 [00:01<149:23:00, 90.14it/s]\u001b[A\n",
      "  0%|          | 175/48475068 [00:01<150:48:42, 89.29it/s]\u001b[A\n",
      "  0%|          | 185/48475068 [00:02<148:28:15, 90.69it/s]\u001b[A\n",
      "  0%|          | 195/48475068 [00:02<149:30:22, 90.06it/s]\u001b[A\n",
      "  0%|          | 205/48475068 [00:02<146:30:40, 91.91it/s]\u001b[A\n",
      "  0%|          | 215/48475068 [00:02<148:03:53, 90.94it/s]\u001b[A\n",
      "  0%|          | 225/48475068 [00:02<146:42:39, 91.78it/s]\u001b[A\n",
      "  0%|          | 235/48475068 [00:02<149:35:32, 90.01it/s]\u001b[A\n",
      "  0%|          | 245/48475068 [00:02<147:24:31, 91.35it/s]\u001b[A\n",
      "  0%|          | 255/48475068 [00:02<149:34:23, 90.02it/s]\u001b[A\n",
      "  0%|          | 265/48475068 [00:02<146:53:02, 91.67it/s]\u001b[A\n",
      "  0%|          | 275/48475068 [00:03<148:51:34, 90.46it/s]\u001b[A\n",
      "  0%|          | 285/48475068 [00:03<147:12:03, 91.48it/s]\u001b[A\n",
      "  0%|          | 295/48475068 [00:03<148:48:37, 90.49it/s]\u001b[A\n",
      "  0%|          | 305/48475068 [00:03<146:56:07, 91.64it/s]\u001b[A\n",
      "  0%|          | 315/48475068 [00:03<152:02:43, 88.56it/s]\u001b[A\n",
      "  0%|          | 325/48475068 [00:03<149:28:07, 90.09it/s]\u001b[A\n",
      "  0%|          | 335/48475068 [00:03<150:26:00, 89.51it/s]\u001b[A\n",
      "  0%|          | 345/48475068 [00:03<147:36:59, 91.22it/s]\u001b[A\n",
      "  0%|          | 355/48475068 [00:03<150:29:01, 89.48it/s]\u001b[A\n",
      "  0%|          | 364/48475068 [00:04<150:56:07, 89.21it/s]\u001b[A\n",
      "  0%|          | 373/48475068 [00:04<151:49:59, 88.68it/s]\u001b[A\n",
      "  0%|          | 383/48475068 [00:04<152:18:07, 88.41it/s]\u001b[A\n",
      "  0%|          | 393/48475068 [00:04<149:35:48, 90.01it/s]\u001b[A\n",
      "  0%|          | 403/48475068 [00:04<151:26:30, 88.91it/s]\u001b[A\n",
      "  0%|          | 412/48475068 [00:04<151:13:21, 89.04it/s]\u001b[A\n",
      "  0%|          | 421/48475068 [00:04<157:01:58, 85.75it/s]\u001b[A\n",
      "  0%|          | 431/48475068 [00:04<155:53:34, 86.37it/s]\u001b[A\n",
      "  0%|          | 441/48475068 [00:04<152:07:53, 88.51it/s]\u001b[A\n",
      "  0%|          | 451/48475068 [00:05<153:04:23, 87.97it/s]\u001b[A\n",
      "  0%|          | 461/48475068 [00:05<149:45:03, 89.92it/s]\u001b[A\n",
      "  0%|          | 471/48475068 [00:05<150:53:36, 89.24it/s]\u001b[A\n",
      "  0%|          | 481/48475068 [00:05<147:50:56, 91.07it/s]\u001b[A\n",
      "  0%|          | 491/48475068 [00:05<149:42:13, 89.95it/s]\u001b[A\n",
      "  0%|          | 501/48475068 [00:05<149:04:49, 90.32it/s]\u001b[A\n",
      "  0%|          | 511/48475068 [00:05<150:49:53, 89.27it/s]\u001b[A\n",
      "  0%|          | 521/48475068 [00:05<148:02:21, 90.96it/s]\u001b[A\n",
      "  0%|          | 531/48475068 [00:05<148:42:34, 90.55it/s]\u001b[A\n",
      "  0%|          | 541/48475068 [00:06<147:27:57, 91.31it/s]\u001b[A\n",
      "  0%|          | 551/48475068 [00:06<149:04:28, 90.32it/s]\u001b[A\n",
      "  0%|          | 561/48475068 [00:06<149:36:46, 90.00it/s]\u001b[A\n",
      "  0%|          | 571/48475068 [00:06<156:33:56, 86.00it/s]\u001b[A\n",
      "  0%|          | 581/48475068 [00:06<153:35:55, 87.66it/s]\u001b[A\n",
      "  0%|          | 591/48475068 [00:06<154:43:12, 87.03it/s]\u001b[A\n",
      "  0%|          | 601/48475068 [00:06<151:22:28, 88.95it/s]\u001b[A\n",
      "  0%|          | 611/48475068 [00:06<151:29:36, 88.88it/s]\u001b[A\n",
      "  0%|          | 621/48475068 [00:06<148:30:49, 90.67it/s]\u001b[A\n",
      "  0%|          | 631/48475068 [00:07<150:14:42, 89.62it/s]\u001b[A\n",
      "  0%|          | 641/48475068 [00:07<147:32:14, 91.27it/s]\u001b[A\n",
      "  0%|          | 651/48475068 [00:07<148:42:09, 90.55it/s]\u001b[A\n",
      "  0%|          | 661/48475068 [00:07<146:26:43, 91.95it/s]\u001b[A\n",
      "  0%|          | 671/48475068 [00:07<148:19:35, 90.78it/s]\u001b[A\n",
      "  1%|          | 363739/48475068 [1:06:29<149:25:46, 89.44it/s]"
     ]
    }
   ],
   "source": [
    "save_file_path = \"./params/model.ckpt\"\n",
    "# runner = Runner(agent=dqn, env=env, env_name=env_name, is_initialize=False,\n",
    "#                 load_file_path=load_file_path, tensorboard_dir=\"./logs\")\n",
    "runner.fit(save_video_path=None, **_fit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:02,794] Making new env: Breakout-v0\n",
      "[2017-07-30 21:57:03,411] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/tomoaki/work/Development/RL/breakout_videos')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./breakout_dqn_params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:03,497] Restoring parameters from ./breakout_dqn_params/model.ckpt\n",
      "[2017-07-30 21:57:03,531] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-07-30 21:57:03,538] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:35,712] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000001.mp4\n",
      "\n",
      "[2017-07-30 22:01:21,034] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000008.mp4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from rltensor.agents import DQN\n",
    "from rltensor.processors import AtariProcessor\n",
    "from rltensor.networks import DuelingModel\n",
    "\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":False, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        \"env_name\": 'Breakout-v0',\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "}\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(env, conf, q_network_cls=DuelingModel)\n",
    "dqn.play(num_episode=10, ep=0.05, load_file_path=\"./breakout_dqn_params/model.ckpt\",\n",
    "         save_video_path=\"./breakout_videos\", render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if [1, 2, 3]:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 4\n",
    "while count < 5:\n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"%s\" % True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.insert(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = deque([1, 2, 3], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.append(3)\n",
    "x.append(3)\n",
    "x.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f829244df969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_false\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'select'"
     ]
    }
   ],
   "source": [
    " result = tf.select(pred, val_if_true, val_if_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.bool, (None,))\n",
    "y = tf.cast(x, tf.int32)\n",
    "z = tf.one_hot(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "print(y.eval(feed_dict={x:[True, False, True]}))\n",
    "print(z.eval(feed_dict={x:[True, False, True]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.arange(10).astype(int)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.random.choice(range(0, 5), 3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1, 2, 3, 4], 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "np.append(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
