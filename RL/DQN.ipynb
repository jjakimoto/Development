{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomoaki/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import deque\n",
    "from six.moves import xrange\n",
    "from gym import wrappers\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Runner(object):\n",
    "    def __init__(self, agent, env, env_name=\"env\",\n",
    "                 tensorboard_dir=\"./logs\", scalar_summary_tags=None,\n",
    "                 histogram_summary_tags=None, load_file_path=None,\n",
    "                 *args, **kwargs):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "\n",
    "        if scalar_summary_tags is None:\n",
    "            scalar_summary_tags = [\n",
    "                'average.reward', 'average.loss', 'average.q',\n",
    "                'episode.cumulative_reward', 'episode.max_reward',\n",
    "                'episode.min_reward', 'episode.avg_reward',\n",
    "                'episode.num_of_game', 'training.epsilon',\n",
    "                'training.learning_rate',\n",
    "                'training.num_step_per_sec', 'training.time']\n",
    "        self.scalar_summary_tags = scalar_summary_tags\n",
    "\n",
    "        if histogram_summary_tags is None:\n",
    "            histogram_summary_tags = ['episode.rewards', 'episode.actions']\n",
    "        self.histogram_summary_tags = histogram_summary_tags\n",
    "\n",
    "        self.load_file_path = load_file_path\n",
    "\n",
    "    def fit(self, t_max, num_max_start_steps=0,\n",
    "            save_file_path=None,\n",
    "            save_video_path=None, overwrite=True,\n",
    "            render_freq=None,\n",
    "            log_freq=10,\n",
    "            avg_length=1000):\n",
    "        self._initialization()\n",
    "        # Save Model\n",
    "        self.agent.save_params(save_file_path, overwrite)\n",
    "        # Record Viodeo\n",
    "        if save_video_path is not None:\n",
    "            self.env = wrappers.Monitor(self.env,\n",
    "                                        save_video_path,\n",
    "                                        force=overwrite)\n",
    "        # initialize target netwoork\n",
    "        self.agent.init_update()\n",
    "        # initialize enviroment\n",
    "        observation = self.env.reset()\n",
    "        action = self.env.action_space.sample()\n",
    "        # This slightly changes the start position between games.\n",
    "        terminal = False\n",
    "        reward = 0\n",
    "        if num_max_start_steps == 0:\n",
    "            num_random_start_steps = 0\n",
    "        else:\n",
    "            num_random_start_steps = np.random.randint(num_max_start_steps)\n",
    "        for _ in xrange(num_random_start_steps):\n",
    "            action = self.env.action_space.sample()\n",
    "            observation, reward, terminal, info = self.env.step(action)\n",
    "            observation = deepcopy(observation)\n",
    "            print(np.mean(observation), observation.shape)\n",
    "            if terminal:\n",
    "                observation = self.env.reset()\n",
    "        self.agent.observe(observation, action, reward, terminal,\n",
    "                           training=False, is_store=False)\n",
    "\n",
    "        # accumulate results\n",
    "        total_reward = deque(maxlen=avg_length)\n",
    "        total_loss = deque(maxlen=avg_length)\n",
    "        total_q_val = deque(maxlen=avg_length)\n",
    "        ep_rewards = []\n",
    "        ep_losses = []\n",
    "        ep_q_vals = []\n",
    "        ep_actions = []\n",
    "        num_ep = 1\n",
    "        step = self.agent.global_step\n",
    "        st = time.time()\n",
    "        _st = st\n",
    "        for t in tqdm(xrange(t_max)):\n",
    "            try:\n",
    "                # Update step\n",
    "                self.agent.update_step()\n",
    "                # 1. predict\n",
    "                state = self.agent.get_recent_state()\n",
    "                action = self.agent.predict(state)\n",
    "                # 2. act\n",
    "                observation, reward, terminal, info = self.env.step(action)\n",
    "                # 3. store data and train network\n",
    "                if t < self.agent.t_learn_start:\n",
    "                    response = self.agent.observe(observation, action, reward,\n",
    "                                                  terminal, training=False,\n",
    "                                                  is_store=True)\n",
    "                    if terminal:\n",
    "                        observation = self.env.reset()\n",
    "                    continue\n",
    "                else:\n",
    "                    response = self.agent.observe(observation, action, reward,\n",
    "                                                  terminal, training=True,\n",
    "                                                  is_store=True)\n",
    "                q, loss, error, is_update = response\n",
    "                step = self.agent.global_step\n",
    "                # update statistics\n",
    "                total_reward.append(reward)\n",
    "                total_loss.append(loss)\n",
    "                total_q_val.append(np.mean(q))\n",
    "                ep_actions.append(action)\n",
    "                ep_rewards.append(reward)\n",
    "                ep_losses.append(loss)\n",
    "                ep_q_vals.append(np.mean(q))\n",
    "                # Visualize reuslts\n",
    "                if render_freq is not None:\n",
    "                    if step % render_freq == 0:\n",
    "                        self.env.render()\n",
    "                # Write summary\n",
    "                if log_freq is not None and step % log_freq == 0:\n",
    "                    num_per_sec = log_freq / (time.time() - _st)\n",
    "                    _st = time.time()\n",
    "                    epsilon = self.agent.epsilon\n",
    "                    learning_rate = self.agent.learning_rate\n",
    "                    avg_r = np.mean(total_reward)\n",
    "                    avg_loss = np.mean(total_loss)\n",
    "                    avg_q_val = np.mean(total_q_val)\n",
    "                    tag_dict = {'episode.num_of_game': num_ep,\n",
    "                                'average.reward': avg_r,\n",
    "                                'average.loss': avg_loss,\n",
    "                                'average.q': avg_q_val,\n",
    "                                'training.epsilon': epsilon,\n",
    "                                'training.learning_rate': learning_rate,\n",
    "                                'training.num_step_per_sec': num_per_sec,\n",
    "                                'training.time': time.time() - st}\n",
    "                    self._inject_summary(tag_dict, step)\n",
    "                if terminal:\n",
    "                    try:\n",
    "                        cum_ep_reward = np.sum(ep_rewards)\n",
    "                        max_ep_reward = np.max(ep_rewards)\n",
    "                        min_ep_reward = np.min(ep_rewards)\n",
    "                        avg_ep_reward = np.mean(ep_rewards)\n",
    "                    except:\n",
    "                        cum_ep_reward = 0\n",
    "                        max_ep_reward = 0\n",
    "                        min_ep_reward = 0\n",
    "                        avg_ep_reward = 0\n",
    "\n",
    "                    tag_dict = {'episode.cumulative_reward': cum_ep_reward,\n",
    "                                'episode.max_reward': max_ep_reward,\n",
    "                                'episode.min_reward': min_ep_reward,\n",
    "                                'episode.avg_reward': avg_ep_reward,\n",
    "                                'episode.rewards': ep_rewards}\n",
    "                    if hasattr(self.agent.memory, \"priorities\"):\n",
    "                        tag_dict['episode.actions'] =\\\n",
    "                            self.agent.memory.priorities\n",
    "                    self._inject_summary(tag_dict, num_ep)\n",
    "                    observation = self.env.reset()\n",
    "                    response = self.agent.observe(observation, None, 0, False,\n",
    "                                                  training=False,\n",
    "                                                  is_store=False)\n",
    "                    self.agent.memory.reset()\n",
    "                    ep_rewards = []\n",
    "                    ep_losses = []\n",
    "                    ep_q_vals = []\n",
    "                    ep_actions = []\n",
    "                    num_ep += 1\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "        # Update parameters before finishing\n",
    "        self.agent.save_params(save_file_path, True)\n",
    "\n",
    "    def play(self, num_episode=1, ep=0.05, overwrite=True, load_file_path=None, save_video_path=None, render_freq=None):\n",
    "        tf.global_variables_initializer().run(session=self.sess);\n",
    "        if load_file_path is not None:\n",
    "            self.load_params(load_file_path)\n",
    "        # Record Viodeo\n",
    "        if save_video_path is not None:\n",
    "            self.env = wrappers.Monitor(self.env, save_video_path, force=overwrite)\n",
    "        for num_ep in range(1, num_episode + 1):\n",
    "            # initialize enviroment\n",
    "            observation = self.env.reset()\n",
    "            self.memory.reset()\n",
    "            action = self.env.action_space.sample()\n",
    "            reward = 0\n",
    "            terminal = False\n",
    "            observation, action, reward_, terminal = self.processor.preprocess(observation, action, reward, terminal)\n",
    "            self.memory.append(observation, action, reward_, terminal, is_store=False)\n",
    "            ep_rewards = []\n",
    "            step = 1\n",
    "            while not terminal:\n",
    "                # 1. predict\n",
    "                state = self.memory.get_recent_state()\n",
    "                action = self.predict(state, ep)\n",
    "                # 2. act\n",
    "                observation, reward, terminal, info = self.env.step(action)\n",
    "                # initialize memory\n",
    "                observation, action, reward_, terminal = self.processor.preprocess(observation, action, reward, terminal)\n",
    "                self.memory.append(observation, action, reward_, terminal, is_store=False)\n",
    "                # accumulate results\n",
    "                ep_rewards.append(reward)\n",
    "                # Visualize reuslts\n",
    "                if render_freq is not None:\n",
    "                    if step % render_freq == 0:\n",
    "                        self.env.render()\n",
    "                if terminal:\n",
    "                    try:\n",
    "                        cum_ep_reward = np.sum(ep_rewards)\n",
    "                        max_ep_reward = np.max(ep_rewards)\n",
    "                        min_ep_reward = np.min(ep_rewards)\n",
    "                        avg_ep_reward = np.mean(ep_rewards)\n",
    "                    except:\n",
    "                        cum_ep_reward, max_ep_reward, min_ep_reward, avg_ep_reward = 0, 0, 0, 0\n",
    "                    tag_dict = {'episode.cumulative_reward': cum_ep_reward,\n",
    "                                'episode.max_reward': max_ep_reward,\n",
    "                                'episode.min_reward': min_ep_reward,\n",
    "                                'episode.avg_reward': avg_ep_reward,\n",
    "                                'episode.rewards': ep_rewards}\n",
    "                    self._inject_summary(tag_dict, num_ep)\n",
    "                step += 1\n",
    "\n",
    "    def _initialization(self):\n",
    "        # Initialize graph\n",
    "        with self.agent.sess.as_default():\n",
    "            with tf.name_scope(\"summary\"):\n",
    "                self._build_summaries()\n",
    "            if self.load_file_path is not None:\n",
    "                self.load_params(self.load_file_path)\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "    def _build_summaries(self):\n",
    "        self.writer = tf.summary.FileWriter(\n",
    "            self.tensorboard_dir, self.agent.sess.graph)\n",
    "        self.summary_placeholders = {}\n",
    "        self.summary_ops = {}\n",
    "        for tag in self.scalar_summary_tags:\n",
    "            self.summary_placeholders[tag] =\\\n",
    "                tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag] =\\\n",
    "                tf.summary.scalar(\"%s/%s\" % (self.env_name, tag),\n",
    "                                  self.summary_placeholders[tag])\n",
    "\n",
    "        for tag in self.histogram_summary_tags:\n",
    "            self.summary_placeholders[tag] =\\\n",
    "                tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag] = tf.summary.histogram(\n",
    "                tag,\n",
    "                self.summary_placeholders[tag])\n",
    "\n",
    "    def _inject_summary(self, tag_dict, step):\n",
    "        summary_str_lists = self.agent.sess.run(\n",
    "            [self.summary_ops[tag] for tag in tag_dict.keys()],\n",
    "            {self.summary_placeholders[tag]: value for tag, value in tag_dict.items()})\n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-19 09:25:31,256] Making new env: Breakout-v0\n",
      "[2017-12-19 09:25:31,392] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tensorflow graph...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from rltensor.agents import DQN\n",
    "from rltensor.processors import AtariProcessor\n",
    "# from rltensor.executions import Runner\n",
    "from rltensor.configs import dqn_config, fit_config\n",
    "\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "conf = dict(\n",
    "    action_spec={\"type\": \"int\", \"shape\": env.action_space.n},\n",
    ")\n",
    "default_config = dqn_config()\n",
    "conf.update(default_config)\n",
    "\n",
    "_fit_config = fit_config()\n",
    "env_name = 'Breakout-v0'\n",
    "env = gym.make(env_name)\n",
    "with tf.device('/cpu:0'):\n",
    "    tf.reset_default_graph()\n",
    "    dqn = DQN(**conf)\n",
    "    runner = Runner(agent=dqn, env=env, env_name=env_name, tensorboard_dir=\"./logs\")\n",
    "    runner.fit(save_video_path=\"./video\", **_fit_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "x = list(deque(maxlen=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__dict__': <attribute '__dict__' of 'FitConfig' objects>,\n",
       "              '__doc__': None,\n",
       "              '__module__': 'rltensor.configs',\n",
       "              '__weakref__': <attribute '__weakref__' of 'FitConfig' objects>,\n",
       "              'log_freq': (1001,),\n",
       "              'num_max_start_steps': (30,),\n",
       "              't_max': (50000000,)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_config(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:02,794] Making new env: Breakout-v0\n",
      "[2017-07-30 21:57:03,411] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/tomoaki/work/Development/RL/breakout_videos')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./breakout_dqn_params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:03,497] Restoring parameters from ./breakout_dqn_params/model.ckpt\n",
      "[2017-07-30 21:57:03,531] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-07-30 21:57:03,538] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 21:57:35,712] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000001.mp4\n",
      "\n",
      "[2017-07-30 22:01:21,034] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.2.3241.video000008.mp4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from rltensor.agents import DQN\n",
    "from rltensor.processors import AtariProcessor\n",
    "from rltensor.networks import DuelingModel\n",
    "\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":False, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        \"env_name\": 'Breakout-v0',\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "}\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(env, conf, q_network_cls=DuelingModel)\n",
    "dqn.play(num_episode=10, ep=0.05, load_file_path=\"./breakout_dqn_params/model.ckpt\",\n",
    "         save_video_path=\"./breakout_videos\", render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if [1, 2, 3]:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 4\n",
    "while count < 5:\n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"%s\" % True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.insert(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = deque([1, 2, 3], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.append(3)\n",
    "x.append(3)\n",
    "x.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f829244df969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_false\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'select'"
     ]
    }
   ],
   "source": [
    " result = tf.select(pred, val_if_true, val_if_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.bool, (None,))\n",
    "y = tf.cast(x, tf.int32)\n",
    "z = tf.one_hot(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "print(y.eval(feed_dict={x:[True, False, True]}))\n",
    "print(z.eval(feed_dict={x:[True, False, True]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.arange(10).astype(int)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.random.choice(range(0, 5), 3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1, 2, 3, 4], 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "np.append(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
