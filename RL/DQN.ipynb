{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rltensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_data(data, width, height, c_dim=3, is_color=True):\n",
    "    \"\"\"resize data for trainining dcgan\n",
    "    Args:\n",
    "        data: list of image data, each of which has a shape,\n",
    "            (width, height, color_dim) if is_color==True\n",
    "            (width, height) otherwisei\n",
    "    \"\"\"\n",
    "    if is_color:\n",
    "        converted_data = np.array([imresize(d, [width, height]) for d in data\n",
    "                                if (len(d.shape)==3 and d.shape[-1] == c_dim)])\n",
    "    else:\n",
    "        # gray scale data\n",
    "        converted_data = np.array([imresize(d, [width, height]) for d in data\n",
    "                                if (len(d.shape)==2)])\n",
    "    return converted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import join, isfile, isdir\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "class DefaultProcessor(object):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        return np.array(x)\n",
    "    \n",
    "    def tensor_process(self, x):\n",
    "        return x\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return self.input_shape\n",
    "    \n",
    "class AtariProcessor(DefaultProcessor):\n",
    "    def __init__(self, height, width):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.input_shape = (height, width)\n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        x = resize_data([x], self.height, self.width)[0]\n",
    "        x = rgb2gray(x)\n",
    "        return x\n",
    "    \n",
    "    def tensor_process(self, x):\n",
    "        # change to (batch, width, hight, window_length)\n",
    "        return tf.transpose(x, [0, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from logging import getLogger\n",
    "import time\n",
    "from collections import deque\n",
    "from six.moves import xrange\n",
    "\n",
    "from rltensor.memories import SequentialMemory\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, env, conf, sess=None):\n",
    "        if sess is None:\n",
    "            sess = tf.Session()\n",
    "        self.sess = sess\n",
    "        self.conf = conf\n",
    "        self.model_dir = conf[\"model_dir\"]\n",
    "        self.limit = conf[\"memory_limit\"]\n",
    "        self.window_length = conf[\"window_length\"]\n",
    "        self.memory = self._get_memory(self.window_length, self.limit)\n",
    "        self.gamma = conf[\"gamma\"]\n",
    "        self.error_clip = conf[\"error_clip\"]\n",
    "        self.processor = conf[\"processor\"]\n",
    "        self.ep_start = conf[\"ep_start\"]\n",
    "        self.ep_end = conf[\"ep_end\"]\n",
    "        self.t_ep_end = conf[\"t_ep_end\"]\n",
    "        self.t_learn_start = conf[\"t_learn_start\"]\n",
    "        self.t_train_freq = conf[\"t_train_freq\"]\n",
    "        self.t_target_q_update_freq = conf[\"t_target_q_update_freq\"]\n",
    "        # Get input and action dim info from env\n",
    "        self.env = env\n",
    "        self.env_name = conf[\"env_name\"]\n",
    "        self.state_dim = self.processor.get_input_shape()\n",
    "        self.action_dim = env.action_space.n\n",
    "        # configure for learning schedule \n",
    "        self.learning_rate = conf[\"learning_rate\"]\n",
    "        self.learning_rate_minimum = conf[\"learning_rate_minimum\"]\n",
    "        self.learning_rate_decay = conf[\"learning_rate_decay\"]\n",
    "        self.learning_rate_decay_step = conf[\"learning_rate_decay_step\"]\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        # reward is in (min_r, max_r)\n",
    "        self.min_r = conf[\"min_r\"]\n",
    "        self.max_r = conf[\"max_r\"]\n",
    "        self.batch_size = conf[\"batch_size\"]\n",
    "        self.log_freq = conf[\"log_freq\"]\n",
    "        self.avg_length = conf[\"avg_length\"]\n",
    "        # Build tensorflow network\n",
    "        st = time.time()\n",
    "        logger.debug(\"Building tensorflow graph...\")\n",
    "        with self.sess.as_default():\n",
    "            self._build_graph()\n",
    "            self.saver = tf.train.Saver()\n",
    "        logger.debug(\"Finished building tensorflow graph, spent time:\", time.time() - st)\n",
    "        if \"load_file_path\" in conf:\n",
    "            self.load_params(conf[\"load_file_path\"])\n",
    "        \n",
    "    def _get_memory(self, window_length, limit):\n",
    "        return SequentialMemory(window_length, limit)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, t_max, num_max_start_steps=0, save_file_path=None, \n",
    "              load_file_path=None,overwrite=True):\n",
    "        tf.global_variables_initializer().run(session=self.sess)\n",
    "        if load_file_path is not None:\n",
    "            self.load_params(load_file_path)\n",
    "        # Save Model\n",
    "        self.save_params(save_file_path, overwrite)\n",
    "        # initialize target netwoork\n",
    "        self.update_target_q_network()\n",
    "        # initialize enviroment\n",
    "        observation = self.env.reset()\n",
    "        action = env.action_space.sample()\n",
    "        # Perform random starts at beginning of episode and do not record them into the experience.\n",
    "        # This slightly changes the start position between games.\n",
    "        if num_max_start_steps == 0:\n",
    "            num_random_start_steps = 0\n",
    "        else:\n",
    "            num_random_start_steps = np.random.randint(num_max_start_steps)\n",
    "        for _ in range(num_random_start_steps):\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            observation = deepcopy(observation)\n",
    "        # initialize memory\n",
    "        observation = self.processor.preprocess(observation)\n",
    "        self.memory.append(observation=observation, reward=0,\n",
    "                              action=action, terminal=False, training=True)\n",
    "        # accumulate results\n",
    "        total_reward = deque(maxlen=self.avg_length)\n",
    "        total_loss = deque(maxlen=self.avg_length)\n",
    "        total_q_val = deque(maxlen=self.avg_length)\n",
    "        ep_rewards = []\n",
    "        ep_losses = []\n",
    "        ep_q_vals = []\n",
    "        ep_actions = []\n",
    "        num_ep = 1\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        for t in tqdm(xrange(t_max)):\n",
    "            try:\n",
    "                # 1. predict\n",
    "                state = self.memory.get_recent_state()\n",
    "                action = self.predict(state)\n",
    "                # 2. act\n",
    "                observation, reward, terminal, info = self.env.step(action)\n",
    "                observation = self.processor.preprocess(observation)\n",
    "                # 3. store data and train network\n",
    "                if t < self.t_learn_start:\n",
    "                    result = self.observe(observation, reward, action, terminal, False)\n",
    "                    continue\n",
    "                else:\n",
    "                    result = self.observe(observation, reward, action, terminal, True)\n",
    "                q, loss, is_update = result\n",
    "                logger.debug(\"a: %d, r:%f, t:%s, q:%.4f, l: %.4f\" % \\\n",
    "                    (action, reward, terminal, np.mean(q), loss))\n",
    "                # Update step\n",
    "                self.sess.run(self.step_update_op)\n",
    "                step = self.global_step.eval(session=self.sess)\n",
    "                # Update target network\n",
    "                if (step + 1) % self.t_target_q_update_freq == 0:\n",
    "                    self.update_target_q_network()\n",
    "                # update statistics\n",
    "                total_reward.append(reward)\n",
    "                total_loss.append(loss)\n",
    "                total_q_val.append(np.mean(q))\n",
    "                ep_actions.append(action)\n",
    "                ep_rewards.append(reward)\n",
    "                ep_losses.append(loss)\n",
    "                ep_q_vals.append(np.mean(q))\n",
    "                # Write summary\n",
    "                if step % self.log_freq == 0:\n",
    "                    epsilon = self.epsilon.eval(session=self.sess)\n",
    "                    learning_rate = self.learning_rate_op.eval(session=self.sess)\n",
    "                    avg_r = np.mean(total_reward)\n",
    "                    avg_loss = np.mean(total_loss)\n",
    "                    avg_q_val = np.mean(total_q_val)\n",
    "                    try:\n",
    "                        max_ep_reward = np.max(ep_rewards)\n",
    "                        min_ep_reward = np.min(ep_rewards)\n",
    "                        avg_ep_reward = np.mean(ep_rewards)\n",
    "                    except:\n",
    "                        max_ep_reward, min_ep_reward, avg_ep_reward = 0, 0, 0\n",
    "                    tag_dict = {'average.reward': avg_r,\n",
    "                                'average.loss': avg_loss,\n",
    "                                'average.q': avg_q_val, \n",
    "                                'episode.max_reward': max_ep_reward, \n",
    "                                'episode.min_reward': min_ep_reward,\n",
    "                                'episode.avg_reward': avg_ep_reward, \n",
    "                                'episode.num_of_game': num_ep,\n",
    "                                'training.epsilon': epsilon,\n",
    "                                'training.learning_rate': learning_rate,\n",
    "                                'episode.rewards': ep_rewards,\n",
    "                                'episode.actions': ep_actions}\n",
    "                    self._inject_summary(tag_dict)\n",
    "            \n",
    "                if terminal:\n",
    "                    observation = self.env.reset()\n",
    "                    observation = self.processor.preprocess(observation)\n",
    "                    self.memory.reset()\n",
    "                    self.memory.append(observation=observation, reward=0,\n",
    "                                   action=action, terminal=False, training=True)\n",
    "                    ep_rewards = []\n",
    "                    ep_losses = []\n",
    "                    ep_q_vals = []\n",
    "                    ep_actions = []\n",
    "                    num_ep += 1\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "        # Update parameters before finishing\n",
    "        self.save_params(save_file_path, True)\n",
    "\n",
    "    def predict(self, s_t, ep):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _get_learning_rate(self):\n",
    "        learning_rate_op = tf.maximum(self.learning_rate_minimum,\n",
    "          tf.train.exponential_decay(\n",
    "              self.learning_rate,\n",
    "              self.global_step,\n",
    "              self.learning_rate_decay_step,\n",
    "              self.learning_rate_decay,\n",
    "              staircase=True))\n",
    "        return learning_rate_op\n",
    "    \n",
    "    def _get_epsilon(self):\n",
    "        rest_steps  = tf.maximum(0., \n",
    "            self.t_ep_end - tf.maximum(0., tf.cast(self.global_step - self.t_learn_start, tf.float32)))\n",
    "        delta_ep = max(0, self.ep_start - self.ep_end)\n",
    "        epsilon = self.ep_end + delta_ep * rest_steps / self.t_ep_end\n",
    "        return epsilon\n",
    "\n",
    "    def update_target_q_network(self):\n",
    "        self.sess.run(self.update_op)\n",
    "        \n",
    "    def _build_summaries(self):\n",
    "        self.writer = tf.summary.FileWriter(self.model_dir, self.sess.graph)\n",
    "        self.summary_placeholders = {}\n",
    "        self.summary_ops = {}\n",
    "        scalar_summary_tags = [\n",
    "                'average.reward', 'average.loss', 'average.q', \n",
    "                'episode.max_reward', 'episode.min_reward', 'episode.avg_reward', \n",
    "                'episode.num_of_game', 'training.epsilon', 'training.learning_rate']\n",
    "        for tag in scalar_summary_tags:\n",
    "            self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag] =\\\n",
    "                tf.summary.scalar(\"%s/%s\" % (self.env_name, tag), self.summary_placeholders[tag])\n",
    "        \n",
    "        histogram_summary_tags = ['episode.rewards', 'episode.actions']\n",
    "        for tag in histogram_summary_tags:\n",
    "            self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag]  = tf.summary.histogram(tag, self.summary_placeholders[tag])\n",
    "    \n",
    "    def _inject_summary(self, tag_dict):\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dict.keys()], {\n",
    "          self.summary_placeholders[tag]: value for tag, value in tag_dict.items()\n",
    "        })\n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str, step)\n",
    "\n",
    "    def load_params(self, file_path):\n",
    "        \"\"\"Loads parameters of an estimator from a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: str, The path to the file.\n",
    "        \"\"\"\n",
    "        self.saver.restore(self.sess, file_path)\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "    def save_params(self, file_path=None, overwrite=True):\n",
    "        \"\"\"Saves parameters of an estimator as a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: str, The path to where the parameters should be saved.\n",
    "            overwrite: bool, If `False` and `file_path` already exists, raises an error.\n",
    "        \"\"\"\n",
    "        if file_path is None:\n",
    "            if not os.path.isdir(\"params\"):\n",
    "                os.mkdir(\"params\")\n",
    "            file_path = \"params/model.ckpt\"\n",
    "        if not overwrite:\n",
    "            _path = \".\".join([file_path, \"meta\"])\n",
    "            if os.path.isfile(_path):\n",
    "                raise NameError(\"%s already exists.\" % file_path)\n",
    "        save_path = self.saver.save(self.sess, file_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from logging import getLogger\n",
    "import random\n",
    "\n",
    "# from .agent import Agent\n",
    "from rltensor.utils import get_shape\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class DQN(Agent):\n",
    "    def __init__(self, env, conf, q_network_cls, sess=None):\n",
    "        self.q_network_cls = q_network_cls\n",
    "        super(DQN, self).__init__(env, conf, sess)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        self.step_update_op = tf.assign(self.global_step, self.global_step + 1)\n",
    "        # training flag\n",
    "        self.training = tf.placeholder(tf.bool, name=\"training\")\n",
    "        # state shape has to be (batch, length,) + input_dim\n",
    "        self.state = tf.placeholder(tf.float32,\n",
    "                                     get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                     name='state')\n",
    "        _state = self.processor.tensor_process(self.state)\n",
    "        self.target_state = tf.placeholder(tf.float32,\n",
    "                                            get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                            name='target_state')\n",
    "        _target_state = self.processor.tensor_process(self.target_state)\n",
    "        # Employ maximal strategy\n",
    "        self.q_network = self.q_network_cls(self.conf[\"q_conf\"], scope_name=\"q_network\")\n",
    "        self.q_val = self.q_network(_state, self.training)\n",
    "        assert self.q_val.get_shape().as_list()[-1] == self.action_dim\n",
    "        self.max_action = tf.argmax(self.q_val, dimension=1)\n",
    "        # Build action graph\n",
    "        self.action = tf.placeholder(tf.int32, (None,), name='action')\n",
    "        action_one_hot = tf.one_hot(self.action, depth=self.action_dim)\n",
    "        self.action_q_val = tf.reduce_sum(self.q_val * action_one_hot, axis=1)\n",
    "        # Build target network\n",
    "        self.target_q_network = self.q_network_cls(self.conf[\"q_conf\"], scope_name=\"target_q_network\")\n",
    "        target_q_val = self.target_q_network(_target_state, self.training)\n",
    "        self.reward = tf.placeholder(tf.float32, (None,), name='reward')\n",
    "        max_one_hot = tf.one_hot(self.max_action, depth=self.action_dim)\n",
    "        max_q_val = tf.reduce_sum(self.q_val * max_one_hot, axis=1)\n",
    "        target_val = self.reward  + self.gamma * max_q_val\n",
    "        # Clip error to stabilize learning\n",
    "        delta = target_val - self.action_q_val\n",
    "        clipped_error = tf.where(tf.abs(delta) < self.error_clip,\n",
    "                                    0.5 * tf.square(delta),\n",
    "                                    tf.abs(delta), name='clipped_error')\n",
    "        self.loss = tf.reduce_mean(clipped_error, name='loss')\n",
    "        # Build optimization\n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        self.epsilon = self._get_epsilon()\n",
    "        self.loss = tf.reduce_mean(tf.square(target_val - self.action_q_val), name='loss')\n",
    "        self.q_optim = tf.train.AdamOptimizer(self.learning_rate_op)\\\n",
    "            .minimize(self.loss, var_list=self.q_network.variables)\n",
    "        self.update_op = self._get_update_op()\n",
    "        with tf.name_scope('summaries'):\n",
    "            self._build_summaries()\n",
    "\n",
    "    def observe(self, observation, reward, action, terminal, training):\n",
    "        # clip reward into  (min_r, max_r)\n",
    "        reward = max(self.min_r, min(self.max_r, reward))\n",
    "        # We always keep data\n",
    "        self.memory.append(observation, action, reward, terminal, training=training)\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        if (step + 1) % self.t_train_freq:\n",
    "            is_update = True\n",
    "        else:\n",
    "            is_update = False\n",
    "        if training:\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            result = self.q_learning_minibatch(experiences, is_update)\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def q_learning_minibatch(self, experiences, is_update=True):\n",
    "        feed_dict = {\n",
    "            self.state: [experience.state0 for experience in experiences],\n",
    "            self.target_state: [experience.state1 for experience in experiences],\n",
    "            self.reward: [experience.reward for experience in experiences],\n",
    "            self.action: [experience.action for experience in experiences],\n",
    "            self.training: True,\n",
    "        }\n",
    "        if is_update:\n",
    "            self.sess.run(self.q_optim, feed_dict=feed_dict)\n",
    "        q_t, loss = self.sess.run([self.action_q_val, self.loss],\n",
    "                                     feed_dict=feed_dict)\n",
    "        return q_t, loss, is_update\n",
    "    \n",
    "    def predict(self, state):\n",
    "        ep = self.epsilon.eval(session=self.sess)\n",
    "        if random.random() < ep:\n",
    "            action = np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            action = self.sess.run(self.max_action, \n",
    "                                   feed_dict={self.state: [state],\n",
    "                                              self.training: False})[0]\n",
    "        return action\n",
    "    \n",
    "    def _get_update_op(self):\n",
    "        update_op = []\n",
    "        for target_var, var in zip(self.target_q_network.variables, self.q_network.variables):\n",
    "            update_op.append(tf.assign(target_var, var))\n",
    "        return update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-12 23:30:05,296] Making new env: DemonAttack-v0\n",
      "  0%|          | 84/10000000 [00:00<3:19:52, 833.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/memories/utils.py:19: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  7%|▋         | 657351/10000000 [8:19:21<481:37:35,  5.39it/s]  "
     ]
    }
   ],
   "source": [
    "from rltensor.networks import FeedForward\n",
    "import gym\n",
    "env = gym.make('DemonAttack-v0')\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":True, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", 'num_hidden': 6, 'activation': tf.nn.softmax}\n",
    "        ],\n",
    "        \"memory_limit\": 1000000,\n",
    "        \"window_length\": 4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"learning_rate_minimum\": 2.5e-4,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 100,\n",
    "        \"ep\": 1e-3,\n",
    "        \"min_r\": -1,\n",
    "        \"max_r\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"error_clip\": 1.0,\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "        \"t_learn_start\": 100,\n",
    "        \"t_train_freq\": 4,\n",
    "        \"t_target_q_update_freq\": 10000,\n",
    "        \"ep_start\": 1.0,\n",
    "        \"ep_end\": 0.1,\n",
    "        \"t_ep_end\": int(1e6),\n",
    "        \"model_dir\": \"./logs/dqn\",\n",
    "        \"log_freq\": 1000,\n",
    "        \"avg_length\": 10000,\n",
    "        \"env_name\": 'DemonAttack-v0'\n",
    "}\n",
    "# logger.setLevel(\"DEBUG\")\n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(env, conf, q_network_cls=FeedForward)\n",
    "dqn.train(int(1e7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2103"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.memory.nb_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4, 0, 2, 2, 3, 1, 0, 1, 1, 0, 3, 0, 4, 0, 3, 0, 2, 0, 1, 1, 2,\n",
       "       2, 3, 4, 2, 3, 3, 3, 0, 0, 1, 2, 4, 2, 2, 1, 0, 0, 2, 1, 4, 4, 2, 0,\n",
       "       4, 4, 2, 3, 3, 1, 4, 4, 3, 3, 0, 2, 0, 4, 1, 2, 3, 4, 4, 3, 2, 2, 0,\n",
       "       1, 4, 4, 1, 3, 1, 3, 2, 1, 2, 2, 4, 0, 2, 2, 0, 3, 3, 2, 0, 1, 0, 0,\n",
       "       3, 2, 4, 3, 4, 3, 1, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "x = tf.Variable(0)\n",
    "y = x + 1\n",
    "# tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "if [1, 2, 3]:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "count = 4\n",
    "while count < 5:\n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%s\" % True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.insert(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = deque([1, 2, 3], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.append(3)\n",
    "x.append(3)\n",
    "x.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
