{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rltensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_data(data, width, height, c_dim=3, is_color=True):\n",
    "    \"\"\"resize data for trainining dcgan\n",
    "    Args:\n",
    "        data: list of image data, each of which has a shape,\n",
    "            (width, height, color_dim) if is_color==True\n",
    "            (width, height) otherwisei\n",
    "    \"\"\"\n",
    "    if is_color:\n",
    "        converted_data = np.array([imresize(d, [width, height]) for d in data\n",
    "                                if (len(d.shape)==3 and d.shape[-1] == c_dim)])\n",
    "    else:\n",
    "        # gray scale data\n",
    "        converted_data = np.array([imresize(d, [width, height]) for d in data\n",
    "                                if (len(d.shape)==2)])\n",
    "    return converted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import join, isfile, isdir\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "class DefaultProcessor(object):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def preprocess(self, observation, action, reward, terminal):\n",
    "        return observation, action, reward, terminal\n",
    "    \n",
    "    def tensor_process(self, x):\n",
    "        return x\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return self.input_shape\n",
    "    \n",
    "class AtariProcessor(DefaultProcessor):\n",
    "    def __init__(self, height, width):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.input_shape = (height, width)\n",
    "    \n",
    "    def preprocess(self, observation, action, reward, terminal):\n",
    "        observation = resize_data([observation], self.height, self.width)[0]\n",
    "        observation = rgb2gray(observation)\n",
    "        # Make the same rewards for every games\n",
    "        reward = min(1, max(-1, reward))\n",
    "        return observation, action, reward, terminal\n",
    "    \n",
    "    def tensor_process(self, x):\n",
    "        # change to (batch, width, hight, window_length)\n",
    "        return tf.transpose(x, [0, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "\n",
    "from rltensor.memories import SequentialMemory\n",
    "from rltensor.memories.utils import sample_batch_indexes\n",
    "\n",
    "\n",
    "class PrioritizedMemory(SequentialMemory):\n",
    "    def __init__(self, window_length, limit, alpha=0.5, beta=0.5, annealing_step=1e6, \n",
    "                 epsilon=1e-4, *args, **kwargs):\n",
    "        super(PrioritizedMemory, self).__init__(window_length, limit, *args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_init = beta\n",
    "        self.annealing_step = annealing_step\n",
    "        self.epsilon = epsilon\n",
    "        self.priorities = None\n",
    "        self.sampled_idx = None\n",
    "\n",
    "    def update_weights(self, step, error=None, *args, **kwargs):\n",
    "        self.priorities = np.ones(self.nb_entries) * self.epsilon\n",
    "        self.beta = self._calc_beta(step)\n",
    "        if error is not None:\n",
    "            self.priorities[self.sampled_idx] = np.abs(error) + self.epsilon\n",
    "            \n",
    "    def add_weights(self):\n",
    "        while len(self.priorities) < self.nb_entries:\n",
    "            self.priorities = np.append(self.priorities, np.max(self.priorities))\n",
    "        \n",
    "    def get_weights(self):\n",
    "        if self.priorities == None:\n",
    "            return None\n",
    "        else:\n",
    "            weights = self.priorities**self.alpha\n",
    "            return weights / np.sum(weights)\n",
    "        \n",
    "    def get_importance_weights(self, batch_size=None):\n",
    "        if self.priorities is None:\n",
    "            return np.ones(batch_size)\n",
    "        else:\n",
    "            weights = self.priorities[self.sampled_idx] ** (-self.alpha*self.beta)\n",
    "            return weights / np.max(weights)\n",
    "    \n",
    "    def _calc_beta(self, step):\n",
    "        return self.beta_init + (1 - self.beta_init) * (self.annealing_step - step) / self.annealing_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from logging import getLogger\n",
    "import time\n",
    "from collections import deque\n",
    "from six.moves import xrange\n",
    "from gym import wrappers \n",
    "\n",
    "from rltensor.memories import SequentialMemory\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, env, conf, sess=None):\n",
    "        if sess is None:\n",
    "            sess = tf.Session()\n",
    "        self.sess = sess\n",
    "        self.conf = conf\n",
    "        self.model_dir = conf[\"model_dir\"]\n",
    "        self.limit = conf[\"memory_limit\"]\n",
    "        self.window_length = conf[\"window_length\"]\n",
    "        self.memory = self._get_memory(self.window_length, self.limit, conf[\"prioritized\"])\n",
    "        self.gamma = conf[\"gamma\"]\n",
    "        self.error_clip = conf[\"error_clip\"]\n",
    "        self.processor = conf[\"processor\"]\n",
    "        self.ep_start = conf[\"ep_start\"]\n",
    "        self.ep_end = conf[\"ep_end\"]\n",
    "        self.t_ep_end = conf[\"t_ep_end\"]\n",
    "        self.t_learn_start = conf[\"t_learn_start\"]\n",
    "        self.t_train_freq = conf[\"t_train_freq\"]\n",
    "        self.t_target_q_update_freq = conf[\"t_target_q_update_freq\"]\n",
    "        # Get input and action dim info from env\n",
    "        self.env = env\n",
    "        self.env_name = conf[\"env_name\"]\n",
    "        self.state_dim = self.processor.get_input_shape()\n",
    "        self.action_dim = env.action_space.n\n",
    "        # configure for learning schedule \n",
    "        self.learning_rate = conf[\"learning_rate\"]\n",
    "        self.learning_rate_minimum = conf[\"learning_rate_minimum\"]\n",
    "        self.learning_rate_decay = conf[\"learning_rate_decay\"]\n",
    "        self.learning_rate_decay_step = conf[\"learning_rate_decay_step\"]\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        # reward is in (min_r, max_r)\n",
    "        self.min_r = conf[\"min_r\"]\n",
    "        self.max_r = conf[\"max_r\"]\n",
    "        self.batch_size = conf[\"batch_size\"]\n",
    "        self.log_freq = conf[\"log_freq\"]\n",
    "        self.avg_length = conf[\"avg_length\"]\n",
    "        # Build tensorflow network\n",
    "        st = time.time()\n",
    "        logger.debug(\"Building tensorflow graph...\")\n",
    "        with self.sess.as_default():\n",
    "            self._build_graph()\n",
    "            self.saver = tf.train.Saver()\n",
    "        logger.debug(\"Finished building tensorflow graph, spent time:\", time.time() - st)\n",
    "        if \"load_file_path\" in conf:\n",
    "            self.load_params(conf[\"load_file_path\"])\n",
    "        \n",
    "    def _get_memory(self, window_length, limit, is_prioritized=True):\n",
    "        if is_prioritized:\n",
    "            return PrioritizedMemory(window_length, limit)\n",
    "        else:\n",
    "            return SequentialMemory(window_length, limit)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, t_max, num_max_start_steps=0, save_file_path=None, \n",
    "              load_file_path=None, save_video_path=None, overwrite=True, render_freq=None):\n",
    "        tf.global_variables_initializer().run(session=self.sess);\n",
    "        if load_file_path is not None:\n",
    "            self.load_params(load_file_path)\n",
    "        # Save Model\n",
    "        self.save_params(save_file_path, overwrite)\n",
    "        # Record Viodeo\n",
    "        if save_video_path is not None:\n",
    "            self.env = wrappers.Monitor(self.env, save_video_path, force=overwrite)\n",
    "        # initialize target netwoork\n",
    "        self.update_target_q_network()\n",
    "        # initialize enviroment\n",
    "        observation = self.env.reset()\n",
    "        action = env.action_space.sample()\n",
    "        # Perform random starts at beginning of episode and do not record them into the experience.\n",
    "        # This slightly changes the start position between games.\n",
    "        if num_max_start_steps == 0:\n",
    "            num_random_start_steps = 0\n",
    "        else:\n",
    "            num_random_start_steps = np.random.randint(num_max_start_steps)\n",
    "        for _ in xrange(num_random_start_steps):\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, terminal, info = env.step(action)\n",
    "            observation = deepcopy(observation)\n",
    "        # initialize memory\n",
    "        terminal = False\n",
    "        reward = 0\n",
    "        observation, action, reward, terminal = self.processor.preprocess(observation, action, reward, terminal)\n",
    "        self.memory.append(observation, action, reward, terminal, is_store=True)\n",
    "        # accumulate results\n",
    "        total_reward = deque(maxlen=self.avg_length)\n",
    "        total_loss = deque(maxlen=self.avg_length)\n",
    "        total_q_val = deque(maxlen=self.avg_length)\n",
    "        ep_rewards = []\n",
    "        ep_losses = []\n",
    "        ep_q_vals = []\n",
    "        ep_actions = []\n",
    "        num_ep = 1\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        # for t in tqdm(xrange(t_max)):\n",
    "        st = time.time()\n",
    "        _st = st\n",
    "        for t in tqdm(xrange(t_max)): \n",
    "            try:\n",
    "                # 1. predict\n",
    "                state = self.memory.get_recent_state()\n",
    "                action = self.predict(state)\n",
    "                # 2. act\n",
    "                observation, reward, terminal, info = self.env.step(action)\n",
    "                observation, action, reward, terminal\\\n",
    "                    = self.processor.preprocess(observation, action, reward, terminal)\n",
    "                # 3. store data and train network\n",
    "                if t < self.t_learn_start:\n",
    "                    result = self.observe(observation, action, reward, terminal, False)\n",
    "                    self.memory.update_weights(step)\n",
    "                    continue\n",
    "                else:\n",
    "                    result = self.observe(observation, action, reward, terminal, True)\n",
    "                q, loss, error, is_update = result\n",
    "                logger.debug(\"a: %d, r:%f, t:%s, q:%.4f, l: %.4f\" % \\\n",
    "                    (action, reward, terminal, np.mean(q), loss))\n",
    "                # Update step\n",
    "                self.sess.run(self.step_update_op);\n",
    "                step = self.global_step.eval(session=self.sess)\n",
    "                self.memory.update_weights(step, error)\n",
    "                # Update target network\n",
    "                if (step + 1) % self.t_target_q_update_freq == 0:\n",
    "                    self.update_target_q_network()\n",
    "                # update statistics\n",
    "                total_reward.append(reward)\n",
    "                total_loss.append(loss)\n",
    "                total_q_val.append(np.mean(q))\n",
    "                ep_actions.append(action)\n",
    "                ep_rewards.append(reward)\n",
    "                ep_losses.append(loss)\n",
    "                ep_q_vals.append(np.mean(q))\n",
    "                # Visualize reuslts\n",
    "                if render_freq is not None:\n",
    "                    if step % render_freq == 0:\n",
    "                        self.env.render()\n",
    "                # Write summary\n",
    "                if step % self.log_freq == 0:\n",
    "                    num_per_sec = self.log_freq / (time.time() - _st)\n",
    "                    _st = time.time()\n",
    "                    epsilon = self.epsilon.eval(session=self.sess)\n",
    "                    learning_rate = self.learning_rate_op.eval(session=self.sess)\n",
    "                    avg_r = np.mean(total_reward)\n",
    "                    avg_loss = np.mean(total_loss)\n",
    "                    avg_q_val = np.mean(total_q_val)\n",
    "                    tag_dict = {'episode.num_of_game': num_ep,\n",
    "                                'average.reward': avg_r,\n",
    "                                'average.loss': avg_loss,\n",
    "                                'average.q': avg_q_val, \n",
    "                                'training.epsilon': epsilon,\n",
    "                                'training.learning_rate': learning_rate,\n",
    "                                'training.num_step_per_sec': num_per_sec,\n",
    "                                'training.time': time.time() - st\n",
    "                                }\n",
    "                    self._inject_summary(tag_dict, step)\n",
    "            \n",
    "                if terminal:\n",
    "                    try:\n",
    "                        cum_ep_reward = np.sum(ep_rewards)\n",
    "                        max_ep_reward = np.max(ep_rewards)\n",
    "                        min_ep_reward = np.min(ep_rewards)\n",
    "                        avg_ep_reward = np.mean(ep_rewards)\n",
    "                    except:\n",
    "                        cum_ep_reward, max_ep_reward, min_ep_reward, avg_ep_reward = 0, 0, 0, 0\n",
    "                    tag_dict = {'episode.cumulative_reward': cum_ep_reward,\n",
    "                                'episode.max_reward': max_ep_reward, \n",
    "                                'episode.min_reward': min_ep_reward,\n",
    "                                'episode.avg_reward': avg_ep_reward, \n",
    "                                'episode.rewards': ep_rewards,\n",
    "                                # 'episode.actions': ep_actions}\n",
    "                                'episode.actions': self.memory.priorities}\n",
    "                    self._inject_summary(tag_dict, num_ep)\n",
    "                    observation = self.env.reset()\n",
    "                    observation, action, reward, terminal\\\n",
    "                        = self.processor.preprocess(observation, None, 0, False)\n",
    "                    self.memory.reset()\n",
    "                    self.memory.append(observation, action, reward, terminal, is_store=False)\n",
    "                    ep_rewards = []\n",
    "                    ep_losses = []\n",
    "                    ep_q_vals = []\n",
    "                    ep_actions = []\n",
    "                    num_ep += 1\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "        # Update parameters before finishing\n",
    "        self.save_params(save_file_path, True)\n",
    "\n",
    "    def predict(self, s_t, ep):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _get_learning_rate(self):\n",
    "        learning_rate_op = tf.maximum(self.learning_rate_minimum,\n",
    "          tf.train.exponential_decay(\n",
    "              self.learning_rate,\n",
    "              self.global_step,\n",
    "              self.learning_rate_decay_step,\n",
    "              self.learning_rate_decay,\n",
    "              staircase=True))\n",
    "        return learning_rate_op\n",
    "    \n",
    "    def _get_epsilon(self):\n",
    "        rest_steps  = tf.maximum(0., \n",
    "            self.t_ep_end - tf.maximum(0., tf.cast(self.global_step - self.t_learn_start, tf.float32)))\n",
    "        delta_ep = max(0, self.ep_start - self.ep_end)\n",
    "        epsilon = self.ep_end + delta_ep * rest_steps / self.t_ep_end\n",
    "        return epsilon\n",
    "\n",
    "    def update_target_q_network(self):\n",
    "        self.sess.run(self.update_op);\n",
    "        \n",
    "    def _build_summaries(self):\n",
    "        self.writer = tf.summary.FileWriter(self.model_dir, self.sess.graph)\n",
    "        self.summary_placeholders = {}\n",
    "        self.summary_ops = {}\n",
    "        scalar_summary_tags = [\n",
    "                'average.reward', 'average.loss', 'average.q', 'episode.cumulative_reward', \n",
    "                'episode.max_reward', 'episode.min_reward', 'episode.avg_reward', \n",
    "                'episode.num_of_game', 'training.epsilon', 'training.learning_rate',\n",
    "                'training.num_step_per_sec', 'training.time']\n",
    "        for tag in scalar_summary_tags:\n",
    "            self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag] =\\\n",
    "                tf.summary.scalar(\"%s/%s\" % (self.env_name, tag), self.summary_placeholders[tag])\n",
    "        \n",
    "        histogram_summary_tags = ['episode.rewards', 'episode.actions']\n",
    "        for tag in histogram_summary_tags:\n",
    "            self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag]  = tf.summary.histogram(tag, self.summary_placeholders[tag])\n",
    "    \n",
    "    def _inject_summary(self, tag_dict, step):\n",
    "        summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dict.keys()], {\n",
    "          self.summary_placeholders[tag]: value for tag, value in tag_dict.items()\n",
    "        })\n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str, step)\n",
    "\n",
    "    def load_params(self, file_path):\n",
    "        \"\"\"Loads parameters of an estimator from a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: str, The path to the file.\n",
    "        \"\"\"\n",
    "        self.saver.restore(self.sess, file_path)\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "    def save_params(self, file_path=None, overwrite=True):\n",
    "        \"\"\"Saves parameters of an estimator as a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: str, The path to where the parameters should be saved.\n",
    "            overwrite: bool, If `False` and `file_path` already exists, raises an error.\n",
    "        \"\"\"\n",
    "        if file_path is None:\n",
    "            if not os.path.isdir(\"params\"):\n",
    "                os.mkdir(\"params\")\n",
    "            file_path = \"params/model.ckpt\"\n",
    "        if not overwrite:\n",
    "            _path = \".\".join([file_path, \"meta\"])\n",
    "            if os.path.isfile(_path):\n",
    "                raise NameError(\"%s already exists.\" % file_path)\n",
    "        save_path = self.saver.save(self.sess, file_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from logging import getLogger\n",
    "import random\n",
    "\n",
    "# from .agent import Agent\n",
    "from rltensor.utils import get_shape\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class DQN(Agent):\n",
    "    def __init__(self, env, conf, q_network_cls, sess=None):\n",
    "        self.q_network_cls = q_network_cls\n",
    "        # Strategy configure\n",
    "        self.double_q = conf[\"double_q\"]\n",
    "        super(DQN, self).__init__(env, conf, sess)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        self.step_update_op = tf.assign(self.global_step, self.global_step + 1)\n",
    "        # training flag\n",
    "        self.training = tf.placeholder(tf.bool, name=\"training\")\n",
    "        # state shape has to be (batch, length,) + input_dim\n",
    "        self.state = tf.placeholder(tf.float32,\n",
    "                                     get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                     name='state')\n",
    "        _state = self.processor.tensor_process(self.state)\n",
    "        self.target_state = tf.placeholder(tf.float32,\n",
    "                                            get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                            name='target_state')\n",
    "        _target_state = self.processor.tensor_process(self.target_state)\n",
    "        # Employ maximal strategy\n",
    "        self.q_network = self.q_network_cls(self.action_dim, self.conf[\"q_conf\"], scope_name=\"q_network\")\n",
    "        self.q_val = self.q_network(_state, self.training)\n",
    "        assert self.q_val.get_shape().as_list()[-1] == self.action_dim\n",
    "        self.max_action = tf.argmax(self.q_val, dimension=1)\n",
    "        # Build action graph\n",
    "        self.action = tf.placeholder(tf.int32, (None,), name='action')\n",
    "        action_one_hot = tf.one_hot(self.action, depth=self.action_dim)\n",
    "        self.action_q_val = tf.reduce_sum(self.q_val * action_one_hot, axis=1)\n",
    "        # Build target network\n",
    "        self.target_q_network = self.q_network_cls(self.action_dim, self.conf[\"q_conf\"],\n",
    "                                                   scope_name=\"target_q_network\")\n",
    "        target_q_val = self.target_q_network(_target_state, self.training)\n",
    "        self.reward = tf.placeholder(tf.float32, (None,), name='reward')\n",
    "        if self.double_q:\n",
    "            max_one_hot = tf.one_hot(self.max_action, depth=self.action_dim)\n",
    "            max_q_val = tf.reduce_sum(target_q_val * max_one_hot, axis=1)\n",
    "        else:\n",
    "            max_q_val = tf.reduce_max(target_q_val, axis=1)\n",
    "        # Make sure to have only reward for singla when teminal=False\n",
    "        reward_q = self.reward  + self.gamma * max_q_val\n",
    "        self.terminal = tf.placeholder(tf.bool, (None,), name=\"terminal\")\n",
    "        _target_val = tf.concat([tf.expand_dims(reward_q, 1), tf.expand_dims(self.reward, 1)], axis=1)\n",
    "        onehot_terminal = tf.one_hot(tf.cast(self.terminal, tf.int32),  2)\n",
    "        target_val = tf.reduce_sum(_target_val * onehot_terminal, axis=1)\n",
    "        # Clip error to stabilize learning\n",
    "        self.error = target_val - self.action_q_val\n",
    "        clipped_error = tf.where(tf.abs(self.error) < self.error_clip,\n",
    "                                    0.5 * tf.square(self.error),\n",
    "                                    tf.abs(self.error), name='clipped_error')\n",
    "        self.weights = tf.placeholder(tf.float32, (None,), name=\"importance_weights\")\n",
    "        self.loss = tf.reduce_mean(self.weights * clipped_error, name='loss')\n",
    "        # Build optimization\n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        self.epsilon = self._get_epsilon()\n",
    "        self.loss = tf.reduce_mean(tf.square(target_val - self.action_q_val), name='loss')\n",
    "        self.q_optim = tf.train.AdamOptimizer(self.learning_rate_op)\\\n",
    "            .minimize(self.loss, var_list=self.q_network.variables)\n",
    "        self.update_op = self._get_update_op()\n",
    "        with tf.name_scope('summaries'):\n",
    "            self._build_summaries()\n",
    "\n",
    "    def observe(self, observation, action, reward, terminal, training):\n",
    "        # clip reward into  (min_r, max_r)\n",
    "        reward = max(self.min_r, min(self.max_r, reward))\n",
    "        # We always keep data\n",
    "        self.memory.append(observation, action, reward, terminal, is_store=True)\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        if (step + 1) % self.t_train_freq:\n",
    "            is_update = True\n",
    "        else:\n",
    "            is_update = False\n",
    "        if training:\n",
    "            self.memory.add_weights()\n",
    "            weights = self.memory.get_weights()\n",
    "            experiences = self.memory.sample(self.batch_size, weights)\n",
    "            weights = self.memory.get_importance_weights()\n",
    "            if weights is None:\n",
    "                weights = np.ones(self.batch_size)\n",
    "            result = self.q_learning_minibatch(experiences, weights, is_update)\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def q_learning_minibatch(self, experiences, batch_weights, is_update=True):\n",
    "        feed_dict = {\n",
    "            self.state: [experience.state0 for experience in experiences],\n",
    "            self.target_state: [experience.state1 for experience in experiences],\n",
    "            self.reward: [experience.reward for experience in experiences],\n",
    "            self.action: [experience.action for experience in experiences],\n",
    "            self.terminal: [experience.terminal1 for experience in experiences],\n",
    "            self.weights: batch_weights,\n",
    "            self.training: False,\n",
    "        }\n",
    "        if is_update:\n",
    "            self.sess.run(self.q_optim, feed_dict=feed_dict);\n",
    "        q_t, loss, error = self.sess.run([self.action_q_val, self.loss, self.error],\n",
    "                                     feed_dict=feed_dict)\n",
    "        return q_t, loss, error, is_update\n",
    "    \n",
    "    def predict(self, state):\n",
    "        ep = self.epsilon.eval(session=self.sess)\n",
    "        if random.random() < ep:\n",
    "            action = np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            action = self.sess.run(self.max_action, \n",
    "                                   feed_dict={self.state: [state],\n",
    "                                              self.training: False})[0]\n",
    "        return action\n",
    "    \n",
    "    def _get_update_op(self):\n",
    "        update_op = []\n",
    "        for target_var, var in zip(self.target_q_network.variables, self.q_network.variables):\n",
    "            update_op.append(tf.assign(target_var, var))\n",
    "        return update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rltensor.networks import FeedForward\n",
    "\n",
    "class DuelingModel(FeedForward):\n",
    "    def __init__(self, output_dim, model_params=None, scope_name=None, *args, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        if model_params is None:\n",
    "            model_params = mlp_conf[\"model\"]\n",
    "        if scope_name is None:\n",
    "            scope_name = \"dueling\"\n",
    "        super().__init__(model_params, scope_name)\n",
    "        self.feature_model = FeedForward(model_params, scope_name=\"feature_network\")\n",
    "        self.advantage_model = FeedForward([{\"name\": \"dense\", \"num_hidden\": 512},\n",
    "                                         {\"name\": \"dense\", \"num_hidden\": output_dim},],\n",
    "                                          scope_name=\"advantage_network\")\n",
    "        self.state_model = FeedForward([{\"name\": \"dense\", \"num_hidden\": 512},\n",
    "                                        {\"name\": \"dense\", \"num_hidden\": 1},],\n",
    "                                         scope_name=\"state_network\")\n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        with tf.variable_scope(self.scope_name, reuse=self.reuse):\n",
    "            x = self.feature_model(x, training)\n",
    "            advantage = self.advantage_model(x, training)\n",
    "            state = self.state_model(x, training)\n",
    "            if self.reuse is False:\n",
    "                self.global_scope_name = tf.get_variable_scope().name\n",
    "                self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.global_scope_name)\n",
    "            mean_advantage = tf.concat([tf.reduce_mean(advantage, axis=1, keep_dims=True)\n",
    "                                        for _ in range(self.output_dim)], \n",
    "                                       axis=1)\n",
    "            advantage = advantage - mean_advantage\n",
    "            state = tf.concat([state for _ in range(self.output_dim)], axis=1)\n",
    "        self.reuse = True\n",
    "        return state + advantage\n",
    "            \n",
    "            \n",
    "class MLPModel(FeedForward):\n",
    "    def __init__(self, output_dim, model_params=None, scope_name=None, *args, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        if model_params is None:\n",
    "            model_params = mlp_conf[\"model\"]\n",
    "        if scope_name is None:\n",
    "            scope_name = \"mlp\"\n",
    "        model_params.append({\"name\": \"dense\", \"num_hidden\": output_dim})\n",
    "        super().__init__(model_params, scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-20 01:29:18,539] Making new env: DemonAttack-v0\n",
      "[2017-07-20 01:29:19,632] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/tomoaki/work/Development/RL/videos')\n",
      "[2017-07-20 01:29:19,976] Clearing 6 monitor files from previous run (because force=True was provided)\n",
      "[2017-07-20 01:29:19,991] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000000.mp4\n",
      "\n",
      "\n",
      "  0%|          | 0/10000000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 67/10000000 [00:00<4:09:42, 667.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "\n",
      "  0%|          | 101/10000000 [00:00<6:13:22, 446.37it/s]\u001b[A\n",
      "  0%|          | 123/10000000 [00:00<20:05:07, 138.30it/s]\u001b[A\n",
      "  0%|          | 2775/10000000 [00:49<56:24:29, 49.23it/s][2017-07-20 01:30:09,713] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000001.mp4\n",
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  0%|          | 15077/10000000 [04:43<54:01:53, 51.33it/s][2017-07-20 01:34:04,151] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000008.mp4\n",
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  0%|          | 44030/10000000 [14:44<59:51:31, 46.20it/s][2017-07-20 01:44:04,700] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000027.mp4\n",
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  1%|          | 93585/10000000 [34:31<72:14:58, 38.09it/s][2017-07-20 02:03:51,946] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000064.mp4\n",
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  2%|▏         | 171929/10000000 [1:10:29<78:26:44, 34.80it/s][2017-07-20 02:39:49,792] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000125.mp4\n",
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  3%|▎         | 269917/10000000 [1:55:50<76:14:20, 35.45it/s] [2017-07-20 03:25:10,855] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000216.mp4\n",
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  4%|▍         | 384153/10000000 [2:48:44<73:41:13, 36.25it/s] [2017-07-20 04:18:04,676] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000343.mp4\n",
      "  4%|▍         | 384157/10000000 [2:48:44<108:19:42, 24.66it/s]/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  5%|▌         | 548771/10000000 [4:05:36<72:37:24, 36.15it/s] [2017-07-20 05:34:56,126] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000512.mp4\n",
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  8%|▊         | 765765/10000000 [5:48:05<71:25:14, 35.91it/s] [2017-07-20 07:17:25,877] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.1.21792.video000729.mp4\n",
      "  8%|▊         | 765769/10000000 [5:48:05<97:12:15, 26.39it/s]/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  9%|▉         | 876713/10000000 [6:40:56<73:33:16, 34.45it/s]"
     ]
    }
   ],
   "source": [
    "from rltensor.networks import FeedForward\n",
    "import gym\n",
    "\n",
    "env = gym.make('DemonAttack-v0')\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":True, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        'double_q': True,\n",
    "        \"memory_limit\": 100000,\n",
    "        \"window_length\": 4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"learning_rate_minimum\": 2.5e-4,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 100,\n",
    "        \"ep\": 1e-3,\n",
    "        \"min_r\": -1,\n",
    "        \"max_r\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"error_clip\": 1.0,\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "        \"t_learn_start\": 100,\n",
    "        \"t_train_freq\": 4,\n",
    "        \"t_target_q_update_freq\": 10000,\n",
    "        \"ep_start\": 1.0,\n",
    "        \"ep_end\": 0.1,\n",
    "        \"t_ep_end\": int(1e6),\n",
    "        \"model_dir\": \"./logs/dqn\",\n",
    "        \"log_freq\": 1000,\n",
    "        \"avg_length\": 10000,\n",
    "        \"env_name\": 'DemonAttack-v0',\n",
    "        \"prioritized\": True,\n",
    "}\n",
    "# logger.setLevel(\"DEBUG\")\n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(env, conf, q_network_cls=DuelingModel)\n",
    "dqn.train(int(1e7), render_freq=None, save_video_path=\"./videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(dqn.memory.priorities, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [1, 2, 3]:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 4\n",
    "while count < 5:\n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"%s\" % True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.insert(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = deque([1, 2, 3], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.append(3)\n",
    "x.append(3)\n",
    "x.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f829244df969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_false\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'select'"
     ]
    }
   ],
   "source": [
    " result = tf.select(pred, val_if_true, val_if_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.bool, (None,))\n",
    "y = tf.cast(x, tf.int32)\n",
    "z = tf.one_hot(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "print(y.eval(feed_dict={x:[True, False, True]}))\n",
    "print(z.eval(feed_dict={x:[True, False, True]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.arange(10).astype(int)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.random.choice(range(0, 5), 3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1, 2, 3, 4], 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "np.append(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
