{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "df = pd.read_excel(\"data/stock_data.xlsx\", sheetname=names, index_col=0)\n",
    "panel_stock = pd.Panel.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from logging import getLogger\n",
    "import time\n",
    "from collections import deque\n",
    "from six.moves import xrange\n",
    "from gym import wrappers \n",
    "from copy import deepcopy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from rltensor.memories import SequentialMemory, PrioritizedMemory\n",
    "from rltensor.params import default_conf as d_conf\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, env, conf, default_conf=None, sess=None, *args, **kwargs):\n",
    "        if sess is None:\n",
    "            sess = tf.Session()\n",
    "        self.sess = sess\n",
    "        if default_conf is None:\n",
    "            default_conf = d_conf\n",
    "        self.default_conf = default_conf\n",
    "        conf = self._set_conf(conf)\n",
    "        self.conf = conf\n",
    "        self.model_dir = conf[\"model_dir\"]\n",
    "        self.limit = conf[\"memory_limit\"]\n",
    "        self.window_length = conf[\"window_length\"]\n",
    "        self.memory = self._get_memory(self.window_length, self.limit, conf[\"prioritized\"])\n",
    "        self.gamma = conf[\"gamma\"]\n",
    "        self.error_clip = conf[\"error_clip\"]\n",
    "        self.processor = conf[\"processor\"]\n",
    "        self.ep_start = conf[\"ep_start\"]\n",
    "        self.ep_end = conf[\"ep_end\"]\n",
    "        self.t_ep_end = conf[\"t_ep_end\"]\n",
    "        self.t_learn_start = conf[\"t_learn_start\"]\n",
    "        self.t_train_freq = conf[\"t_train_freq\"]\n",
    "        self.t_target_q_update_freq = conf[\"t_target_q_update_freq\"]\n",
    "        # Get input and action dim info from env\n",
    "        self.env = env\n",
    "        self.env_name = conf[\"env_name\"]\n",
    "        self.state_dim = self.processor.get_input_shape()\n",
    "        self.action_dim = env.action_space.n\n",
    "        # configure for learning schedule \n",
    "        self.learning_rate = conf[\"learning_rate\"]\n",
    "        self.learning_rate_minimum = conf[\"learning_rate_minimum\"]\n",
    "        self.learning_rate_decay = conf[\"learning_rate_decay\"]\n",
    "        self.learning_rate_decay_step = conf[\"learning_rate_decay_step\"]\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        # reward is in (min_r, max_r)\n",
    "        self.min_r = conf[\"min_r\"]\n",
    "        self.max_r = conf[\"max_r\"]\n",
    "        self.batch_size = conf[\"batch_size\"]\n",
    "        self.log_freq = conf[\"log_freq\"]\n",
    "        self.avg_length = conf[\"avg_length\"]\n",
    "        self.update_op = None\n",
    "        # Build tensorflow network\n",
    "        st = time.time()\n",
    "        logger.debug(\"Building tensorflow graph...\")\n",
    "        with self.sess.as_default():\n",
    "            self.learning_rate_op = self._get_learning_rate()\n",
    "            self.epsilon = self._get_epsilon()\n",
    "            self.step_update_op = tf.assign(self.global_step, self.global_step + 1)\n",
    "            self._build_graph()\n",
    "            self.saver = tf.train.Saver()\n",
    "        logger.debug(\"Finished building tensorflow graph, spent time:\", time.time() - st)\n",
    "        if \"load_file_path\" in conf:\n",
    "            self.load_params(conf[\"load_file_path\"])\n",
    "        \n",
    "    def _get_memory(self, window_length, limit, is_prioritized=True):\n",
    "        if is_prioritized:\n",
    "            return PrioritizedMemory(window_length, limit)\n",
    "        else:\n",
    "            return SequentialMemory(window_length, limit)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, t_max, num_max_start_steps=0, save_file_path=None, \n",
    "              load_file_path=None, save_video_path=None, overwrite=True, render_freq=None):\n",
    "        tf.global_variables_initializer().run(session=self.sess);\n",
    "        if load_file_path is not None:\n",
    "            self.load_params(load_file_path)\n",
    "        # Save Model\n",
    "        self.save_params(save_file_path, overwrite)\n",
    "        # Record Viodeo\n",
    "        if save_video_path is not None:\n",
    "            self.env = wrappers.Monitor(self.env, save_video_path, force=overwrite)\n",
    "        # initialize target netwoork\n",
    "        self.update_target_q_network()\n",
    "        # initialize enviroment\n",
    "        observation = self.env.reset()\n",
    "        action = self.env.action_space.sample()\n",
    "        # Perform random starts at beginning of episode and do not record them into the experience.\n",
    "        # This slightly changes the start position between games.\n",
    "        if num_max_start_steps == 0:\n",
    "            num_random_start_steps = 0\n",
    "        else:\n",
    "            num_random_start_steps = np.random.randint(num_max_start_steps)\n",
    "        for _ in xrange(num_random_start_steps):\n",
    "            action = self.env.action_space.sample()\n",
    "            observation, reward, terminal, info = self.env.step(action)\n",
    "            observation = deepcopy(observation)\n",
    "        # initialize memory\n",
    "        terminal = False\n",
    "        reward = 0\n",
    "        observation, action, reward_, terminal = self.processor.preprocess(observation, action, reward, terminal)\n",
    "        self.memory.append(observation, action, reward_, terminal, is_store=False)\n",
    "        # accumulate results\n",
    "        total_reward = deque(maxlen=self.avg_length)\n",
    "        total_loss = deque(maxlen=self.avg_length)\n",
    "        total_q_val = deque(maxlen=self.avg_length)\n",
    "        ep_rewards = []\n",
    "        ep_losses = []\n",
    "        ep_q_vals = []\n",
    "        ep_actions = []\n",
    "        num_ep = 1\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        # for t in tqdm(xrange(t_max)):\n",
    "        st = time.time()\n",
    "        _st = st\n",
    "        for t in tqdm(xrange(t_max)): \n",
    "            try:\n",
    "                # 1. predict\n",
    "                state = self.memory.get_recent_state()\n",
    "                action = self.predict(state)\n",
    "                # 2. act\n",
    "                observation, reward, terminal, info = self.env.step(action)\n",
    "                observation, action, reward_, terminal\\\n",
    "                    = self.processor.preprocess(observation, action, reward, terminal)\n",
    "                # 3. store data and train network\n",
    "                if t < self.t_learn_start:\n",
    "                    result = self.observe(observation, action, reward_, terminal, False)\n",
    "                    self.memory.update_weights(step)\n",
    "                    continue\n",
    "                else:\n",
    "                    result = self.observe(observation, action, reward_, terminal, True)\n",
    "                q, loss, error, is_update = result\n",
    "                # Update step\n",
    "                if self.update_op is not None:\n",
    "                    self.sess.run(self.step_update_op);\n",
    "                step = self.global_step.eval(session=self.sess)\n",
    "                self.memory.update_weights(step, error)\n",
    "                # Update target network\n",
    "                if (step + 1) % self.t_target_q_update_freq == 0:\n",
    "                    self.update_target_q_network()\n",
    "                # update statistics\n",
    "                total_reward.append(reward)\n",
    "                total_loss.append(loss)\n",
    "                total_q_val.append(np.mean(q))\n",
    "                ep_actions.append(action)\n",
    "                ep_rewards.append(reward)\n",
    "                ep_losses.append(loss)\n",
    "                ep_q_vals.append(np.mean(q))\n",
    "                # Visualize reuslts\n",
    "                if render_freq is not None:\n",
    "                    if step % render_freq == 0:\n",
    "                        self.env.render()\n",
    "                # Write summary\n",
    "                if self.log_freq is not None and step % self.log_freq == 0:\n",
    "                    num_per_sec = self.log_freq / (time.time() - _st)\n",
    "                    _st = time.time()\n",
    "                    epsilon = self.epsilon.eval(session=self.sess)\n",
    "                    learning_rate = self.learning_rate_op.eval(session=self.sess)\n",
    "                    avg_r = np.mean(total_reward)\n",
    "                    avg_loss = np.mean(total_loss)\n",
    "                    avg_q_val = np.mean(total_q_val)\n",
    "                    tag_dict = {'episode.num_of_game': num_ep,\n",
    "                                'average.reward': avg_r,\n",
    "                                'average.loss': avg_loss,\n",
    "                                'average.q': avg_q_val, \n",
    "                                'training.epsilon': epsilon,\n",
    "                                'training.learning_rate': learning_rate,\n",
    "                                'training.num_step_per_sec': num_per_sec,\n",
    "                                'training.time': time.time() - st}\n",
    "                    self._inject_summary(tag_dict, step)\n",
    "            \n",
    "                if terminal:\n",
    "                    try:\n",
    "                        cum_ep_reward = np.sum(ep_rewards)\n",
    "                        max_ep_reward = np.max(ep_rewards)\n",
    "                        min_ep_reward = np.min(ep_rewards)\n",
    "                        avg_ep_reward = np.mean(ep_rewards)\n",
    "                    except:\n",
    "                        cum_ep_reward, max_ep_reward, min_ep_reward, avg_ep_reward = 0, 0, 0, 0\n",
    "                        \n",
    "                    tag_dict = {'episode.cumulative_reward': cum_ep_reward,\n",
    "                                'episode.max_reward': max_ep_reward, \n",
    "                                'episode.min_reward': min_ep_reward,\n",
    "                                'episode.avg_reward': avg_ep_reward, \n",
    "                                'episode.rewards': ep_rewards}\n",
    "                    if hasattr(self.memory, \"priorities\"):\n",
    "                        self.memory['episode.actions'] = self.memory.priorities\n",
    "                    self._inject_summary(tag_dict, num_ep)\n",
    "                    observation = self.env.reset()\n",
    "                    observation, action, reward_, terminal\\\n",
    "                        = self.processor.preprocess(observation, None, 0, False)\n",
    "                    self.memory.reset()\n",
    "                    self.memory.append(observation, action, reward_, terminal, is_store=False)\n",
    "                    ep_rewards = []\n",
    "                    ep_losses = []\n",
    "                    ep_q_vals = []\n",
    "                    ep_actions = []\n",
    "                    num_ep += 1\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "        # Update parameters before finishing\n",
    "        self.save_params(save_file_path, True)\n",
    "        \n",
    "    def play(self, num_episode=1, ep=0.05, overwrite=True, load_file_path=None, save_video_path=None, render_freq=None):\n",
    "        tf.global_variables_initializer().run(session=self.sess);\n",
    "        if load_file_path is not None:\n",
    "            self.load_params(load_file_path)\n",
    "        # Record Viodeo\n",
    "        if save_video_path is not None:\n",
    "            self.env = wrappers.Monitor(self.env, save_video_path, force=overwrite)\n",
    "        for num_ep in range(1, num_episode + 1):\n",
    "            # initialize enviroment\n",
    "            observation = self.env.reset()\n",
    "            self.memory.reset()\n",
    "            action = self.env.action_space.sample()\n",
    "            reward = 0\n",
    "            terminal = False\n",
    "            observation, action, reward_, terminal = self.processor.preprocess(observation, action, reward, terminal)\n",
    "            self.memory.append(observation, action, reward_, terminal, is_store=False)\n",
    "            ep_rewards = []\n",
    "            step = 1\n",
    "            while not terminal:\n",
    "                # 1. predict\n",
    "                state = self.memory.get_recent_state()\n",
    "                action = self.predict(state, ep)\n",
    "                # 2. act\n",
    "                observation, reward, terminal, info = self.env.step(action)\n",
    "                # initialize memory\n",
    "                observation, action, reward_, terminal = self.processor.preprocess(observation, action, reward, terminal)\n",
    "                self.memory.append(observation, action, reward_, terminal, is_store=False)\n",
    "                # accumulate results\n",
    "                ep_rewards.append(reward)\n",
    "                # Visualize reuslts\n",
    "                if render_freq is not None:\n",
    "                    if step % render_freq == 0:\n",
    "                        self.env.render()\n",
    "                if terminal:\n",
    "                    try:\n",
    "                        cum_ep_reward = np.sum(ep_rewards)\n",
    "                        max_ep_reward = np.max(ep_rewards)\n",
    "                        min_ep_reward = np.min(ep_rewards)\n",
    "                        avg_ep_reward = np.mean(ep_rewards)\n",
    "                    except:\n",
    "                        cum_ep_reward, max_ep_reward, min_ep_reward, avg_ep_reward = 0, 0, 0, 0\n",
    "                    tag_dict = {'episode.cumulative_reward': cum_ep_reward,\n",
    "                                'episode.max_reward': max_ep_reward, \n",
    "                                'episode.min_reward': min_ep_reward,\n",
    "                                'episode.avg_reward': avg_ep_reward, \n",
    "                                'episode.rewards': ep_rewards}\n",
    "                    self._inject_summary(tag_dict, num_ep)\n",
    "                step += 1\n",
    "\n",
    "    def predict(self, s_t, ep):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _set_conf(self, conf):\n",
    "        conf = deepcopy(conf)\n",
    "        for key in self.default_conf.keys():\n",
    "            if key not in conf:\n",
    "                conf[key] = self.default_conf[key]\n",
    "        return conf\n",
    "    \n",
    "    def _get_learning_rate(self):\n",
    "        learning_rate_op = tf.maximum(self.learning_rate_minimum,\n",
    "          tf.train.exponential_decay(\n",
    "              self.learning_rate,\n",
    "              self.global_step,\n",
    "              self.learning_rate_decay_step,\n",
    "              self.learning_rate_decay,\n",
    "              staircase=True))\n",
    "        return learning_rate_op\n",
    "    \n",
    "    def _get_epsilon(self):\n",
    "        rest_steps  = tf.maximum(0., \n",
    "            self.t_ep_end - tf.maximum(0., tf.cast(self.global_step - self.t_learn_start, tf.float32)))\n",
    "        delta_ep = max(0, self.ep_start - self.ep_end)\n",
    "        epsilon = self.ep_end + delta_ep * rest_steps / self.t_ep_end\n",
    "        return epsilon\n",
    "\n",
    "    def update_target_q_network(self):\n",
    "        self.sess.run(self.update_op);\n",
    "        \n",
    "    def _build_summaries(self):\n",
    "        self.writer = tf.summary.FileWriter(self.model_dir, self.sess.graph)\n",
    "        self.summary_placeholders = {}\n",
    "        self.summary_ops = {}\n",
    "        scalar_summary_tags = [\n",
    "                'average.reward', 'average.loss', 'average.q', 'episode.cumulative_reward', \n",
    "                'episode.max_reward', 'episode.min_reward', 'episode.avg_reward', \n",
    "                'episode.num_of_game', 'training.epsilon', 'training.learning_rate',\n",
    "                'training.num_step_per_sec', 'training.time']\n",
    "        for tag in scalar_summary_tags:\n",
    "            self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag] =\\\n",
    "                tf.summary.scalar(\"%s/%s\" % (self.env_name, tag), self.summary_placeholders[tag])\n",
    "        \n",
    "        histogram_summary_tags = ['episode.rewards', 'episode.actions']\n",
    "        for tag in histogram_summary_tags:\n",
    "            self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag]  = tf.summary.histogram(tag, self.summary_placeholders[tag])\n",
    "    \n",
    "    def _inject_summary(self, tag_dict, step):\n",
    "        summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dict.keys()], {\n",
    "          self.summary_placeholders[tag]: value for tag, value in tag_dict.items()\n",
    "        })\n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str, step)\n",
    "\n",
    "    def load_params(self, file_path):\n",
    "        \"\"\"Loads parameters of an estimator from a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: str, The path to the file.\n",
    "        \"\"\"\n",
    "        self.saver.restore(self.sess, file_path)\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "    def save_params(self, file_path=None, overwrite=True):\n",
    "        \"\"\"Saves parameters of an estimator as a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: str, The path to where the parameters should be saved.\n",
    "            overwrite: bool, If `False` and `file_path` already exists, raises an error.\n",
    "        \"\"\"\n",
    "        if file_path is None:\n",
    "            if not os.path.isdir(\"params\"):\n",
    "                os.mkdir(\"params\")\n",
    "            file_path = \"params/model.ckpt\"\n",
    "        if not overwrite:\n",
    "            _path = \".\".join([file_path, \"meta\"])\n",
    "            if os.path.isfile(_path):\n",
    "                raise NameError(\"%s already exists.\" % file_path)\n",
    "        save_path = self.saver.save(self.sess, file_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rltensor.environments.core import Env\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "class ActionSpace(object):\n",
    "    n = None\n",
    "    def sample(self):\n",
    "        action = np.random.randn(self.n) ** 2\n",
    "        action = action / np.sum(action)\n",
    "        return action\n",
    "\n",
    "class TradingEnv(Env):\n",
    "    metadata = {'render.modes': []}\n",
    "    reward_range = (-np.inf, np.inf)\n",
    "    \n",
    "    def __init__(self, data,  st=None, end=None):\n",
    "        self.data = data\n",
    "        self.action_space = ActionSpace()\n",
    "        self.action_space.n = data.axes[2].values.shape[0]\n",
    "        if st is None:\n",
    "            st = data.axes[0]\n",
    "        if end is None:\n",
    "            end = data.axes[-1]\n",
    "        self.st = st\n",
    "        self.end = end\n",
    "        self._sepc = None\n",
    "        self.volume = data.ix[\"Volume\"]\n",
    "        self.close = data.ix[\"Close\"]\n",
    "        self.open = data.ix[\"Open\"]\n",
    "        self.high  = data.ix[\"High\"]\n",
    "        self.low = data.ix[\"Low\"]\n",
    "        self.time_idx = self.open.index\n",
    "        self._preprocess()\n",
    "        self.returns = self._calc_returns(self.close)\n",
    "        \n",
    "        \n",
    "    def _step(self, action):\n",
    "        self.time_step += 1\n",
    "        r_t = self.returns.values[self.time_step]\n",
    "        reward = np.dot(r_t, action)\n",
    "        # OHLC x num_stock\n",
    "        observation = np.stack((self.open.values[self.time_step],\n",
    "                                 self.high.values[self.time_step],\n",
    "                                 self.low.values[self.time_step],\n",
    "                                 self.close.values[self.time_step],\n",
    "                                 self.volume.values[self.time_step]), axis=1)\n",
    "        if self.low.shape[0] - 1 <= self.time_step:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        return observation, reward, done, None\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.time_step = 0\n",
    "        observation = np.stack((self.open.values[self.time_step],\n",
    "                                 self.high.values[self.time_step],\n",
    "                                 self.low.values[self.time_step],\n",
    "                                 self.close.values[self.time_step],\n",
    "                                 self.volume.values[self.time_step]), axis=1)\n",
    "        return observation\n",
    "        \n",
    "    def _render(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def _close(self):\n",
    "        pass\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        return time()\n",
    "    \n",
    "    def _calc_returns(self, df):\n",
    "        returns = df.pct_change(1)\n",
    "        returns.values[0] = np.zeros_like(df.values[0])\n",
    "        return returns\n",
    "        \n",
    "    def _preprocess(self):\n",
    "        self.volume = self.volume.replace(np.nan, 0)\n",
    "        cols = self.open.columns.values\n",
    "        init_val = np.array([self.open[c].dropna().values[0] for c in cols])\n",
    "        self.open = self._normalize(self.open, init_val)\n",
    "        self.high = self._normalize(self.high, init_val)\n",
    "        self.low = self._normalize(self.low, init_val)\n",
    "        self.close = self._normalize(self.close, init_val)\n",
    "        \n",
    "    def _normalize(self, df, init_val):\n",
    "        df = df / init_val\n",
    "        df = df.replace(np.nan, 1)\n",
    "        return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv(panel_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rltensor.memories import SequentialMemory\n",
    "# from rltensor.agents import Agent\n",
    "\n",
    "class RandomTradingAgent(Agent):\n",
    "    def __init__(self, env, conf, default_conf=None, sess=None, *args, **kwargs):\n",
    "        super().__init__(env, conf, default_conf, sess, *args, **kwargs)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            self._build_summaries()\n",
    "\n",
    "    def observe(self, observation, action, reward, terminal, training):\n",
    "        # clip reward into  (min_r, max_r)\n",
    "        reward = max(self.min_r, min(self.max_r, reward))\n",
    "        # We always keep data\n",
    "        self.memory.append(observation, action, reward, terminal, is_store=True)\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        return np.zeros(self.batch_size), np.zeros(self.batch_size), np.zeros(self.batch_size), False\n",
    "    \n",
    "    def predict(self, state, ep=None):\n",
    "        action = np.random.randn(self.action_dim) ** 2\n",
    "        action = action / np.sum(action)\n",
    "        return action\n",
    "    \n",
    "    def update_target_q_network(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DefaultProcessor(object):\n",
    "    def __init__(self, input_shape=None):\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def preprocess(self, observation, action, reward, terminal):\n",
    "        return observation, action, reward, terminal\n",
    "    \n",
    "    def tensor_process(self, x):\n",
    "        return x\n",
    "    \n",
    "    def get_input_shape(self):\n",
    "        return self.input_shape\n",
    "    \n",
    "    def get_action_value(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "class TradingProcessor(DefaultProcessor):\n",
    "    def __init__(self, num_stock, num_feature):\n",
    "        self.height = num_stock\n",
    "        self.width = num_feature\n",
    "        self.input_shape = (num_stock, num_feature)\n",
    "    \n",
    "    def preprocess(self, observation, action, reward, terminal):\n",
    "        reward = np.log(1 + reward)\n",
    "        # we use high, low close\n",
    "        observation = observation[:, 1:]\n",
    "        return observation, action, reward, terminal\n",
    "    \n",
    "    def tensor_process(self, x):\n",
    "        return x\n",
    "    \n",
    "    def get_reward(self, state0,  state1, action):\n",
    "        # feature 2 has to be close prices\n",
    "        returns = state1[:, -1, :, 2] / state0[:, -1, :, 2]\n",
    "        return tf.log(tf.reduce_sum(returns * action, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rltensor.memories import SequentialMemory\n",
    "# from rltensor.agents.agent import Agent\n",
    "from rltensor.utils import get_shape\n",
    "from time import time\n",
    "import time\n",
    "\n",
    "class PolicyGradient(Agent):\n",
    "    def __init__(self, env, conf, action_network_cls, default_conf=None, sess=None, *args, **kwargs):\n",
    "        self.action_network_cls = action_network_cls\n",
    "        super().__init__(env, conf, default_conf, sess, *args, **kwargs)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        # training flag\n",
    "        self.training = tf.placeholder(tf.bool, name=\"training\")\n",
    "        # state shape has to be (batch, length,) + input_dim\n",
    "        self.state = tf.placeholder(tf.float32,\n",
    "                                     get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                     name='state')\n",
    "        _state = self.processor.tensor_process(self.state)\n",
    "        self.target_state = tf.placeholder(tf.float32,\n",
    "                                            get_shape(self.state_dim, maxlen=self.window_length),\n",
    "                                            name='target_state')\n",
    "        _target_state = self.processor.tensor_process(self.target_state)\n",
    "        # Employ maximal strategy\n",
    "        self.action_network = self.action_network_cls(self.action_dim, self.conf[\"action_conf\"],\n",
    "                                              scope_name=\"action_network\")\n",
    "        self.action = self.action_network(_state, self.training)\n",
    "        reward = self.processor.get_reward(_state, _target_state, self.action)\n",
    "        self.terminal = tf.placeholder(tf.bool, (None,), name=\"terminal\")\n",
    "        self.loss = tf.reduce_mean(-reward, name='loss')\n",
    "        # Build optimization\n",
    "        self.action_optim = tf.train.AdamOptimizer(self.learning_rate_op)\\\n",
    "            .minimize(self.loss, var_list=self.action_network.variables)\n",
    "        with tf.name_scope('summaries'):\n",
    "            self._build_summaries()\n",
    "\n",
    "    def observe(self, observation, action, reward, terminal, training):\n",
    "        # clip reward into  (min_r, max_r)\n",
    "        reward = max(self.min_r, min(self.max_r, reward))\n",
    "        # We always keep data\n",
    "        self.memory.append(observation, action, reward, terminal, is_store=True)\n",
    "        step = self.global_step.eval(session=self.sess)\n",
    "        if (step + 1) % self.t_train_freq:\n",
    "            is_update = True\n",
    "        else:\n",
    "            is_update = False\n",
    "        if training:\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            result = self.action_learning_minibatch(experiences, is_update)\n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def action_learning_minibatch(self, experiences, batch_weights, is_update=True):\n",
    "        feed_dict = {\n",
    "            self.state: [experience.state0 for experience in experiences],\n",
    "            self.target_state: [experience.state1 for experience in experiences],\n",
    "            self.terminal: [experience.terminal1 for experience in experiences],\n",
    "            self.training: True}\n",
    "        if is_update:\n",
    "            self.sess.run(self.action_optim, feed_dict=feed_dict);\n",
    "        loss = self.sess.run([self.loss], feed_dict=feed_dict)\n",
    "        # To have compatibility with other q-learning, we return pseudo values q, loss, error, is_udpate\n",
    "        return np.zeros(self.batch_size), loss, np.zeros(self.batch_size), is_update\n",
    "    \n",
    "    def predict(self, state, ep=None):\n",
    "        action = self.sess.run(self.action, feed_dict={self.state: [state], self.training: False})[0]\n",
    "        return action\n",
    "    \n",
    "    def update_target_q_network(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Env.__del__ of <__main__.TradingEnv object at 0x7f1e0791ce80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/environments/core.py\", line 179, in __del__\n",
      "TypeError: 'DataFrame' object is not callable\n",
      "Exception ignored in: <bound method Env.__del__ of <__main__.TradingEnv object at 0x7f1e03a1a1d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/rltensor-0.1.0-py3.6.egg/rltensor/environments/core.py\", line 179, in __del__\n",
      "TypeError: 'DataFrame' object is not callable\n",
      "\n",
      "  0%|          | 0/10000000 [00:00<?, ?it/s]\u001b[A/home/tomoaki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in log\n",
      "\n",
      "  0%|          | 34/10000000 [00:00<8:14:33, 336.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n",
      "(501, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "  0%|          | 68/10000000 [00:00<8:13:52, 337.47it/s]\u001b[A\n",
      "  0%|          | 101/10000000 [00:00<12:02:39, 230.63it/s]\u001b[A\n",
      "  0%|          | 118/10000000 [00:01<75:07:32, 36.97it/s] \u001b[A\n",
      "  0%|          | 130/10000000 [00:02<118:40:38, 23.41it/s]\u001b[A\n",
      "  0%|          | 372/10000000 [00:22<224:45:03, 12.36it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-a176cf0c954d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_network_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMLPModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-7bc72f244918>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, t_max, num_max_start_steps, save_file_path, load_file_path, save_video_path, overwrite, render_freq)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Update parameters before finishing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_video_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-7bc72f244918>\u001b[0m in \u001b[0;36msave_params\u001b[0;34m(self, file_path, overwrite)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s already exists.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved in file: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         model_checkpoint_path = sess.run(\n\u001b[1;32m   1471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "  0%|          | 372/10000000 [00:33<246:40:54, 11.26it/s]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from rltensor.networks import MLPModel\n",
    "\n",
    "shape = (env.open.shape[1], 4)\n",
    "conf = {\"action_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(4, 1), \"num_filter\":32, \"stride\":(2, 1),\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(4, 1), \"num_filter\":64, \"stride\":(2, 1),\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":False, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        'double_q': True,\n",
    "        \"memory_limit\": 100000,\n",
    "        \"window_length\": 20,\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"learning_rate_minimum\": 2.5e-4,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 100,\n",
    "        \"min_r\": -np.inf,\n",
    "        \"max_r\": np.inf,\n",
    "        \"batch_size\": 32,\n",
    "        \"t_learn_start\": 100,\n",
    "        \"t_train_freq\": 1,\n",
    "        \"model_dir\": \"./logs/trading\",\n",
    "        \"processor\": TradingProcessor(shape[0], shape[1]),\n",
    "        \"log_freq\": 1000,\n",
    "        \"avg_length\": 10000,\n",
    "        \"env_name\": 'Trading',\n",
    "        \"prioritized\": False\n",
    "}\n",
    "tf.reset_default_graph()\n",
    "agent = PolicyGradient(env, conf, action_network_cls=MLPModel)\n",
    "agent.train(int(1e7), render_freq=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.volume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = panel_stock.axes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(df.loc[ts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = env.open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((np.arange(10), np.arange(10))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panel_stock.axes[2].values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(np.ones((3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack((np.arange(10), np.arange(10)), axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-26 19:23:53,161] Making new env: Breakout-v0\n",
      "Exception ignored in: <bound method Monitor.__del__ of <Monitor<TimeLimit<AtariEnv<Breakout-v0>>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 239, in __del__\n",
      "    self.close()\n",
      "  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 145, in close\n",
      "    self.stats_recorder.close()\n",
      "  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/gym/monitoring/stats_recorder.py\", line 89, in close\n",
      "    self.flush()\n",
      "  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/gym/monitoring/stats_recorder.py\", line 96, in flush\n",
      "    with atomic_write.atomic_write(self.path) as f:\n",
      "  File \"/home/tomoaki/anaconda3/lib/python3.6/contextlib.py\", line 82, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/home/tomoaki/anaconda3/lib/python3.6/site-packages/gym/utils/atomic_write.py\", line 45, in atomic_write\n",
      "    with open(tmppath, 'wb' if binary else 'w') as file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './videos/openaigym.episode_batch.1.3383.stats.json~'\n",
      "[2017-07-26 19:23:54,055] Creating monitor directory ./videos\n",
      "[2017-07-26 19:23:54,072] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000000.mp4\n",
      "\n",
      "\n",
      "  0%|          | 0/10000000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 66/10000000 [00:00<4:14:21, 655.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "  0%|          | 101/10000000 [00:00<5:22:55, 516.12it/s]\u001b[A\n",
      "  0%|          | 126/10000000 [00:00<18:38:44, 148.98it/s]\u001b[A\n",
      "  0%|          | 145/10000000 [00:00<27:55:37, 99.46it/s] \u001b[A\n",
      "  0%|          | 160/10000000 [00:01<34:28:29, 80.57it/s]\u001b[A\n",
      "  0%|          | 172/10000000 [00:01<40:20:48, 68.85it/s]\u001b[A\n",
      "  0%|          | 182/10000000 [00:01<43:22:30, 64.04it/s]\u001b[A\n",
      "  0%|          | 199/10000000 [00:01<45:48:05, 60.65it/s][2017-07-26 19:23:56,161] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000001.mp4\n",
      "  0%|          | 1869/10000000 [00:31<47:04:30, 59.00it/s][2017-07-26 19:24:26,189] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000008.mp4\n",
      "  0%|          | 6499/10000000 [01:55<50:48:13, 54.64it/s][2017-07-26 19:25:49,843] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000027.mp4\n",
      "  0%|          | 14930/10000000 [04:38<46:55:06, 59.12it/s][2017-07-26 19:28:33,017] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000064.mp4\n",
      "  0%|          | 29253/10000000 [08:59<52:31:32, 52.73it/s][2017-07-26 19:32:53,539] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000125.mp4\n",
      "  1%|          | 51263/10000000 [15:56<49:30:39, 55.82it/s][2017-07-26 19:39:50,371] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000216.mp4\n",
      "  1%|          | 82453/10000000 [25:51<51:57:34, 53.02it/s][2017-07-26 19:49:45,693] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000343.mp4\n",
      "  1%|          | 122306/10000000 [38:35<46:16:25, 59.30it/s][2017-07-26 20:02:29,755] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000512.mp4\n",
      "  2%|▏         | 175307/10000000 [54:21<48:50:54, 55.87it/s][2017-07-26 20:18:16,096] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video000729.mp4\n",
      "  2%|▏         | 244326/10000000 [1:16:07<55:59:46, 48.39it/s][2017-07-26 20:40:02,170] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video001000.mp4\n",
      "  6%|▌         | 610640/10000000 [3:12:33<48:45:09, 53.50it/s][2017-07-26 22:36:27,438] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video002000.mp4\n",
      " 14%|█▎        | 1363677/10000000 [7:18:25<42:01:37, 57.08it/s][2017-07-27 02:42:19,806] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video003000.mp4\n",
      " 22%|██▏       | 2222853/10000000 [12:03:06<39:20:20, 54.92it/s][2017-07-27 07:27:00,556] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video004000.mp4\n",
      " 31%|███       | 3112507/10000000 [16:56:08<39:11:03, 48.83it/s][2017-07-27 12:20:02,487] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video005000.mp4\n",
      " 40%|████      | 4004443/10000000 [21:47:22<33:52:07, 49.17it/s][2017-07-27 17:11:16,554] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video006000.mp4\n",
      " 49%|████▉     | 4875219/10000000 [26:27:20<29:22:12, 48.47it/s][2017-07-27 21:51:14,689] Starting new video recorder writing to /home/tomoaki/work/Development/RL/videos/openaigym.video.2.3383.video007000.mp4\n",
      " 53%|█████▎    | 5327041/10000000 [28:46:50<22:02:38, 58.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from rltensor.agents import DQN\n",
    "from rltensor.processors import AtariProcessor\n",
    "from rltensor.networks import DuelingModel\n",
    "\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":True, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        'double_q': True,\n",
    "        \"memory_limit\": 100000,\n",
    "        \"window_length\": 4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"learning_rate_minimum\": 2.5e-4,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 100,\n",
    "        \"ep\": 1e-3,\n",
    "        \"min_r\": -1,\n",
    "        \"max_r\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"error_clip\": 1.0,\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "        \"t_learn_start\": 100,\n",
    "        \"t_train_freq\": 4,\n",
    "        \"t_target_q_update_freq\": 10000,\n",
    "        \"ep_start\": 1.0,\n",
    "        \"ep_end\": 0.1,\n",
    "        \"t_ep_end\": int(1e6),\n",
    "        \"model_dir\": \"./logs/dqn\",\n",
    "        \"log_freq\": 1000,\n",
    "        \"avg_length\": 10000,\n",
    "        \"env_name\": 'DemonAttack-v0',\n",
    "        \"prioritized\": True,\n",
    "}\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":False, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        \"env_name\": 'DemonAttack-v0',\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "}\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(env, conf, q_network_cls=DuelingModel)\n",
    "dqn.train(int(1e7), render_freq=None, save_video_path=\"./videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-28 00:48:07,087] Making new env: Breakout-v0\n",
      "[2017-07-28 00:48:07,764] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/tomoaki/work/Development/RL/breakout_videos')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./breakout_dqn_params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-28 00:48:07,834] Restoring parameters from ./breakout_dqn_params/model.ckpt\n",
      "[2017-07-28 00:48:07,868] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-07-28 00:48:07,876] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.3.20170.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-28 00:48:27,366] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.3.20170.video000001.mp4\n",
      "[2017-07-28 00:52:21,563] Starting new video recorder writing to /home/tomoaki/work/Development/RL/breakout_videos/openaigym.video.3.20170.video000008.mp4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from rltensor.agents import DQN\n",
    "from rltensor.processors import AtariProcessor\n",
    "from rltensor.networks import DuelingModel\n",
    "\n",
    "\n",
    "conf = {\"q_conf\":[\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(8, 8), \"num_filter\":32, \"stride\":4,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(5, 5), \"num_filter\":64, \"stride\":2,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "           {\"name\": \"conv2d\", \"kernel_size\": (3, 3), \"num_filter\":64, \"stride\":1,\n",
    "             \"padding\": 'SAME', \"is_batch\":False, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":False, \"num_hidden\": 512, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        \"env_name\": 'Breakout-v0',\n",
    "        \"processor\": AtariProcessor(84, 84),\n",
    "}\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "tf.reset_default_graph()\n",
    "dqn = DQN(env, conf, q_network_cls=DuelingModel)\n",
    "dqn.play(num_episode=10, ep=0.05, load_file_path=\"./breakout_dqn_params/model.ckpt\",\n",
    "         save_video_path=\"./breakout_videos\", render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if [1, 2, 3]:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 4\n",
    "while count < 5:\n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"%s\" % True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.insert(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = deque([1, 2, 3], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.append(3)\n",
    "x.append(3)\n",
    "x.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f829244df969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_if_false\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'select'"
     ]
    }
   ],
   "source": [
    " result = tf.select(pred, val_if_true, val_if_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.bool, (None,))\n",
    "y = tf.cast(x, tf.int32)\n",
    "z = tf.one_hot(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "print(y.eval(feed_dict={x:[True, False, True]}))\n",
    "print(z.eval(feed_dict={x:[True, False, True]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.arange(10).astype(int)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.random.choice(range(0, 5), 3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1, 2, 3, 4], 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "np.append(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
