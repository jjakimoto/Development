{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/13/2018\n",
    "# Exploratory Data Analysis (EDA)\n",
    "* Get comfortable with data\n",
    "* Find magic features\n",
    "* Steps:\n",
    "    1. Getting domain knowledge\n",
    "    - It helps to deeper understand the problem\n",
    "    2. Checking if the data is intuitive\n",
    "    - And agrees with domain knowledge\n",
    "    3. Understanding how the data was generated\n",
    "    - As it is crucial to set up a proper validation\n",
    "    \n",
    "## Anonymized Data\n",
    "* Explore individual features\n",
    "    * Guess the meaning of the columns\n",
    "    * Guess the types of the columns\n",
    "* Explore feature relations\n",
    "    * Find relations between pairs\n",
    "    * Find feature groups\n",
    "    \n",
    "## Visualization\n",
    "* Explore individual features\n",
    "    * Histograms\n",
    "    * Plots\n",
    "    * Statistics\n",
    "* Explore feature relations\n",
    "    * Pairs\n",
    "        * Scatter plots\n",
    "        * Scatter matrix\n",
    "        * Corrplot\n",
    "    * Groups\n",
    "        * Corrplot + clustering\n",
    "        * Plot (index vs feature statistics)\n",
    "        \n",
    "## Clean Data\n",
    "* Duplicated feature\n",
    "* Duplicated rows\n",
    "* Constant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/31/2018\n",
    "# Collaborative Filtering\n",
    "* Blog post: https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0\n",
    "\n",
    "## Memory Based\n",
    "* Based on similarity, e.g., cosine similarity\n",
    "* Predict rates by weighted average, where weights are determined according to similarities\n",
    "* Two types:\n",
    "    * Item-Item: Users who liked this item also liked ...\n",
    "    * User-Item: Users who are similar to you also liked ...\n",
    "    \n",
    "## Model Based\n",
    "* Learn parameters\n",
    "* Three types:\n",
    "    * Clustering based: Similarity is calculated by unsupervised\n",
    "    * Matrix factorization based: Use SVD and find efficient approximation\n",
    "    * Deep learning: CNN, RNN, etc.\n",
    "* Deep Learning Based:\n",
    "    * Nice blog: https://medium.com/@libreai/a-glimpse-into-deep-learning-for-recommender-systems-d66ae0681775\n",
    "    * Auto Encoder: https://arxiv.org/pdf/1606.07792.pdf\n",
    "    * CNN: https://arxiv.org/pdf/1510.01784.pdf\n",
    "    * Survey: https://arxiv.org/pdf/1707.07435.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/20/2018\n",
    "# EM Algorithm\n",
    "* Handles maximization of likelihood including intractable integration of hidden variables\n",
    "\n",
    "## Problem\n",
    "Maximize\n",
    "$$l(\\theta) = \\sum_{i=1}^N log p(x_i | \\theta) = \\sum_{i=1}^{N} log [ \\sum_{z_i} p(x_i, z_i | \\theta)]$$\n",
    "\n",
    "## Algorithm\n",
    "#### E Step\n",
    "* Define the complete data log likelihood\n",
    "$$l_c(\\theta) = \\sum_{i=1}^N log p(x_i, z_i | \\theta)$$\n",
    "* Use the expected data log likelihood because $z_i$ is unknown.\n",
    "$$Q(\\theta, \\theta^{t-1}) = E[l_c(\\theta) | \\mathcal{D}, \\theta^{t-1}]$$\n",
    "* Q is called auxiliary function\n",
    "\n",
    "#### M Step\n",
    "* Maximize auxiliary function\n",
    "$$\\theta^t = arg\\underset{\\theta}{max}Q(\\theta, \\theta^{t-1})$$\n",
    "\n",
    "\n",
    "\n",
    "# K-means Algorithm\n",
    "* Can be constructed from EM algorithms\n",
    "\n",
    "#### E Step\n",
    "* $p(z_i = k | x_i, \\theta) \\approx \\bf{I}(k = z_i^*)$, \n",
    "where $z_i^* = arg\\underset{k}{max} p(z_i = k | x_i, \\theta)$\n",
    "* This is called hard EM\n",
    "* $z_i^* = arg\\underset{k}{min} ||x_i - \\mu_k||$\n",
    "\n",
    "#### M Step\n",
    "* $\\mu_k =  \\frac{1}{N_k} \\sum_{i:z_i = k} x_i$\n",
    "\n",
    "#### Vector Quantization\n",
    "* $encode(x_i) = arg\\underset{k}{min} ||x_i - \\mu_k||$\n",
    "* $decode(k) = \\mu_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "05/13/2018\n",
    "# Naive Bayes\n",
    "* Features are conditional independent given the class label.\n",
    "$$ p(x | y=c, \\theta) = \\prod_{j=1}^D p(x_j | y=c, \\theta_j)$$\n",
    "* Efficiency is O(CD), where C is the number of class and D is that of features.\n",
    "* Immune to overfitting due to the simplicity.\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "* Greedy algorithm\n",
    "* Solving completely is NP-Complete\n",
    "* Algorithms:\n",
    "    1. Find the best feature and threshold to minimize cost function: cost_above_threshold + cost_below_threshold\n",
    "    2. Calculate the gain of splitting\n",
    "    $$ \\Delta = cost(\\mathcal{D}) - (\\frac{|\\mathcal{D}_{L}|}{|\\mathcal{D}|}cost(\\mathcal{D}_{L}) + c\\frac{|\\mathcal{D}_{R}|}{|\\mathcal{D}|}cost(\\mathcal{D}_{R}))$$\n",
    "    3. If the gain is larger than the gain threshold determined in advance, split the tree\n",
    "    4. Iterate 1-3 until reaching the max-depth or stopping.\n",
    "* Pros:\n",
    "    - interpretable\n",
    "    - relatively robust to outliers\n",
    "    - scale well to large datasets\n",
    "    - can be modified to handle missing data\n",
    "* Cons:\n",
    "    - Not very accurate due to greedy algorithm\n",
    "    - Unstable: susceptible to small input change => random forest\n",
    "    \n",
    "    \n",
    "# Kernel\n",
    "* Something like similarity metric\n",
    "* Kernel Machine\n",
    "    - Feature vector with centroids: $\\phi(x) = [k(x, \\mu_{1}), \\dots, k(x, \\mu_{K})]$. Define features by how far from centroids with kernel metrics\n",
    "    - Feature vector with data points: $\\phi(x) = [k(x, x_{1}), \\dots, k(x, x_{N})]$. Define features by how far from data points with kernel metrics\n",
    "    - Logistic regression: $p(y | x, \\theta) = Ber(w^t \\phi(x))$\n",
    "    - L1VM: With L1 reguralization\n",
    "    - L2VML: With L2 regurlaization\n",
    "    - RVM: With the reguralization coming from ARD gaussian prior\n",
    "    - SVM: Introduce sparsity through loss function not regularization\n",
    "* Kernel trick replace inner product with kernel\n",
    "* Kernelized ridge regression: Dual problem change the complexity from $O(D^3)$ to $O(N^3)$. Thus, effective in high dimensional data.\n",
    "* Kernel PCA\n",
    "\n",
    "\n",
    "# Linear Decision Boundary\n",
    "* Define $y(\\bf{x}) = \\bf{w} \\cdot \\bf{x} + b$\n",
    "* $\\bf{w}$ is a perpendicular vector of a plane defined by $y(\\bf{x}) = 0$\n",
    "* Value for component of $\\bf{w}$ direction is calculated by $\\frac{\\bf{w}}{||\\bf{w}||} \\cdot \\bf{x}$\n",
    "* Distance between $y(\\bf{x})$ and origin point is $\\frac{\\bf{w}}{||\\bf{w}||} \\cdot \\bf{x}= -\\frac{b}{||\\bf{w}||}$\n",
    "* Distance from the $y(\\bf{x})$ in general \n",
    "$$\\frac{\\bf{w}}{||\\bf{w}||} \\cdot \\bf{x} - \\frac{\\bf{w}}{||\\bf{w}||} \\cdot \\bf{x}_{project} = \\frac{y(\\bf{x})}{||\\bf{w}||} - \\frac{y(\\bf{x_{project}})}{||\\bf{w}||} = \\frac{y(\\bf{x})}{||\\bf{w}||}$$ \n",
    "\n",
    "    \n",
    "# SVM\n",
    "* Perceptron has infinity number of solutions => the best one is determined by validation\n",
    "* SVM choses the best way to split through the concept called margin\n",
    "* Makes sparse solution\n",
    "* Maximum Margin:\n",
    "$$\\underset{w, b}{arg max}\\{\\frac{1}{||\\bf{w}||} \\underset{n}{min} [t_n (\\bf{w} \\cdot \\phi(\\bf{x}) + b)]\\}$$\n",
    "* Maximization is scale with respect to w and b. We can change objective functions to \n",
    "    * $t_n (\\bf{w} \\cdot \\phi(\\bf{x}) + b) = 1$ for minimized n\n",
    "    * $t_n (\\bf{w} \\cdot \\phi(\\bf{x}) + b) \\geq 1$ for arbitrary n\n",
    "    * $max \\frac{1}{||\\bf{w}||}$ is equivalent to $min||\\bf{w}||^2$\n",
    "    * $L = \\frac{1}{2}||\\bf{w}||^2 - \\sum a_n \\{t_n (\\bf{w} \\cdot \\phi(\\bf{x}) + b) - 1\\}$\n",
    "* Able to introduce regularization through slack variables, which makes soft margin\n",
    "* Introduce sparsity by changing loss function \n",
    "    - Regression: Epsilon intensive loss function:\n",
    "        \\begin{equation}\n",
    "        L_{\\epsilon} (y, \\hat{y}) =\n",
    "        \\begin{cases}\n",
    "              0, & \\text{if}\\ |y - \\hat{y}| < \\epsilon \\\\\n",
    "              |y - \\hat{y}| - \\epsilon, & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "      \\end{equation} \n",
    "    - Classification: Hinge loss\n",
    "    $$L_{\\epsilon} (y, \\hat{y}) = (1 - y \\hat{y})_{+}$$\n",
    "* $C = 1 / \\lambda$\n",
    "$$Loss = C \\sum_{i=1}^N L_{\\epsilon} (y_i, \\hat{y}_i) + \\frac{1}{2} ||w||^2$$\n",
    "* $\\hat{w} = \\sum \\alpha_i x_i$, $\\alpha \\geq 0$. $\\alpha$ is sparse and $x_i$ for $\\alpha > 0$ is called support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = set([3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.remove(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4, 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 4, 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
