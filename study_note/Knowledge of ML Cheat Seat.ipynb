{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL\n",
    "* SELECT DISTINCT column FROM table_name;\n",
    "* SELECT * FROM Customers WHERE Country='Mexico';\n",
    "* SELECT column1, column2 FROM table_name WHERE condition1 AND(OR) condition2 AND(OR) condition3;\n",
    "* SELECT * FROM Customers WHERE Country='Germany' AND City='Berlin';\n",
    "* SELECT * FROM Customers WHERE Country='German' AND (City='Berlin' OR City='Munchen')\n",
    "* SELECT * FROM Customers ORDER BY Country (DESC, ASC);\n",
    "* INSERT INTO table_name (column1, column2, column3, ...) VALUES (value1, value2, value3, ...);\n",
    "* UPDATE talbe_name SET column1=value1, column2=value2, ... WHERE condition;\n",
    "* DELETE FROM talbe_name WHERE condition;\n",
    "* SELECT TOP number (PERCENT) colum1, colum2, ... FROM table_name WHERE condition LIMIT number;\n",
    "* SELECT MIN(column_name) FROM table_name WHERE condition;\n",
    "* SELECT column1, column2, ... FROM table_name WEHRE column_name LIKE pattern;\n",
    "* SELECT column_names FROM table_name WHERE columns_name IN (value1, value2, ...);\n",
    "* SELECT column_names FROM table_name WHERE column_name BETWEEN value1 AND value2;\n",
    "* SELECT table1.column1, table2.column2, ... FROM table1 INNER JOIN table2 ON table1.some_name1=table2.some_name2\n",
    "* SELECT column_names FROM table1 UNION SELECT column_names FROM table2;\n",
    "* SELECT COUNT(CustomerID), Country FROM Customers GROUP BY Country;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "09/30/2018\n",
    "# Stationary Process\n",
    "* Strictly Stationary: the joint distribution of $X(t_1), \\dots, X(t_k)$ is the same as that of $X(t_1 + \\tau), \\dots, X(t_k + \\tau)$ does not depend on $t$\n",
    "* Second-Order Stationary (Weakly Stationary)\n",
    "    * $Cov[X(t), X(t + \\tau)] = \\gamma(\\tau)$, (does not depend on $t$)\n",
    "    * $E[X(t)] = \\mu$\n",
    "* Autocorrelation Function for a weakly stationary process\n",
    "$$ \\rho(\\tau) = \\gamma(\\tau) / \\gamma(0)$$\n",
    "\n",
    "# Moving Average Process\n",
    "* $X_t = \\beta_0 Z_t + \\beta_1 Z_{t - 1} + \\dots + \\beta_q Z_{t-q}$\n",
    ", where $Z$ is a purely random process with mean 0 and variance $\\sigma_{Z}^2$\n",
    "* Invertible: $Z_t = \\Sigma_{j=0}^\\infty \\pi_j X_{t-j}$, where $\\Sigma |\\pi_j| < \\infty$\n",
    "* $B^j X_t = X_{t - j}$\n",
    "* $X_t = (\\beta_0 + \\beta_1 B + \\dots + \\beta_q B^q) Z_t = \\theta(B) Z_t$\n",
    "* Invertible => all roots of $\\theta(B) = 0$ lie outside of the unit circle (necessary for inverted equation to have convergence sequence coefficients)\n",
    "\n",
    "# Autoregressive Process\n",
    "* $X_t = \\alpha_1 X_{t - 1} + \\dots + \\alpha_p X_{t - p} + Z_t$\n",
    "* $Z_t = (1 - \\alpha_1 B - \\dots - \\alpha_p B^p) X_t = \\phi(B) X_t$\n",
    "* Stationary => all roots of $\\phi(B) = 0$ lie outside of the unit circle\n",
    "\n",
    "\n",
    "# Unit Root Test\n",
    "* Theory, http://faculty.chicagobooth.edu/ruey.tsay/teaching/uts/lec11-08.pdf\n",
    "* Null Hypothesis is there is a unit root\n",
    "* $\\phi(B) = (1 - B)\\phi^*(B)$\n",
    "* Use t-ratio test\n",
    "* Estimated coefficients through linear regression\n",
    "* Alternative hypothesis is that the coefficient is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NLP\n",
    "### Word2Vec\n",
    "* Distributed Representations of Words and Phrases and their Compositionality, https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "* Skip-gram: Learn a classifier to predict surrounding words given a word\n",
    "* Continuous Bag of Words (CBOW): Predict next words given other words in a context\n",
    "* Learn a classifier to predict a word given other words in a context\n",
    "* Maximize the average log probability\n",
    "* \\# words ranges fomr 100K to 10M\n",
    "* Optimize the model with hierarchical softmax or negative sampling\n",
    "\n",
    "\n",
    "\n",
    "### Doc2Vec\n",
    "* Distributed Representations of Sentences and Documents, https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "* Distributed Memory: add paragraph vector to the input of COBW\n",
    "* Bag of Words: predict words given a paragraph vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/13/2018\n",
    "# Exploratory Data Analysis (EDA)\n",
    "* Get comfortable with data\n",
    "* Find magic features\n",
    "* Steps:\n",
    "    1. Getting domain knowledge\n",
    "    - It helps to deeper understand the problem\n",
    "    2. Checking if the data is intuitive\n",
    "    - And agrees with domain knowledge\n",
    "    3. Understanding how the data was generated\n",
    "    - As it is crucial to set up a proper validation\n",
    "    \n",
    "## Anonymized Data\n",
    "* Explore individual features\n",
    "    * Guess the meaning of the columns\n",
    "    * Guess the types of the columns\n",
    "* Explore feature relations\n",
    "    * Find relations between pairs\n",
    "    * Find feature groups\n",
    "    \n",
    "## Visualization\n",
    "* Explore individual features\n",
    "    * Histograms\n",
    "    * Plots\n",
    "    * Statistics\n",
    "* Explore feature relations\n",
    "    * Pairs\n",
    "        * Scatter plots\n",
    "        * Scatter matrix\n",
    "        * Corrplot\n",
    "    * Groups\n",
    "        * Corrplot + clustering\n",
    "        * Plot (index vs feature statistics)\n",
    "        \n",
    "## Clean Data\n",
    "* Duplicated feature\n",
    "* Duplicated rows\n",
    "* Constant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/31/2018\n",
    "# Collaborative Filtering\n",
    "* Blog post: https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0\n",
    "\n",
    "## Memory Based\n",
    "* Based on similarity, e.g., cosine similarity\n",
    "* Predict rates by weighted average, where weights are determined according to similarities\n",
    "* Two types:\n",
    "    * Item-Item: Users who liked this item also liked ...\n",
    "    * User-Item: Users who are similar to you also liked ...\n",
    "    \n",
    "## Model Based\n",
    "* Learn parameters\n",
    "* Three types:\n",
    "    * Clustering based: Similarity is calculated by unsupervised\n",
    "    * Matrix factorization based: Use SVD and find efficient approximation\n",
    "    * Deep learning: CNN, RNN, etc.\n",
    "* Deep Learning Based:\n",
    "    * Nice blog: https://medium.com/@libreai/a-glimpse-into-deep-learning-for-recommender-systems-d66ae0681775\n",
    "    * Auto Encoder: https://arxiv.org/pdf/1606.07792.pdf\n",
    "    * CNN: https://arxiv.org/pdf/1510.01784.pdf\n",
    "    * Survey: https://arxiv.org/pdf/1707.07435.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/20/2018\n",
    "# EM Algorithm\n",
    "* Handles maximization of likelihood including intractable integration of hidden variables\n",
    "\n",
    "## Problem\n",
    "Maximize\n",
    "$$l(\\theta) = \\sum_{i=1}^N log p(x_i | \\theta) = \\sum_{i=1}^{N} log [ \\sum_{z_i} p(x_i, z_i | \\theta)]$$\n",
    "\n",
    "## Algorithm\n",
    "#### E Step\n",
    "* Define the complete data log likelihood\n",
    "$$l_c(\\theta) = \\sum_{i=1}^N log p(x_i, z_i | \\theta)$$\n",
    "* Use the expected data log likelihood because $z_i$ is unknown.\n",
    "$$Q(\\theta, \\theta^{t-1}) = E[l_c(\\theta) | \\mathcal{D}, \\theta^{t-1}]$$\n",
    "* Q is called auxiliary function\n",
    "\n",
    "#### M Step\n",
    "* Maximize auxiliary function\n",
    "$$\\theta^t = arg\\underset{\\theta}{max}Q(\\theta, \\theta^{t-1})$$\n",
    "\n",
    "\n",
    "\n",
    "# K-means Algorithm\n",
    "* Can be constructed from EM algorithms\n",
    "\n",
    "#### E Step\n",
    "* $p(z_i = k | x_i, \\theta) \\approx \\bf{I}(k = z_i^*)$, \n",
    "where $z_i^* = arg\\underset{k}{max} p(z_i = k | x_i, \\theta)$\n",
    "* This is called hard EM\n",
    "* $z_i^* = arg\\underset{k}{min} ||x_i - \\mu_k||$\n",
    "\n",
    "#### M Step\n",
    "* $\\mu_k =  \\frac{1}{N_k} \\sum_{i:z_i = k} x_i$\n",
    "\n",
    "#### Vector Quantization\n",
    "* $encode(x_i) = arg\\underset{k}{min} ||x_i - \\mu_k||$\n",
    "* $decode(k) = \\mu_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "05/13/2018\n",
    "# Naive Bayes\n",
    "* Features are conditional independent given the class label.\n",
    "$$ p(x | y=c, \\theta) = \\prod_{j=1}^D p(x_j | y=c, \\theta_j)$$\n",
    "* Efficiency is O(CD), where C is the number of class and D is that of features.\n",
    "* Immune to overfitting due to the simplicity.\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "* Greedy algorithm\n",
    "* Solving completely is NP-Complete\n",
    "* Algorithms:\n",
    "    1. Find the best feature and threshold to minimize cost function: cost_above_threshold + cost_below_threshold\n",
    "    2. Calculate the gain of splitting\n",
    "    $$ \\Delta = cost(\\mathcal{D}) - (\\frac{|\\mathcal{D}_{L}|}{|\\mathcal{D}|}cost(\\mathcal{D}_{L}) + c\\frac{|\\mathcal{D}_{R}|}{|\\mathcal{D}|}cost(\\mathcal{D}_{R}))$$\n",
    "    3. If the gain is larger than the gain threshold determined in advance, split the tree\n",
    "    4. Iterate 1-3 until reaching the max-depth or stopping.\n",
    "* Pros:\n",
    "    - interpretable\n",
    "    - relatively robust to outliers\n",
    "    - scale well to large datasets\n",
    "    - can be modified to handle missing data\n",
    "* Cons:\n",
    "    - Not very accurate due to greedy algorithm\n",
    "    - Unstable: susceptible to small input change => random forest\n",
    "    \n",
    "    \n",
    "# Kernel\n",
    "* Something like similarity metric\n",
    "* Kernel Machine\n",
    "    - Feature vector with centroids: $\\phi(x) = [k(x, \\mu_{1}), \\dots, k(x, \\mu_{K})]$. Define features by how far from centroids with kernel metrics\n",
    "    - Feature vector with data points: $\\phi(x) = [k(x, x_{1}), \\dots, k(x, x_{N})]$. Define features by how far from data points with kernel metrics\n",
    "    - Logistic regression: $p(y | x, \\theta) = Ber(w^t \\phi(x))$\n",
    "    - L1VM: With L1 reguralization\n",
    "    - L2VML: With L2 regurlaization\n",
    "    - RVM: With the reguralization coming from ARD gaussian prior\n",
    "    - SVM: Introduce sparsity through loss function not regularization\n",
    "* Kernel trick replace inner product with kernel\n",
    "* Kernelized ridge regression: Dual problem change the complexity from $O(D^3)$ to $O(N^3)$. Thus, effective in high dimensional data.\n",
    "* Kernel PCA\n",
    "\n",
    "\n",
    "# Linear Decision Boundary\n",
    "* Define $y(\\bf{x}) = \\bf{w} \\cdot \\bf{x} + b$\n",
    "* $\\bf{w}$ is a perpendicular vector of a plane defined by $y(\\bf{x}) = 0$\n",
    "* Value for component of $\\bf{w}$ direction is calculated by $\\frac{\\bf{w}}{||\\bf{w}||} \\cdot \\bf{x}$\n",
    "* Distance between $y(\\bf{x})$ and origin point is $\\frac{\\bf{w}}{||\\bf{w}||} \\cdot \\bf{x}= -\\frac{b}{||\\bf{w}||}$\n",
    "* Distance from the $y(\\bf{x})$ in general \n",
    "$$\\frac{\\bf{w}}{||\\bf{w}||} \\cdot \\bf{x} - \\frac{\\bf{w}}{||\\bf{w}||} \\cdot \\bf{x}_{project} = \\frac{y(\\bf{x})}{||\\bf{w}||} - \\frac{y(\\bf{x_{project}})}{||\\bf{w}||} = \\frac{y(\\bf{x})}{||\\bf{w}||}$$ \n",
    "\n",
    "    \n",
    "# SVM\n",
    "* Perceptron has infinity number of solutions => the best one is determined by validation\n",
    "* SVM choses the best way to split through the concept called margin\n",
    "* Makes sparse solution\n",
    "* Maximum Margin:\n",
    "$$\\underset{w, b}{arg max}\\{\\frac{1}{||\\bf{w}||} \\underset{n}{min} [t_n (\\bf{w} \\cdot \\phi(\\bf{x}) + b)]\\}$$\n",
    "* Maximization is scale with respect to w and b. We can change objective functions to \n",
    "    * $t_n (\\bf{w} \\cdot \\phi(\\bf{x}) + b) = 1$ for minimized n\n",
    "    * $t_n (\\bf{w} \\cdot \\phi(\\bf{x}) + b) \\geq 1$ for arbitrary n\n",
    "    * $max \\frac{1}{||\\bf{w}||}$ is equivalent to $min||\\bf{w}||^2$\n",
    "    * $L = \\frac{1}{2}||\\bf{w}||^2 - \\sum a_n \\{t_n (\\bf{w} \\cdot \\phi(\\bf{x}) + b) - 1\\}$\n",
    "* Able to introduce regularization through slack variables, which makes soft margin\n",
    "* Introduce sparsity by changing loss function \n",
    "    - Regression: Epsilon intensive loss function:\n",
    "        \\begin{equation}\n",
    "        L_{\\epsilon} (y, \\hat{y}) =\n",
    "        \\begin{cases}\n",
    "              0, & \\text{if}\\ |y - \\hat{y}| < \\epsilon \\\\\n",
    "              |y - \\hat{y}| - \\epsilon, & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "      \\end{equation} \n",
    "    - Classification: Hinge loss\n",
    "    $$L_{\\epsilon} (y, \\hat{y}) = (1 - y \\hat{y})_{+}$$\n",
    "* $C = 1 / \\lambda$\n",
    "$$Loss = C \\sum_{i=1}^N L_{\\epsilon} (y_i, \\hat{y}_i) + \\frac{1}{2} ||w||^2$$\n",
    "* $\\hat{w} = \\sum \\alpha_i x_i$, $\\alpha \\geq 0$. $\\alpha$ is sparse and $x_i$ for $\\alpha > 0$ is called support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
