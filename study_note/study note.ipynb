{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04/29/2018\n",
    "\n",
    "# Unit Root\n",
    "[Quora: What is unit root in a time series?](https://www.quora.com/What-is-unit-root-in-a-time-series)\n",
    "[Wikipedia: Unit Root](https://en.wikipedia.org/wiki/Unit_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04/25/2018\n",
    "# P-NP problem\n",
    "Please refer to [this reference](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa).\n",
    "### P (Polynomial Time)\n",
    "* Can be solved in polynomial time\n",
    "\n",
    "### NP (Non-deterministic Polynomial Time)\n",
    "* If given yes instance, you can check if it is true in polynomical time\n",
    "* You are not sure if it can be solve in polynomial time\n",
    "\n",
    "### NP-Complete\n",
    "* NP problem that any NP problems can be reduce to in polynomial time\n",
    "\n",
    "### NP-Hard\n",
    "* Not have to be in NP\n",
    "* Not Have to decision problem\n",
    "* Intuitively, At least as hard as NP-Complete\n",
    "* Rigorously, if there is a NP-complete problem reducible to in polynomial time\n",
    "\n",
    "# Naive Bayes\n",
    "* Features are conditional indepentend given the class label.\n",
    "$$ p(x | y=c, \\theta) = \\prod_{j=1}^D p(x_j | y=c, \\theta_j)$$\n",
    "* Efficiency is O(CD), where C is the nubmer of class and D is that of features.\n",
    "* Immune to overfitting due to the simplicity.\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "* Greedy algorithm\n",
    "* Solving completely is NP-Complete\n",
    "* Algorithms:\n",
    "    1. Find the best feaure and threshold to minimize cost function: cost_above_threshold + cost_below_threshold\n",
    "    2. Calculate the gain of splitting\n",
    "    $$ \\Delta = cost(\\mathcal{D}) - (\\frac{|\\mathcal{D}_{L}|}{|\\mathcal{D}|}cost(\\mathcal{D}_{L}) + c\\frac{|\\mathcal{D}_{R}|}{|\\mathcal{D}|}ost(\\mathcal{D}_{R}))$$\n",
    "    3. If the gain is larger than the gain threshold determined in advance, split the tree\n",
    "    4. Iterate 1-3 until reaching the max-depth or stopping.\n",
    "* Pros:\n",
    "    - intepretable\n",
    "    - relatively robust to outliers\n",
    "    - scale well to large datasets\n",
    "    - can be modified to handle missing data\n",
    "* Cons:\n",
    "    - Not very accurate due to greedy algorithm\n",
    "    - Unstable: susceptible to smalle input change => random forest\n",
    "    \n",
    "    \n",
    "# Kernel\n",
    "* Something like smilarity metric\n",
    "* Kernel Machine\n",
    "    - Feature vector with centroids: $\\phi(x) = [k(x, \\mu_{1}), \\dots, k(x, \\mu_{K})]$. Define features by how far from centroids with kernel metrics\n",
    "    - Feature vector with data points: $\\phi(x) = [k(x, x_{1}), \\dots, k(x, x_{N})]$. Define features by how far from data points with kernel metrics\n",
    "    - Logistic regression: $p(y | x, \\theta) = Ber(w^t \\phi(x))$\n",
    "    - L1VM: With L1 reguralization\n",
    "    - L2VML: With L2 regurlaization\n",
    "    - RVM: With the reguralization coming from ARD gaussian prior\n",
    "    - SVM: Introduce sparsity through loss fuction not regularization\n",
    "* Kernel trick replace inner product with kernel\n",
    "* Kernelized ridge regression: Dual problem change the complexity from $O(D^3)$ to $O(N^3)$. Thsu, effective in high dimensional data.\n",
    "* Kernel PCA\n",
    "\n",
    "    \n",
    "# SVM\n",
    "* Introduce sparsity by changing loss function \n",
    "    - Regression: Epsilon intensive loss function:\n",
    "        \\begin{equation}\n",
    "        L_{\\epsilon} (y, \\hat{y}) =\n",
    "        \\begin{cases}\n",
    "              0, & \\text{if}\\ |y - \\hat{y}| < \\epsilon \\\\\n",
    "              |y - \\hat{y}| - \\epsilon, & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "      \\end{equation} \n",
    "    - Classification: Hinge loss\n",
    "    $$L_{\\epsilon} (y, \\hat{y}) = (1 - y \\hat{y})_{+}$$\n",
    "* $C = 1 / \\lambda$\n",
    "$$Loss = C \\sum_{i=1}^N L_{\\epsilon} (y_i, \\hat{y}_i) + \\frac{1}{2} ||w||^2$$\n",
    "* $\\hat{w} = \\sum \\alpha_i x_i$, $\\alpha \\geq 0$. $\\alpha$ is sparse and $x_i$ for $\\alpha > 0$ is called support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    X=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ a=1 \\\\\n",
    "      1, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
