{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/25/2018\n",
    "## Feature Pyramid Networks for Object Detection\n",
    "* Paper, https://arxiv.org/pdf/1612.03144.pdf\n",
    "* Each convolution takes 2 strides\n",
    "* In bottom up path, we use residual connection with conv $1 \\times 1$\n",
    "* Produce features for each scale\n",
    "* Final layers use conv $3 \\times 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/22/2018\n",
    "# Image Segmentation\n",
    "* Good reference, https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4\n",
    "\n",
    "## Selective Search\n",
    "* Paper, https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf\n",
    "* Use various simlarity metrics such as texture, color, and size\n",
    "* Merge pairs of regions from one with the largest similariy \n",
    "\n",
    "## R-CNN\n",
    "* Paper, https://arxiv.org/pdf/1311.2524.pdf\n",
    "* Region proposals\n",
    "    * Based on [Selective Search](https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf)\n",
    "    * Roughly, 2000 proposals\n",
    "* Feature extraction with CNN\n",
    "* Classification by SVM\n",
    "* Improves proposals accuracy by regression\n",
    "\n",
    "## Fast R-CNN\n",
    "* Paper, https://arxiv.org/pdf/1504.08083.pdf\n",
    "* Feature Extraction\n",
    "    * Produces a conv feature map: processes the whole image with CNN + pooling\n",
    "    * Extracts a fixed-length feature for each proposal object with a region of interest (RoI)\n",
    "    * We do not repeat the first process for each region proposal, which makes it faster to compute\n",
    "    * Choose features by considering the ratio of image sizes and actual proposals coordinates\n",
    "* Two modules for outputs\n",
    "    * Softmax probability to classify the object ('background' class is included)\n",
    "    * 4 values to define a bounding box\n",
    "* The RoI pooling layer\n",
    "    * Defined by a four-tuple (r, c, h, w): top-left corner (r, c) and its height and width (h, w)\n",
    "    * Splits into $H \\times W$ grid sub-windows, where each size is $h/H \\times w/W$\n",
    "    * Uses max pooling to generate $H \\times W$ features\n",
    "    * Try to sample regions from the same image for computational efficient training\n",
    "    * Output is fixed size\n",
    "* Initializing from pre-trained networks\n",
    "* Multi-task loss\n",
    "    * $p = (p_0, p_1, \\dots, p_K)$\n",
    "    * $t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$\n",
    "    * u is ground truth label\n",
    "    * v is ground truth box coordinates\n",
    "    * $L(p, u, t^u, v) = L_{cls}(p, u) + \\lambda \\bf{1}_{[u \\geq 1]} L_{loc}(t^u, v)$\n",
    "* Mini-batch sampling\n",
    "    * Object proposal is the same as R-CNN\n",
    "    * 25% of the RoI from object proposals that have IoU with a ground truth at least 0.5\n",
    "    * IoU between $[0.1, 0.5)$ => background label\n",
    "    * Horizontally flipped with probability 0.5\n",
    "* Get rid of SVM from R-CNN\n",
    "\n",
    "## Faster R-CNN\n",
    "* Paper, https://arxiv.org/pdf/1506.01497.pdf\n",
    "* No need to use selective search for region proposals\n",
    "* Use two modules\n",
    "    * CNN proposal regressor\n",
    "    * CNN binary or two-class classifier to predict if it is object or not\n",
    "* Use attention to tell Fast R-CNN module where to look\n",
    "* Anchor\n",
    "    * Slide small $n \\times n$ windows\n",
    "    * Proposes k boxes for each sliding point\n",
    "    * Use scale and ratio parameters, example $k=9$ for 3 scales and 3 ratios\n",
    "* Loss Function\n",
    "    * Assign positive labels to the highest IoU anchor or the one with IoU higher than 0.7\n",
    "    * Could assign multiple positive labels for a single box\n",
    "    * Assign negative labels to anchor with IoU lower than 0.3\n",
    "* Training\n",
    "    1. RPN is initialized by ImageNet model and fine tuned for region proposal task\n",
    "    2. Train Fast R-CNN detection network with fine tuned RPN\n",
    "    3. Initialize RPN with the detector network and fine tune only layers after feature extraction\n",
    "    4. Fine tune only layers unique to the detector network\n",
    "    \n",
    "## Fully Convolutional Network\n",
    "* Paper, https://arxiv.org/pdf/1605.06211.pdf\n",
    "* Receptive field: The size taken by kernel, e.g., $3 \\times 3$\n",
    "* Replace the fully connected last layer of classifier CNN network with convolution\n",
    "* Use skip connection from the middle layer to pass denser information\n",
    "* Upsampling is in-network, not deconvolution\n",
    "* Output of each pixel is softmax whose output corresponds to each class\n",
    "    \n",
    "## Mask R-CNN\n",
    "* Paper, https://arxiv.org/pdf/1703.06870.pdf\n",
    "* Improvement\n",
    "    * Apply fully convolutional network (FCN) to RoI\n",
    "    * RoIAlign instead of RoIPool\n",
    "* Output: $K \\times m^2$\n",
    "* Calculate binary probability unlike FCN\n",
    "* RoIAlign: Calculate fractional index value by bilinear interpolation\n",
    "* Use Feature Pyramid Network\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/19/2018\n",
    "# Object Detection\n",
    "* Metrics: IoU (Intersection of Union) = (Intersection / Union), which range between 0 and 1\n",
    "\n",
    "# Face Detection using Haar Cascades\n",
    "- Based on base functions like Haar functions\n",
    "- AdaBoost\n",
    "- Weak learner is based on simple threshold for each feature\n",
    "- Used when speed is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/18/2018\n",
    "# NN Architecture\n",
    "- Residual Net\n",
    "    - Projection parameters increases accuracy only slightly\n",
    "    - Identity with zero padding is good enough\n",
    "- Stochastic Depth\n",
    "    - Skip the mapping\n",
    "    - Survival rate decays linearly along with depth\n",
    "    - When network is large, 25% depth reduction expectedly\n",
    "    - In test time, multiply probability to scale down\n",
    "- Dense Net\n",
    "    - Connect to all layers, L(L + 1) / 2 connection\n",
    "    - Concatenation of previous features\n",
    "    - Dense Block: (BN + ReLU + Conv 1*1 + BN + ReLU + Conv 3*3) * L\n",
    "    - Conv 1*1 is feature selection\n",
    "    - Transition Layer: Conv 1*1 + Pooling\n",
    "    - You may reduce the number of features at transition layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GPU\n",
    "- Simple Multiple cores\n",
    "- CPU optimizes single execution\n",
    "- CPU - ferrari, GPU - freight truck\n",
    "- Fetch bunch of memory, e.g., 750GB/s while CPU 50GB/s\n",
    "- Takes time for only initial fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/14/2018\n",
    "# CAPM\n",
    "## Mean Variance Optimization\n",
    "* Efficient Frontier: A curve of maximized expected return given risk, standard deviation\n",
    "* Sharpe Ratio: $\\frac{E_H - r_f}{\\sigma_H}$, where $E_H$ and $\\sigma_H$ are the expected return and standard deviation of return of stock H, respectively\n",
    "* The highest Sharpe ration is achieved by the setting point of Efficient Frontier and linear line\n",
    "\n",
    "## Improve Sharpe Ratio\n",
    "* Adding an Uncorrelated Stock: if its amount is small and $E_S > r_f$, expected return keep the same and risk reduces\n",
    "* Adding a Perfectly Correlated Stock: $E_S - r_f = \\alpha + \\beta (E_P - r_f)$\n",
    "    - If $\\alpha > 0$, stock has more risk premium and improve the Sharpe ratio\n",
    "* Adding a Imperfectly Correlated Stock:\n",
    "    - Correlated part: Mimicked through the same way\n",
    "    - Uncorrelated part: Risk is diversified away\n",
    "* $E_S - r_f > \\beta (E_P - r_f)$ implies buying the stock improves the Sharpe ratio\n",
    "\n",
    "\n",
    "## Equilibrium\n",
    "* $E_S - r_f = \\beta (E_P - r_f)$ for all stocks in the market\n",
    "* The Sharpe Ratio of any asset is no higher than the Sharpe Ratio of the market portfolio\n",
    "\n",
    "\n",
    "## Three Implications\n",
    "* The expected return of an asset does not depend on its stand-alone risk, but depends on beta\n",
    "* Offers the method to measure the risk that cannot be diversified away\n",
    "* The expected return of an asset does not depend on the growth rate of its expected future cash flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/13/2018\n",
    "# Spark\n",
    "## Shared Variables\n",
    "* Broadcast Variables\n",
    "    - Used for common data like input\n",
    "    - Cached on each machine to reduce communication cost\n",
    "* Accumulators\n",
    "    - Only added through an associative and commutative operation\n",
    "    - Used to implement sum or counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/10/2018\n",
    "# Data Structures\n",
    "## Array\n",
    "- Fixed size of memories\n",
    "- Fast for index access\n",
    "- Slow to insert element because you have to move all elements, worst case O(N)\n",
    "\n",
    "## List\n",
    "- Each element has forward and backward element's address\n",
    "- Variable length\n",
    "- Slow for index access\n",
    "- Fast to insert element\n",
    "- Fast to access to head or tail\n",
    "\n",
    "\n",
    "# Scala\n",
    "## Introduction \n",
    "- Reference, https://docs.scala-lang.org/tour/tour-of-scala.html\n",
    "\n",
    "## Infinity\n",
    "- Double.PositiveInfinity\n",
    "- Double.NegativeInfinity\n",
    "\n",
    "## Seq\n",
    "- Seq has both LinkedList and ArrayList traits\n",
    "- List is an implementation of LinkedList, which means fast to access to head or tail\n",
    "\n",
    "## Trait\n",
    "- Instanciation: new YourTrait{}\n",
    "- Used for mixin\n",
    "- Not define initialize parameters\n",
    "- Reference, https://qiita.com/f81@github/items/5b96af593812286eec49\n",
    "\n",
    "## Implicit Class\n",
    "- Automatically detect the case of application and apply it to that case\n",
    "- Reference, https://qiita.com/miyatin0212/items/f70cf68e89e4367fcf2e\n",
    "\n",
    "## Unit\n",
    "- The same as void of C++ and C\n",
    "\n",
    "\n",
    "# Install Command\n",
    "## dpkg\n",
    "- Low-level system tool to extract, analyse, unpack and install or remove .deb file\n",
    "- Expertly handles Debian packages, rather than their dependencies\n",
    "- The difference from apt-get, https://unix.stackexchange.com/questions/104592/what-is-the-difference-between-apt-get-and-dpkg\n",
    "    - Dependency Management: Search the meta data related to packages either locally or remotely.\n",
    "    - apt-get makes use of dpkg\n",
    "- Reference, https://askubuntu.com/questions/173465/what-is-dpkg-for\n",
    "\n",
    "## apt-get\n",
    "- APT (the Advanced Packaging Tool): The Debian .deb software packaging system\n",
    "- The command-line tool for working with APT software packages\n",
    "- Makes use of dpkg to do the actual package installations\n",
    "- Understand dependency and install something if necessary\n",
    "- Reference, https://www.computerhope.com/unix/apt-get.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/08/2018\n",
    "# Computer Properties\n",
    "## Terminologies\n",
    "* Socket\n",
    "    - CPU: Connection that allows CPU to be connected to a motherboard\n",
    "    - Programming Language: communication between processors or a client and server\n",
    "    - Network: THe end points of communications between processes on a computer network such as the Internet\n",
    "    \n",
    "* Cache: To store data locally in order to speed up subsequent retrievals\n",
    "* Cache Memory: Memory bank that bridges main memory and the processor\n",
    "   \n",
    "## The output of lscpu\n",
    "* CPU: Physical processor\n",
    "* Socket: The number of sockets/slots for physical processors\n",
    "* Core: The number of cores per processor\n",
    "* Thread per core: The number of threads per core\n",
    "\n",
    "\n",
    "\n",
    "# Spark\n",
    "## Spark Introduction\n",
    "* Developed by Matei Zaharia @ UC Berkeley, 2012\n",
    "* Flexibility and extensibility of MapReduce\n",
    "* Faster than Hadoop 100 times when data is stored and 10 times when accessing disk\n",
    "* Spark job is associated with a chain of object dependencies organized in a direct acyclic graph (DAG)\n",
    "* A blog post about features, https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html\n",
    "\n",
    "## RDD (Resilient Distributed Dataset)\n",
    "* A collection of distributed immutable Java Virtual Machine (JVM) objects\n",
    "* Python data is stored within these JVM objects\n",
    "* Calculated against, cached, and stored in-memory\n",
    "* Significantly slower in Python\n",
    "* Transformations: map, filter, reduce, etc.\n",
    "* Actions: collect, take, etc.\n",
    "\n",
    "## DataFrame\n",
    "* Name columns\n",
    "* Cover the performance gap across all the languages\n",
    "* Fast query performance due to the Spark SQL Engine's Catalyst Optimizer and Project Tungsten\n",
    "* .cache(): Put RDD into memory, which makes execution faster\n",
    "    - Used for iterative machine learning application\n",
    "    - Used for stand alone application\n",
    "* .persist(level: StorageLevel): Put RDD into disk\n",
    "* .persist() is the same as cache\n",
    "\n",
    "## Datasets\n",
    "* Only for Java and Scala\n",
    "* Type-safe\n",
    "* API is unified with DataFrame from Apache SPark 2.0\n",
    "\n",
    "## Spark SQL\n",
    "* SQL interface\n",
    "* Return results with DataFrame/Datasets interface\n",
    "\n",
    "## Catalyst Optimizer\n",
    "* Powers both spark SQL and DataFrame\n",
    "* Add new optimizations and features to SparkSQL\n",
    "* Extend optimizer\n",
    "\n",
    "\n",
    "## Spark Streaming\n",
    "* Real time analysis https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/\n",
    "\n",
    "## Scheduling\n",
    "* SparkContext\n",
    "    - Set up internal services\n",
    "    - Establish a connection to Spark execution environment\n",
    "    - Organize schedulers, graphs, etc.\n",
    "* DAGScheduler: Scheduler based on Jobs and Stages\n",
    "    - Computes an execution DAG\n",
    "    - Determines the preferred location to run each task on\n",
    "    - Handles failure\n",
    "* Job: A work item submitted to DAGScheduler to compute the result of action\n",
    "* Stage: A set of parallel tasks, physical unit of execution\n",
    "* RDD => Action => Job => Stage\n",
    "\n",
    "## PySpark vs Spark (Scala)\n",
    "* Reference, https://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python\n",
    "* Computational overhead of PySpark\n",
    "    - Overhead of JVM communication\n",
    "    - Process-based execution (thread based for Scala)\n",
    "    - Performance of python code itself\n",
    "    - Reference counting used as the first line garbage collection method in CPython\n",
    "* E.g., UDF would slows your application because it requires moving data back and forth\n",
    "* Pure Python methods can slow down your applications\n",
    "* If you want to build complex analytics, go for Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/07/2018\n",
    "# Mindset to get over  fear of talking to someone\n",
    "* ‘Be who you are and say what you feel, because those who matter don’t mind, and those that mind, don’t matter.’ – Theodor Seuss Geisel\n",
    "* ‘Women love to be approached, so I will be a nice guy today and do something good by approaching her’.\n",
    "* Make an eye contact\n",
    "* Say hello to strangers\n",
    "\n",
    "# Quote to conquer fear \n",
    "\n",
    "1. \"Don't wait until everything is just right. It will never be perfect. There will always be challenges, obstacles, and less than perfect conditions. So what? Get started now. With each step you take, you will grow stronger and stronger, more and more skilled, more and more self-confident, and more and more successful.\" — Mark Victor Hansen\n",
    "\n",
    "2. \"Speak up. Believe in yourself. Take risks.\" — Sheryl Sandberg\n",
    "\n",
    "3. \"Inaction breeds doubt and fear. Action breeds confidence and courage. If you want to conquer fear, do not sit home and think about it. Go out and get busy.\" — Dale Carnegie\n",
    "\n",
    "4. \"All the great speakers were bad speakers at first.\" — Ralph Waldo Emerson\n",
    "\n",
    "5. \"Do the best you can until you know better. Then when you know better, do better.\" — Maya Angelou\n",
    "\n",
    "6. \"Always remember you are braver than you believe, stronger than you seem, and smarter than you think.\" — Christopher Robin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04/29/2018\n",
    "\n",
    "# Unit Root\n",
    "[Quora: What is unit root in a time series?](https://www.quora.com/What-is-unit-root-in-a-time-series)\n",
    "[Wikipedia: Unit Root](https://en.wikipedia.org/wiki/Unit_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04/25/2018\n",
    "# P-NP problem\n",
    "Please refer to [this reference](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa).\n",
    "### P (Polynomial Time)\n",
    "* Can be solved in polynomial time\n",
    "\n",
    "### NP (Non-deterministic Polynomial Time)\n",
    "* If given yes instance, you can check if it is true in polynomical time\n",
    "* You are not sure if it can be solve in polynomial time\n",
    "\n",
    "### NP-Complete\n",
    "* NP problem that any NP problems can be reduce to in polynomial time\n",
    "\n",
    "### NP-Hard\n",
    "* Not have to be in NP\n",
    "* Not Have to decision problem\n",
    "* Intuitively, At least as hard as NP-Complete\n",
    "* Rigorously, if there is a NP-complete problem reducible to in polynomial time\n",
    "\n",
    "# Naive Bayes\n",
    "* Features are conditional indepentend given the class label.\n",
    "$$ p(x | y=c, \\theta) = \\prod_{j=1}^D p(x_j | y=c, \\theta_j)$$\n",
    "* Efficiency is O(CD), where C is the nubmer of class and D is that of features.\n",
    "* Immune to overfitting due to the simplicity.\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "* Greedy algorithm\n",
    "* Solving completely is NP-Complete\n",
    "* Algorithms:\n",
    "    1. Find the best feaure and threshold to minimize cost function: cost_above_threshold + cost_below_threshold\n",
    "    2. Calculate the gain of splitting\n",
    "    $$ \\Delta = cost(\\mathcal{D}) - (\\frac{|\\mathcal{D}_{L}|}{|\\mathcal{D}|}cost(\\mathcal{D}_{L}) + c\\frac{|\\mathcal{D}_{R}|}{|\\mathcal{D}|}ost(\\mathcal{D}_{R}))$$\n",
    "    3. If the gain is larger than the gain threshold determined in advance, split the tree\n",
    "    4. Iterate 1-3 until reaching the max-depth or stopping.\n",
    "* Pros:\n",
    "    - intepretable\n",
    "    - relatively robust to outliers\n",
    "    - scale well to large datasets\n",
    "    - can be modified to handle missing data\n",
    "* Cons:\n",
    "    - Not very accurate due to greedy algorithm\n",
    "    - Unstable: susceptible to smalle input change => random forest\n",
    "    \n",
    "    \n",
    "# Kernel\n",
    "* Something like smilarity metric\n",
    "* Kernel Machine\n",
    "    - Feature vector with centroids: $\\phi(x) = [k(x, \\mu_{1}), \\dots, k(x, \\mu_{K})]$. Define features by how far from centroids with kernel metrics\n",
    "    - Feature vector with data points: $\\phi(x) = [k(x, x_{1}), \\dots, k(x, x_{N})]$. Define features by how far from data points with kernel metrics\n",
    "    - Logistic regression: $p(y | x, \\theta) = Ber(w^t \\phi(x))$\n",
    "    - L1VM: With L1 reguralization\n",
    "    - L2VML: With L2 regurlaization\n",
    "    - RVM: With the reguralization coming from ARD gaussian prior\n",
    "    - SVM: Introduce sparsity through loss fuction not regularization\n",
    "* Kernel trick replace inner product with kernel\n",
    "* Kernelized ridge regression: Dual problem change the complexity from $O(D^3)$ to $O(N^3)$. Thsu, effective in high dimensional data.\n",
    "* Kernel PCA\n",
    "\n",
    "    \n",
    "# SVM\n",
    "* Introduce sparsity by changing loss function \n",
    "    - Regression: Epsilon intensive loss function:\n",
    "        \\begin{equation}\n",
    "        L_{\\epsilon} (y, \\hat{y}) =\n",
    "        \\begin{cases}\n",
    "              0, & \\text{if}\\ |y - \\hat{y}| < \\epsilon \\\\\n",
    "              |y - \\hat{y}| - \\epsilon, & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "      \\end{equation} \n",
    "    - Classification: Hinge loss\n",
    "    $$L_{\\epsilon} (y, \\hat{y}) = (1 - y \\hat{y})_{+}$$\n",
    "* $C = 1 / \\lambda$\n",
    "$$Loss = C \\sum_{i=1}^N L_{\\epsilon} (y_i, \\hat{y}_i) + \\frac{1}{2} ||w||^2$$\n",
    "* $\\hat{w} = \\sum \\alpha_i x_i$, $\\alpha \\geq 0$. $\\alpha$ is sparse and $x_i$ for $\\alpha > 0$ is called support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    X=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ a=1 \\\\\n",
    "      1, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
