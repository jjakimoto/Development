{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/16/2018\n",
    "# Neural scene representation and rendering\n",
    "* Paper, http://science.sciencemag.org/content/sci/360/6394/1204.full.pdf\n",
    "* Two hierarchy: Representation and generation\n",
    "* Learn from unlabled a few angle images\n",
    "* Representation learning algorithm could be anything\n",
    "* Fine tune representation by inference task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/07/2018\n",
    "# Hierarchical Neural Story Generation\n",
    "* Paper, https://arxiv.org/pdf/1805.04833.pdf\n",
    "* github, https://github.com/pytorch/fairseq\n",
    "* Hierarchical approach: input => prompt(topic) => story\n",
    "* 303,358 human generated stories paired with writing prompts from online forum\n",
    "* Scraped Reddit API\n",
    "* Convolutional Seq2Seq with attention\n",
    "* Self-Attention for decoder to access to history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/06/2018\n",
    "# A Universal Music Translation Network\n",
    "* Paper, https://arxiv.org/pdf/1805.07848.pdf\n",
    "* Sample, https://www.youtube.com/watch?v=vdxCqNWTpUs&feature=youtu.be\n",
    "* Wavenet based\n",
    "* Denoising Autoencoder\n",
    "* Fully convolutional networks\n",
    "* One shared encoder, multiple  decdoders\n",
    "\n",
    "\n",
    "# Normalization\n",
    "## Weight Normalization\n",
    "* Paper, https://arxiv.org/pdf/1602.07868.pdf\n",
    "* Let $v$ be a weight parameter, $w = g \\frac{v}{||v||}$\n",
    "* Gradient is parpenedicular to weight\n",
    "* Weight parametres keep growing until gradient equqls to zero\n",
    "* Initialize parameters g and b with batch normalization\n",
    "* Mean only batch normalization with weight normalization: less noise and computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/03/2018\n",
    "# NLP Preprocess\n",
    "* Reference, https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html\n",
    "\n",
    "## Tokenization\n",
    "* Split text into tokens, e.g., word\n",
    "* library: nltk.tokenize.word_tokenize\n",
    "\n",
    "## Normalization\n",
    "* Stemming: eliminate affixes, e.g., running => run\n",
    "* Lemmatization: capture the canonical form, e.g., better => good\n",
    "* Correct contraction: https://github.com/ian-beaver/pycontractions\n",
    "* Everything else: lower case, remove stop words, remove punctuation, etc.\n",
    "\n",
    "# Dialog Datasets\n",
    "## Daily Dialog\n",
    "* Paper, https://arxiv.org/pdf/1710.03957.pdf\n",
    "* Dataset, http://yanran.li/dailydialog\n",
    "* Crawled from the service for English learner\n",
    "* Approximately, 8 Turns on the average\n",
    "* 13,118 multi-turn dialogs\n",
    "* Use spelling autocorrect, https://github.com/phatpiglet/autocorrect/\n",
    "* Annotate the type of conversation, e.g., Questions\n",
    "* Twitter, Weibo, and other social media are not proper to learn dialogs\n",
    "* Label for emotion\n",
    "\n",
    "## Multi Domain-Aware\n",
    "* Paper, https://arxiv.org/pdf/1704.00200.pdf\n",
    "* Created after interviewed with sellers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/31/2018\n",
    "# Network\n",
    "## Router\n",
    "* Configure LAN\n",
    "* Wifi or Wired Connection\n",
    "\n",
    "## Modem\n",
    "* Connect to the Internet\n",
    "* Transform signal from digital to analog to use phone cable\n",
    "\n",
    "\n",
    "# YOLO\n",
    "## V1\n",
    "* Paper, https://pjreddie.com/media/files/papers/yolo_1.pdf\n",
    "* Predict bounding boxes and their class probabilities at the same time\n",
    "* 45 frames per second on Titan X GPU\n",
    "* Divides input image into $S \\times S$\n",
    "* Predict B bounding boxes for each cell\n",
    "* Each bounding box consists of 5 predictions: x, y, z, w and confidence\n",
    "* Confidence is equivalent to IoU\n",
    "* Multi Task Loss with class probability, coordinates, etc.\n",
    "* Unable to predict a lot of bounding boxes in a small region\n",
    "\n",
    "\n",
    "## V2\n",
    "* Paper, https://arxiv.org/pdf/1612.08242.pdf\n",
    "* Nice blog post, http://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html\n",
    "* Consider larger number of classes\n",
    "* Introduce batch normalization\n",
    "* Fine tune pretrained AlexNet with hight resolution $448 \\times 448$\n",
    "* Use $416 \\times 416$ input images\n",
    "* Use anchors to predict bounding boxes\n",
    "* Anchors: predict bounding boxes based on scale and aspect ratio\n",
    "* Predict anchor prior with IoU distance K-Means\n",
    "* Predict boxes by parameterization\n",
    "* Hierarchical label based on WordTree\n",
    "\n",
    "## V3\n",
    "* Paper, https://pjreddie.com/media/files/papers/YOLOv3.pdf\n",
    "* Nice blog post, https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b\n",
    "* Tutorial, https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/\n",
    "* Classification is learned through cross entropy\n",
    "* Good at classifying small objects\n",
    "* Three scales detection similar to Feature Pyramid Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/30/2018\n",
    "# Non-Maximal Suppression\n",
    "* Nice blog in Japanese, https://meideru.com/archives/3538\n",
    "* Python Implementation, https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/\n",
    "* Compute $IoU = \\frac{Intersection}{Union}$\n",
    "* Get rid of boxes with higher IoU\n",
    "\n",
    "# URL and URI\n",
    "* URL: Address of resource\n",
    "* URN: Name on the Internet like serial number\n",
    "* URI: Includes both URL and URL, http:... is not included in URL but in URI\n",
    "\n",
    "# HTTP Methods\n",
    "## GET\n",
    "* For getting data such as HTML, images, movies, etc.\n",
    "* Easy to see what you are sending from query\n",
    "* URL is followed by '?' and parameters are separated by '&'\n",
    "* e.g., http://www.google.co.jp/search?q=Yahoo&lr=lang_ja&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:ja:official&client=firefox-a\n",
    "\n",
    "## POST\n",
    "* For sending data from a client to a server\n",
    "* Unable to see what you are sending from query\n",
    "* Able to send large data\n",
    "* Data is send as a body not included in URL\n",
    "\n",
    "\n",
    "## Other methods\n",
    "* HEAD: Get only head data\n",
    "* PUT: Make and replace URI\n",
    "* DELETE: Delete contents of URI\n",
    "* OPTIONS: Get usable methods for a given URI\n",
    "\n",
    "\n",
    "# VPN\n",
    "* Reference1, https://searchnetworking.techtarget.com/definition/virtual-private-network\n",
    "* Reference2, https://www.howtogeek.com/133680/htg-explains-what-is-a-vpn/\n",
    "* Virtual Private Network\n",
    "* Safe and encrypted connection over a less secure network\n",
    "* There are several different protocols: IP security, SSL and TLS, PPTP, etc.\n",
    "* Connects your device to the server somewhere on the Internet and browse using its Internet connection\n",
    "* Remote Access\n",
    "    * Uses public telecommunication infrastructure like the Internet\n",
    "    * E.g., Wi-Fi hotspot, Access to region-restricted websites\n",
    "    * Gateway => Authenticate => Creates a network link back to the device => Reach the internal resources\n",
    "\n",
    "# DropConnect\n",
    "* Paper, http://proceedings.mlr.press/v28/wan13.pdf\n",
    "* Masking on weight instead of input units\n",
    "* When masking matrix is the certain form, it is equivalent to dropout\n",
    "* Generalization of DropOut\n",
    "* Moment matching and sampling at the inference stage\n",
    "\n",
    "# Adding One Neuron Can Eliminate All Bad Local Minima\n",
    "* Paper, https://arxiv.org/pdf/1805.08671.pdf\n",
    "* Two assumptions\n",
    "    * Monotonically increasing smooth function\n",
    "    * Able to label of training set completely\n",
    "* Add exponential unit and its regularization to loss\n",
    "* Considered as skip connection\n",
    "* Every local minimum is global minimum and it maximize accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/29/2018\n",
    "# Small steps and giant leaps\n",
    "* Paper, https://arxiv.org/pdf/1805.08095.pdf\n",
    "* Estimate second order gradient through Gauss Newton Approximation: $ \\hat{H} = J_{\\phi} H_L J_{\\phi}^T$\n",
    "* Compute through backkward and forward propagation\n",
    "* Nail down to the following optimization problems:\n",
    "$$\\Delta_w = \\underset{z}{argmin}\\ \\hat{f}(z) = \\underset{z}{argmin}\\frac{1}{2} z^T H z + z^T J$$\n",
    "* Replace $J$ with $\\hat{H}z + J$\n",
    "* Use momentum method\n",
    "* Stabilize and achieve better results than previous GD methods\n",
    "* Tune regularization parameter $\\lambda$ depending on the real and guess objective change rate: $\\gamma = \\frac{f(w + z) - f(w)}{\\hat{f}(z)}$\n",
    "If $\\gamma$ is big, reduce $\\lambda$, and vice versa.\n",
    "* Other hyperparameters parameters are tuned by solving the linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/28/2018\n",
    "# Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields\n",
    "* Paper, https://arxiv.org/pdf/1611.08050.pdf\n",
    "* Use keypoint and affinity\n",
    "* Multi stage CNN\n",
    "* Based on COCO 2016 keypoints challenge\n",
    "* Learn confidence map for keypoints and affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/27/2018\n",
    "# Attention\n",
    "* Task: embed sentence into fixed length vector representation\n",
    "* Problem: difficult to store long historical dependency (Bidirectional RNN is one of the solutions)\n",
    "* Similar to memory retrieval\n",
    "* Blog post, http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n",
    "\n",
    "## Examples\n",
    "* Additive, https://arxiv.org/pdf/1409.0473.pdf\n",
    "    * Context Vector: Produced by simply averaging hidden states of source inputs\n",
    "* Multiplicative, https://arxiv.org/pdf/1508.04025.pdf\n",
    "    * Use Global and Local Attention\n",
    "    * Global: Based on similarity scores\n",
    "    * Local: Predict the center of location to look at and assign Gaussian distribution\n",
    "    * Concatenate hidden state of target and context vector to produce hidden representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/26/2018\n",
    "# Attention is All You Need\n",
    "* Paper, https://arxiv.org/pdf/1706.03762.pdf\n",
    "* Encoder\n",
    "    * Stack of six identical layers, N=6\n",
    "    * (SelfAttention) + (LN + Res) + (FF) + (LN + Res)\n",
    "* Decoder\n",
    "    * Stack of six identical layers, N=6\n",
    "    * (SelfAttention) + (LN + Res) + (SelfAttention) + (LN + Res) + (FF) + (LN + Res)\n",
    "* Scaled Dot-Product Attention\n",
    "    * Queries: Q, shape=(num_queries, $d_k$)\n",
    "    * Keys: K, shape=(num_stored, $d_k$)\n",
    "    * Values: V, shape=(num_stored, $d_v$)\n",
    "    * $Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) \\cdot V$\n",
    "* Multi-Head Attention\n",
    "    * h times different linear projection and their attention\n",
    "    * $d_{model}$ is output dimension\n",
    "    * $MultiHead(Q, K, V) = Concat(head_1, \\dots, head_h) \\cdot W^o$, where \n",
    "    $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "    * In the paper, $h=8$, $d_k=d_v=d_{model}/h=64$\n",
    "* Three Usages of Multi-Head Attention\n",
    "    * Q: the previous decoder layer, K and V: output of the encoder\n",
    "    * Self-attention at the encoder, Q, K and, V: previous layer\n",
    "    * Self-attention at the decoder, Q, K and, V: input masked out to prevent information leak\n",
    "* Masking: Set logit values to -inf\n",
    "* Share the same weight between embedding and pre-softmax weights\n",
    "* Positional Encoding: Adding sinodal values of index to input\n",
    "* Training\n",
    "    * WMT2014 English-German, 4.5M sentences, 37K tokens, 100K steps, 0.4sec/step, 12 hours\n",
    "    * WMT 2014 English-French, 36M sentences, 32K word-piece vocabulary, 300K steps, 1.0sec/step, 3.5 days\n",
    "    * Batched together by approximate sequence length\n",
    "    * Each training batch contains 25K source tokens and 25K target tokens\n",
    "    * 8 NVIDIA P 100 GPUs\n",
    "* Regularization\n",
    "    * Residual Dropout: Dropout before added to input. p=0.1\n",
    "    * Label smoothing with e=0.1\n",
    "\n",
    "\n",
    "# Layer Normalization\n",
    "* Paper, https://arxiv.org/pdf/1607.06450.pdf\n",
    "* Able to apply to RNN\n",
    "* Take mean and variance over hidden units not batch\n",
    "* The same behavior in training and test phase\n",
    "* Works good for longer sequence and small batch\n",
    "\n",
    "# Label Smoothing\n",
    "* Paper, https://arxiv.org/pdf/1512.00567.pdf\n",
    "* let label $y=k$, number of class is $K$,\n",
    "$$ Loss = -(1 - \\epsilon) log(p(k)) - \\frac{\\epsilon}{K} \\sum_{i=1}^K log(p(i)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/25/2018\n",
    "# Feature Pyramid Networks for Object Detection\n",
    "* Paper, https://arxiv.org/pdf/1612.03144.pdf\n",
    "* Each convolution takes 2 strides\n",
    "* In bottom up path, we use residual connection with conv $1 \\times 1$\n",
    "* Produce features for each scale\n",
    "* Final layers use conv $3 \\times 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/22/2018\n",
    "# Image Segmentation\n",
    "* Good reference, https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4\n",
    "\n",
    "## Selective Search\n",
    "* Paper, https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf\n",
    "* Use various simlarity metrics such as texture, color, and size\n",
    "* Merge pairs of regions from one with the largest similariy \n",
    "\n",
    "## R-CNN\n",
    "* Paper, https://arxiv.org/pdf/1311.2524.pdf\n",
    "* Region proposals\n",
    "    * Based on [Selective Search](https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf)\n",
    "    * Roughly, 2000 proposals\n",
    "* Feature extraction with CNN\n",
    "* Classification by SVM\n",
    "* Improves proposals accuracy by regression\n",
    "\n",
    "## Fast R-CNN\n",
    "* Paper, https://arxiv.org/pdf/1504.08083.pdf\n",
    "* Feature Extraction\n",
    "    * Produces a conv feature map: processes the whole image with CNN + pooling\n",
    "    * Extracts a fixed-length feature for each proposal object with a region of interest (RoI)\n",
    "    * We do not repeat the first process for each region proposal, which makes it faster to compute\n",
    "    * Choose features by considering the ratio of image sizes and actual proposals coordinates\n",
    "* Two modules for outputs\n",
    "    * Softmax probability to classify the object ('background' class is included)\n",
    "    * 4 values to define a bounding box\n",
    "* The RoI pooling layer\n",
    "    * Defined by a four-tuple (r, c, h, w): top-left corner (r, c) and its height and width (h, w)\n",
    "    * Splits into $H \\times W$ grid sub-windows, where each size is $h/H \\times w/W$\n",
    "    * Uses max pooling to generate $H \\times W$ features\n",
    "    * Try to sample regions from the same image for computational efficient training\n",
    "    * Output is fixed size\n",
    "* Initializing from pre-trained networks\n",
    "* Multi-task loss\n",
    "    * $p = (p_0, p_1, \\dots, p_K)$\n",
    "    * $t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$\n",
    "    * u is ground truth label\n",
    "    * v is ground truth box coordinates\n",
    "    * $L(p, u, t^u, v) = L_{cls}(p, u) + \\lambda \\bf{1}_{[u \\geq 1]} L_{loc}(t^u, v)$\n",
    "* Mini-batch sampling\n",
    "    * Object proposal is the same as R-CNN\n",
    "    * 25% of the RoI from object proposals that have IoU with a ground truth at least 0.5\n",
    "    * IoU between $[0.1, 0.5)$ => background label\n",
    "    * Horizontally flipped with probability 0.5\n",
    "* Get rid of SVM from R-CNN\n",
    "\n",
    "## Faster R-CNN\n",
    "* Paper, https://arxiv.org/pdf/1506.01497.pdf\n",
    "* No need to use selective search for region proposals\n",
    "* Use two modules\n",
    "    * CNN proposal regressor\n",
    "    * CNN binary or two-class classifier to predict if it is object or not\n",
    "* Use attention to tell Fast R-CNN module where to look\n",
    "* Anchor\n",
    "    * Slide small $n \\times n$ windows\n",
    "    * Proposes k boxes for each sliding point\n",
    "    * Use scale and ratio parameters, example $k=9$ for 3 scales and 3 ratios\n",
    "* Loss Function\n",
    "    * Assign positive labels to the highest IoU anchor or the one with IoU higher than 0.7\n",
    "    * Could assign multiple positive labels for a single box\n",
    "    * Assign negative labels to anchor with IoU lower than 0.3\n",
    "* Training\n",
    "    1. RPN is initialized by ImageNet model and fine tuned for region proposal task\n",
    "    2. Train Fast R-CNN detection network with fine tuned RPN\n",
    "    3. Initialize RPN with the detector network and fine tune only layers after feature extraction\n",
    "    4. Fine tune only layers unique to the detector network\n",
    "    \n",
    "## Fully Convolutional Network\n",
    "* Paper, https://arxiv.org/pdf/1605.06211.pdf\n",
    "* Receptive field: The size taken by kernel, e.g., $3 \\times 3$\n",
    "* Replace the fully connected last layer of classifier CNN network with convolution\n",
    "* Use skip connection from the middle layer to pass denser information\n",
    "* Upsampling is in-network, not deconvolution\n",
    "* Output of each pixel is softmax whose output corresponds to each class\n",
    "    \n",
    "## Mask R-CNN\n",
    "* Paper, https://arxiv.org/pdf/1703.06870.pdf\n",
    "* Improvement\n",
    "    * Apply fully convolutional network (FCN) to RoI\n",
    "    * RoIAlign instead of RoIPool\n",
    "* Output: $K \\times m^2$\n",
    "* Calculate binary probability unlike FCN\n",
    "* RoIAlign: Calculate fractional index value by bilinear interpolation\n",
    "* Use Feature Pyramid Network\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/19/2018\n",
    "# Object Detection\n",
    "* Metrics: IoU (Intersection of Union) = (Intersection / Union), which range between 0 and 1\n",
    "\n",
    "# Face Detection using Haar Cascades\n",
    "- Based on base functions like Haar functions\n",
    "- AdaBoost\n",
    "- Weak learner is based on simple threshold for each feature\n",
    "- Used when speed is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/18/2018\n",
    "# NN Architecture\n",
    "- Residual Net\n",
    "    - Projection parameters increases accuracy only slightly\n",
    "    - Identity with zero padding is good enough\n",
    "- Stochastic Depth\n",
    "    - Skip the mapping\n",
    "    - Survival rate decays linearly along with depth\n",
    "    - When network is large, 25% depth reduction expectedly\n",
    "    - In test time, multiply probability to scale down\n",
    "- Dense Net\n",
    "    - Connect to all layers, L(L + 1) / 2 connection\n",
    "    - Concatenation of previous features\n",
    "    - Dense Block: (BN + ReLU + Conv 1*1 + BN + ReLU + Conv 3*3) * L\n",
    "    - Conv 1*1 is feature selection\n",
    "    - Transition Layer: Conv 1*1 + Pooling\n",
    "    - You may reduce the number of features at transition layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GPU\n",
    "- Simple Multiple cores\n",
    "- CPU optimizes single execution\n",
    "- CPU - ferrari, GPU - freight truck\n",
    "- Fetch bunch of memory, e.g., 750GB/s while CPU 50GB/s\n",
    "- Takes time for only initial fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/14/2018\n",
    "# CAPM\n",
    "## Mean Variance Optimization\n",
    "* Efficient Frontier: A curve of maximized expected return given risk, standard deviation\n",
    "* Sharpe Ratio: $\\frac{E_H - r_f}{\\sigma_H}$, where $E_H$ and $\\sigma_H$ are the expected return and standard deviation of return of stock H, respectively\n",
    "* The highest Sharpe ration is achieved by the setting point of Efficient Frontier and linear line\n",
    "\n",
    "## Improve Sharpe Ratio\n",
    "* Adding an Uncorrelated Stock: if its amount is small and $E_S > r_f$, expected return keep the same and risk reduces\n",
    "* Adding a Perfectly Correlated Stock: $E_S - r_f = \\alpha + \\beta (E_P - r_f)$\n",
    "    - If $\\alpha > 0$, stock has more risk premium and improve the Sharpe ratio\n",
    "* Adding a Imperfectly Correlated Stock:\n",
    "    - Correlated part: Mimicked through the same way\n",
    "    - Uncorrelated part: Risk is diversified away\n",
    "* $E_S - r_f > \\beta (E_P - r_f)$ implies buying the stock improves the Sharpe ratio\n",
    "\n",
    "\n",
    "## Equilibrium\n",
    "* $E_S - r_f = \\beta (E_P - r_f)$ for all stocks in the market\n",
    "* The Sharpe Ratio of any asset is no higher than the Sharpe Ratio of the market portfolio\n",
    "\n",
    "\n",
    "## Three Implications\n",
    "* The expected return of an asset does not depend on its stand-alone risk, but depends on beta\n",
    "* Offers the method to measure the risk that cannot be diversified away\n",
    "* The expected return of an asset does not depend on the growth rate of its expected future cash flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/13/2018\n",
    "# Spark\n",
    "## Shared Variables\n",
    "* Broadcast Variables\n",
    "    - Used for common data like input\n",
    "    - Cached on each machine to reduce communication cost\n",
    "* Accumulators\n",
    "    - Only added through an associative and commutative operation\n",
    "    - Used to implement sum or counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/10/2018\n",
    "# Data Structures\n",
    "## Array\n",
    "- Fixed size of memories\n",
    "- Fast for index access\n",
    "- Slow to insert element because you have to move all elements, worst case O(N)\n",
    "\n",
    "## List\n",
    "- Each element has forward and backward element's address\n",
    "- Variable length\n",
    "- Slow for index access\n",
    "- Fast to insert element\n",
    "- Fast to access to head or tail\n",
    "\n",
    "\n",
    "# Scala\n",
    "## Introduction \n",
    "- Reference, https://docs.scala-lang.org/tour/tour-of-scala.html\n",
    "\n",
    "## Infinity\n",
    "- Double.PositiveInfinity\n",
    "- Double.NegativeInfinity\n",
    "\n",
    "## Seq\n",
    "- Seq has both LinkedList and ArrayList traits\n",
    "- List is an implementation of LinkedList, which means fast to access to head or tail\n",
    "\n",
    "## Trait\n",
    "- Instanciation: new YourTrait{}\n",
    "- Used for mixin\n",
    "- Not define initialize parameters\n",
    "- Reference, https://qiita.com/f81@github/items/5b96af593812286eec49\n",
    "\n",
    "## Implicit Class\n",
    "- Automatically detect the case of application and apply it to that case\n",
    "- Reference, https://qiita.com/miyatin0212/items/f70cf68e89e4367fcf2e\n",
    "\n",
    "## Unit\n",
    "- The same as void of C++ and C\n",
    "\n",
    "\n",
    "# Install Command\n",
    "## dpkg\n",
    "- Low-level system tool to extract, analyse, unpack and install or remove .deb file\n",
    "- Expertly handles Debian packages, rather than their dependencies\n",
    "- The difference from apt-get, https://unix.stackexchange.com/questions/104592/what-is-the-difference-between-apt-get-and-dpkg\n",
    "    - Dependency Management: Search the meta data related to packages either locally or remotely.\n",
    "    - apt-get makes use of dpkg\n",
    "- Reference, https://askubuntu.com/questions/173465/what-is-dpkg-for\n",
    "\n",
    "## apt-get\n",
    "- APT (the Advanced Packaging Tool): The Debian .deb software packaging system\n",
    "- The command-line tool for working with APT software packages\n",
    "- Makes use of dpkg to do the actual package installations\n",
    "- Understand dependency and install something if necessary\n",
    "- Reference, https://www.computerhope.com/unix/apt-get.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/08/2018\n",
    "# Computer Properties\n",
    "## Terminologies\n",
    "* Socket\n",
    "    - CPU: Connection that allows CPU to be connected to a motherboard\n",
    "    - Programming Language: communication between processors or a client and server\n",
    "    - Network: THe end points of communications between processes on a computer network such as the Internet\n",
    "    \n",
    "* Cache: To store data locally in order to speed up subsequent retrievals\n",
    "* Cache Memory: Memory bank that bridges main memory and the processor\n",
    "   \n",
    "## The output of lscpu\n",
    "* CPU: Physical processor\n",
    "* Socket: The number of sockets/slots for physical processors\n",
    "* Core: The number of cores per processor\n",
    "* Thread per core: The number of threads per core\n",
    "\n",
    "\n",
    "\n",
    "# Spark\n",
    "## Spark Introduction\n",
    "* Developed by Matei Zaharia @ UC Berkeley, 2012\n",
    "* Flexibility and extensibility of MapReduce\n",
    "* Faster than Hadoop 100 times when data is stored and 10 times when accessing disk\n",
    "* Spark job is associated with a chain of object dependencies organized in a direct acyclic graph (DAG)\n",
    "* A blog post about features, https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html\n",
    "\n",
    "## RDD (Resilient Distributed Dataset)\n",
    "* A collection of distributed immutable Java Virtual Machine (JVM) objects\n",
    "* Python data is stored within these JVM objects\n",
    "* Calculated against, cached, and stored in-memory\n",
    "* Significantly slower in Python\n",
    "* Transformations: map, filter, reduce, etc.\n",
    "* Actions: collect, take, etc.\n",
    "\n",
    "## DataFrame\n",
    "* Name columns\n",
    "* Cover the performance gap across all the languages\n",
    "* Fast query performance due to the Spark SQL Engine's Catalyst Optimizer and Project Tungsten\n",
    "* .cache(): Put RDD into memory, which makes execution faster\n",
    "    - Used for iterative machine learning application\n",
    "    - Used for stand alone application\n",
    "* .persist(level: StorageLevel): Put RDD into disk\n",
    "* .persist() is the same as cache\n",
    "\n",
    "## Datasets\n",
    "* Only for Java and Scala\n",
    "* Type-safe\n",
    "* API is unified with DataFrame from Apache SPark 2.0\n",
    "\n",
    "## Spark SQL\n",
    "* SQL interface\n",
    "* Return results with DataFrame/Datasets interface\n",
    "\n",
    "## Catalyst Optimizer\n",
    "* Powers both spark SQL and DataFrame\n",
    "* Add new optimizations and features to SparkSQL\n",
    "* Extend optimizer\n",
    "\n",
    "\n",
    "## Spark Streaming\n",
    "* Real time analysis https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/\n",
    "\n",
    "## Scheduling\n",
    "* SparkContext\n",
    "    - Set up internal services\n",
    "    - Establish a connection to Spark execution environment\n",
    "    - Organize schedulers, graphs, etc.\n",
    "* DAGScheduler: Scheduler based on Jobs and Stages\n",
    "    - Computes an execution DAG\n",
    "    - Determines the preferred location to run each task on\n",
    "    - Handles failure\n",
    "* Job: A work item submitted to DAGScheduler to compute the result of action\n",
    "* Stage: A set of parallel tasks, physical unit of execution\n",
    "* RDD => Action => Job => Stage\n",
    "\n",
    "## PySpark vs Spark (Scala)\n",
    "* Reference, https://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python\n",
    "* Computational overhead of PySpark\n",
    "    - Overhead of JVM communication\n",
    "    - Process-based execution (thread based for Scala)\n",
    "    - Performance of python code itself\n",
    "    - Reference counting used as the first line garbage collection method in CPython\n",
    "* E.g., UDF would slows your application because it requires moving data back and forth\n",
    "* Pure Python methods can slow down your applications\n",
    "* If you want to build complex analytics, go for Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/07/2018\n",
    "# Mindset to get over  fear of talking to someone\n",
    "* ‘Be who you are and say what you feel, because those who matter don’t mind, and those that mind, don’t matter.’ – Theodor Seuss Geisel\n",
    "* ‘Women love to be approached, so I will be a nice guy today and do something good by approaching her’.\n",
    "* Make an eye contact\n",
    "* Say hello to strangers\n",
    "\n",
    "# Quote to conquer fear \n",
    "\n",
    "1. \"Don't wait until everything is just right. It will never be perfect. There will always be challenges, obstacles, and less than perfect conditions. So what? Get started now. With each step you take, you will grow stronger and stronger, more and more skilled, more and more self-confident, and more and more successful.\" — Mark Victor Hansen\n",
    "\n",
    "2. \"Speak up. Believe in yourself. Take risks.\" — Sheryl Sandberg\n",
    "\n",
    "3. \"Inaction breeds doubt and fear. Action breeds confidence and courage. If you want to conquer fear, do not sit home and think about it. Go out and get busy.\" — Dale Carnegie\n",
    "\n",
    "4. \"All the great speakers were bad speakers at first.\" — Ralph Waldo Emerson\n",
    "\n",
    "5. \"Do the best you can until you know better. Then when you know better, do better.\" — Maya Angelou\n",
    "\n",
    "6. \"Always remember you are braver than you believe, stronger than you seem, and smarter than you think.\" — Christopher Robin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04/29/2018\n",
    "\n",
    "# Unit Root\n",
    "[Quora: What is unit root in a time series?](https://www.quora.com/What-is-unit-root-in-a-time-series)\n",
    "[Wikipedia: Unit Root](https://en.wikipedia.org/wiki/Unit_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04/25/2018\n",
    "# P-NP problem\n",
    "Please refer to [this reference](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa).\n",
    "### P (Polynomial Time)\n",
    "* Can be solved in polynomial time\n",
    "\n",
    "### NP (Non-deterministic Polynomial Time)\n",
    "* If given yes instance, you can check if it is true in polynomical time\n",
    "* You are not sure if it can be solve in polynomial time\n",
    "\n",
    "### NP-Complete\n",
    "* NP problem that any NP problems can be reduce to in polynomial time\n",
    "\n",
    "### NP-Hard\n",
    "* Not have to be in NP\n",
    "* Not Have to decision problem\n",
    "* Intuitively, At least as hard as NP-Complete\n",
    "* Rigorously, if there is a NP-complete problem reducible to in polynomial time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
