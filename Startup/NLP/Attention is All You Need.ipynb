{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        x = self.src_embed(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.tgt_embed(x)\n",
    "        x = self.decoder(x)\n",
    "        return self.generator(x)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        _src = self.src_embed(src)\n",
    "        return self.encoder(_src, src_mask)\n",
    "    \n",
    "    def decode(self, memory, tgt, src_mask, tgt_mask):\n",
    "        _tgt = self.tgt_embed(tgt)\n",
    "        return self.decoder(memory, _tgt, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.bias = nn.Parameter(torch.zeros(size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.scale * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        size = layer.size\n",
    "        self.norm = LayerNorm(size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, drop_prob):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, x, layer):\n",
    "        next_x = layer(self.norm(x))\n",
    "        return x + self.dropout(next_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, drop_prob):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.sublayers = clones(SublayerConnection(size, drop_prob), 2)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        x = self.sublayers[1](x, self.feed_forward)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        size = layer.size\n",
    "        self.norm = LayerNorm(size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, drop_prob):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn  = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayers = clones(SublayerConnection(size, drop_prob), 3)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayers[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        x = self.sublayers[2](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  1,  1,  1],\n",
       "        [ 0,  0,  1,  1,  1],\n",
       "        [ 0,  0,  0,  1,  1],\n",
       "        [ 0,  0,  0,  0,  1],\n",
       "        [ 0,  0,  0,  0,  0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e67fd7940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "mask = subsequent_mask(5)\n",
    "mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    # qurey.size == (n_batch, input_length, n_head, key_dim)\n",
    "    # key.size == (n_batch, memory_length, n_head, key_dim)\n",
    "    # value.size == (n_batch, memory_length, n_head, value_dim)\n",
    "    q_k = torch.matmul(query, key.transpose(-1, -2))\n",
    "    key_dim = query.size()[-1]\n",
    "    scores = q_k / np.sqrt(key_dim)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "    # attention.size == (n_batch, input_length, memory_length)\n",
    "    attention = F.softmax(scores)\n",
    "    if dropout is not None:\n",
    "        attention = dropout(attention)\n",
    "    # output.size == (n_batch, input_length, value_dim)\n",
    "    output = torch.matmul(attention, value)\n",
    "    return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.h_dim = d_model // h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 3)\n",
    "        self.o_linear = nn.Linear(h * self.h_dim, d_model)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        outputs = list()\n",
    "        n_batch = query.size[0]\n",
    "        inputs = (query, key, value)\n",
    "        projs = list()\n",
    "        for x, lin in zip(inputs, self.linears):\n",
    "            x = lin(x)\n",
    "            # Split for multi head\n",
    "            x = torch.cat(torch.chunk(x, dim=-1), dim=0)\n",
    "            projs.append(x)\n",
    "        proj_q, proj_k, proj_v = projs\n",
    "        output, self.attn = attention(proj_q, proj_k, proj_v,\n",
    "                                      mask=mask, dropout=self.dropout)\n",
    "        self.attn = torch.cat(torch.chunk(self.attn, dim=0), dim=-1)\n",
    "        output = torch.cat(torch.chunk(output, dim=0), dim=-1)\n",
    "        return self.o_linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(d_model, d_ff)\n",
    "        self.lin2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def feed_forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.lin2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def feed_forward(self, x):\n",
    "        return self.embeds(x) * np.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, drop_prob, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # Positional Encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        const = 1e4\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\\\n",
    "                             -(np.log(const) / d_model))\n",
    "        div_term = div_term.unsqueeze(0)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, x.size(1)],\n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6,\n",
    "               d_model=512, d_ff=2048, h=8, drop_prob=0.1):\n",
    "    attn = MultiHeadedAttention(h, d_model, drop_prob)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, drop_prob)\n",
    "    pe = PositionalEncoding(d_model, drop_prob)\n",
    "    c = deepcopy\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), drop_prob), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), drop_prob), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(pe)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(pe)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(10, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, tgt=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src == pad).unsqueeze(2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            tgt_mask = \\\n",
    "                self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y == pad).data.sum()\n",
    "            \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt == pad).unsqueeze(-2)\n",
    "        ttgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 4])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(1, 2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 4])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(2, 1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3,  3],\n",
       "        [ 0,  0,  3],\n",
       "        [ 0,  0,  0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7528e-01,  1.0000e-09,  1.0000e-09],\n",
       "        [-5.0348e-01, -3.4601e-01,  1.0000e-09],\n",
       "        [ 2.5800e+00,  7.9626e-01, -5.0441e-01]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.masked_fill(mask / 3, 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0,  0,  0,  0],\n",
       "        [ 1,  1,  0,  0,  0],\n",
       "        [ 1,  1,  1,  0,  0],\n",
       "        [ 1,  1,  1,  1,  0],\n",
       "        [ 1,  1,  1,  1,  1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((5, 5))\n",
    "torch.from_numpy(np.triu(x, k=1)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function chunk:\n",
      "\n",
      "chunk(...)\n",
      "    chunk(tensor, chunks, dim=0) -> List of Tensors\n",
      "    \n",
      "    Splits a tensor into a specific number of chunks.\n",
      "    \n",
      "    Last chunk will be smaller if the tensor size along the given dimension\n",
      "    :attr:`dim` is not divisible by :attr:`chunks`.\n",
      "    \n",
      "    Arguments:\n",
      "        tensor (Tensor): the tensor to split\n",
      "        chunks (int): number of chunks to return\n",
      "        dim (int): dimension along which to split the tensor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 15, 30])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(10, 30)\n",
    "x = torch.randn(20, 15, 10)\n",
    "lin(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function unsqueeze:\n",
      "\n",
      "unsqueeze(...)\n",
      "    unsqueeze(input, dim, out=None) -> Tensor\n",
      "    \n",
      "    Returns a new tensor with a dimension of size one inserted at the\n",
      "    specified position.\n",
      "    \n",
      "    The returned tensor shares the same underlying data with this tensor.\n",
      "    \n",
      "    A negative `dim` value within the range\n",
      "    [-:attr:`input.dim()`, :attr:`input.dim()`) can be used and\n",
      "    will correspond to :meth:`unsqueeze` applied at :attr:`dim` = :attr:`dim + input.dim() + 1`\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        dim (int): the index at which to insert the singleton dimension\n",
      "        out (Tensor, optional): the output tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.tensor([1, 2, 3, 4])\n",
      "        >>> torch.unsqueeze(x, 0)\n",
      "        tensor([[ 1,  2,  3,  4]])\n",
      "        >>> torch.unsqueeze(x, 1)\n",
      "        tensor([[ 1],\n",
      "                [ 2],\n",
      "                [ 3],\n",
      "                [ 4]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.unsqueeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3807,  0.0561,  0.2692,  ...,  0.6745,  0.7825,  0.5816],\n",
       "          [-0.5833,  1.4313, -0.0649,  ...,  0.5097,  1.1076,  0.4495],\n",
       "          [ 1.4976,  1.3701, -0.3923,  ..., -2.7077,  0.8734,  0.4760],\n",
       "          ...,\n",
       "          [-0.9734,  1.5111,  0.5513,  ...,  1.5626,  0.0551, -0.3939],\n",
       "          [-0.7921,  2.0839,  0.8382,  ...,  2.0039, -1.2792, -0.5473],\n",
       "          [-0.7719, -1.2014, -2.1645,  ..., -0.5173,  0.0699,  0.4990]],\n",
       "\n",
       "         [[ 0.5323,  0.2479,  0.1900,  ..., -0.2317, -1.0883,  0.4674],\n",
       "          [-1.1096,  0.9642, -0.2402,  ...,  0.3732, -0.3986,  0.0554],\n",
       "          [ 0.2246, -1.5680, -1.5758,  ..., -0.0553, -0.1274,  0.3998],\n",
       "          ...,\n",
       "          [-0.4647,  1.9455,  1.3348,  ...,  0.9216,  0.4980, -0.1353],\n",
       "          [ 0.8853,  0.5545,  1.1105,  ...,  0.6940, -0.8288, -1.3541],\n",
       "          [ 0.8021,  1.9052,  0.4606,  ...,  0.1953,  0.9813, -2.7392]],\n",
       "\n",
       "         [[-1.6473, -0.1218,  0.3845,  ...,  0.3449,  1.7841,  0.5223],\n",
       "          [-0.9522,  0.5693,  0.0689,  ..., -1.0789, -0.1012, -0.2586],\n",
       "          [-0.5824,  0.8058,  0.3641,  ...,  0.7599, -0.9570,  1.1071],\n",
       "          ...,\n",
       "          [-0.4065,  0.1859,  0.4694,  ...,  0.5535,  0.1820, -0.3711],\n",
       "          [-1.0617, -1.1622,  0.5715,  ...,  1.5812, -0.2236,  0.3312],\n",
       "          [-1.6451,  0.1714,  0.2191,  ..., -0.9856,  0.2259, -0.8207]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.9872,  1.6817, -0.1288,  ..., -0.1299,  1.2602, -0.5630],\n",
       "          [-1.1263,  0.0105, -0.8300,  ..., -0.8879, -0.2186,  1.6597],\n",
       "          [-0.2913, -0.9544,  0.3335,  ..., -0.6512, -1.2558, -1.0929],\n",
       "          ...,\n",
       "          [-0.1285, -1.3269, -0.0407,  ..., -1.1124,  2.1438,  2.2647],\n",
       "          [-0.1493,  0.2246,  1.2499,  ..., -1.0121,  0.1098,  0.3816],\n",
       "          [ 0.8675,  0.1715,  0.0105,  ...,  1.2401,  0.4104, -0.6868]],\n",
       "\n",
       "         [[ 0.7698,  0.4551, -2.0841,  ..., -0.6717,  0.2993, -0.8456],\n",
       "          [ 1.1521, -0.1664, -0.0055,  ..., -1.5454, -1.2749,  0.2078],\n",
       "          [ 0.8619, -0.6367,  1.7731,  ...,  0.4500, -0.2118,  0.2171],\n",
       "          ...,\n",
       "          [ 0.4019,  0.1123,  0.6276,  ..., -0.0862, -0.0215, -0.0861],\n",
       "          [ 0.9525, -1.6093,  1.5698,  ..., -0.8700, -0.2509, -0.4667],\n",
       "          [-0.6565,  0.2633,  1.6614,  ...,  0.6964, -0.4343, -0.7089]],\n",
       "\n",
       "         [[-0.2618,  0.3091, -0.4222,  ...,  1.5764, -0.8642,  0.2248],\n",
       "          [-1.0866,  1.5859,  0.6629,  ...,  1.0831, -2.6325, -0.7500],\n",
       "          [-0.1970,  1.2604, -0.0541,  ..., -0.0558, -0.9018, -0.4503],\n",
       "          ...,\n",
       "          [-0.5189, -0.5412, -0.8760,  ...,  1.2096, -0.5340, -0.1082],\n",
       "          [ 0.4817,  0.6529,  0.6966,  ..., -0.6862,  0.3718, -0.8296],\n",
       "          [-1.0324,  0.4456,  0.2568,  ..., -0.3381, -2.1556,  0.7620]]]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
