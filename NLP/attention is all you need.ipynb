{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "# texts = clean_movie_data(data)\n",
    "df = pd.read_csv(\"data/All-seasons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "texts = df[\"Line\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    We need 4 files\\n    1. train.enc : Encoder input for training\\n    2. train.dec : Decoder input for training\\n    3. test.enc  : Encoder input for testing\\n    4. test.dec  : Decoder input for testing\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "''' \n",
    "    1. Read from 'movie-lines.txt'\n",
    "    2. Create a dictionary with ( key = line_id, value = text )\n",
    "'''\n",
    "def get_id2line():\n",
    "    lines=open('data/cornell movie-dialogs corpus/movie_lines.txt', \"r\",encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        _line = line.split(' +++$+++ ')\n",
    "        if len(_line) == 5:\n",
    "            id2line[_line[0]] = _line[4]\n",
    "    return id2line\n",
    "\n",
    "'''\n",
    "    1. Read from 'movie_conversations.txt'\n",
    "    2. Create a list of [list of line_id's]\n",
    "'''\n",
    "def get_conversations():\n",
    "    conv_lines = open('data/cornell movie-dialogs corpus/movie_conversations.txt', \"r\",encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    convs = [ ]\n",
    "    for line in conv_lines[:-1]:\n",
    "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        convs.append(_line.split(','))\n",
    "    return convs\n",
    "\n",
    "'''\n",
    "    1. Get each conversation\n",
    "    2. Get each line from conversation\n",
    "    3. Save each conversation to file\n",
    "'''\n",
    "def extract_conversations(convs,id2line,path=''):\n",
    "    idx = 0\n",
    "    for conv in convs:\n",
    "        f_conv = open(path + str(idx)+'.txt', 'w')\n",
    "        for line_id in conv:\n",
    "            f_conv.write(id2line[line_id])\n",
    "            f_conv.write('\\n')\n",
    "        f_conv.close()\n",
    "        idx += 1\n",
    "\n",
    "'''\n",
    "    Get lists of all conversations as Questions and Answers\n",
    "    1. [questions]\n",
    "    2. [answers]\n",
    "'''\n",
    "def gather_dataset(convs, id2line):\n",
    "    questions = []; answers = []\n",
    "\n",
    "    for conv in convs:\n",
    "        if len(conv) %2 != 0:\n",
    "            conv = conv[:-1]\n",
    "        for i in range(len(conv)):\n",
    "            if i%2 == 0:\n",
    "                questions.append(id2line[conv[i]])\n",
    "            else:\n",
    "                answers.append(id2line[conv[i]])\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "'''\n",
    "    We need 4 files\n",
    "    1. train.enc : Encoder input for training\n",
    "    2. train.dec : Decoder input for training\n",
    "    3. test.enc  : Encoder input for testing\n",
    "    4. test.dec  : Decoder input for testing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_id2line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304713"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2line  =get_id2line()\n",
    "convs = get_conversations()\n",
    "questions, answers = gather_dataset(convs, id2line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138135\n",
      "138135\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(encoding='utf-8', decode_error='ignore', min_df=3, max_features=20000)\n",
    "x = vectorizer.fit_transform(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about': 0,\n",
       " 'after': 1,\n",
       " 'all': 2,\n",
       " 'and': 3,\n",
       " 'any': 4,\n",
       " 'anyway': 5,\n",
       " 'are': 6,\n",
       " 'as': 7,\n",
       " 'at': 8,\n",
       " 'be': 9,\n",
       " 'but': 10,\n",
       " 'by': 11,\n",
       " 'can': 12,\n",
       " 'do': 13,\n",
       " 'don': 14,\n",
       " 'down': 15,\n",
       " 'for': 16,\n",
       " 'from': 17,\n",
       " 'get': 18,\n",
       " 'have': 19,\n",
       " 'how': 20,\n",
       " 'if': 21,\n",
       " 'in': 22,\n",
       " 'is': 23,\n",
       " 'it': 24,\n",
       " 'just': 25,\n",
       " 'last': 26,\n",
       " 'let': 27,\n",
       " 'like': 28,\n",
       " 'more': 29,\n",
       " 'much': 30,\n",
       " 'new': 31,\n",
       " 'no': 32,\n",
       " 'not': 33,\n",
       " 'of': 34,\n",
       " 'on': 35,\n",
       " 'one': 36,\n",
       " 'or': 37,\n",
       " 'other': 38,\n",
       " 'out': 39,\n",
       " 'over': 40,\n",
       " 'question': 41,\n",
       " 'real': 42,\n",
       " 'right': 43,\n",
       " 'same': 44,\n",
       " 'so': 45,\n",
       " 'some': 46,\n",
       " 'that': 47,\n",
       " 'the': 48,\n",
       " 'them': 49,\n",
       " 'these': 50,\n",
       " 'think': 51,\n",
       " 'this': 52,\n",
       " 'to': 53,\n",
       " 'want': 54,\n",
       " 'was': 55,\n",
       " 'well': 56,\n",
       " 'what': 57,\n",
       " 'will': 58,\n",
       " 'with': 59,\n",
       " 'would': 60,\n",
       " 'you': 61,\n",
       " 'your': 62}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  2,  0,  0,  2,  2,  1,  1,  0,  1,  0,  0,  0,  0,  0,\n",
       "         1,  0,  1,  1,  0,  4,  8,  2,  0,  0,  0,  0,  1,  0,  0,  0,  3,\n",
       "         2,  1,  1,  0,  1,  1,  0,  0,  0,  0,  1,  1,  1,  5, 17,  1,  0,\n",
       "         1,  0,  4,  0,  0,  1,  2,  0,  1,  0,  0,  1],\n",
       "       [ 1,  1,  1,  2,  0,  1,  0,  0,  0,  1,  0,  1,  1,  1,  1,  0,  0,\n",
       "         0,  1,  1,  0,  1,  0,  0,  1,  1,  0,  0,  0,  0,  1,  1,  1,  0,\n",
       "         3,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  2,  2,  0,  1,\n",
       "         0,  0,  2,  0,  0,  1,  0,  1,  0,  0,  7,  2],\n",
       "       [ 1,  0,  0,  0,  1,  0,  0,  2,  0,  0,  0,  0,  0,  2,  0,  0,  2,\n",
       "         0,  0,  1,  0,  1,  1,  4,  2,  0,  1,  0,  2,  1,  0,  0,  0,  3,\n",
       "         6,  2,  1,  0,  0,  0,  1,  1,  0,  0,  1,  1,  1,  4,  5,  0,  0,\n",
       "         1,  3,  4,  1,  0,  1,  0,  0,  0,  3,  3,  4],\n",
       "       [ 1,  0,  1,  2,  0,  1,  0,  3,  1,  2,  1,  0,  0,  1,  0,  1,  1,\n",
       "         1,  0,  1,  1,  1,  2,  4,  1,  0,  1,  0,  0,  2,  1,  1,  0,  1,\n",
       "         4,  3,  1,  1,  1,  0,  1,  1,  1,  0,  1,  0,  0,  1, 14,  1,  0,\n",
       "         0,  2,  5,  0,  1,  0,  1,  3,  2,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  0,  0,  1,  0,  0,\n",
       "         0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  2,  0,  0,  0,  1,  1,  0,\n",
       "         0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  2,  0,  0,\n",
       "         0,  0,  1,  0,  0,  1,  0,  2,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,\n",
       "         0,  0,  0,  0,  0,  2,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  2,  0,  0,\n",
       "         1,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  2,  4,  0,  0,  1,  0,  1,  1,  1,  1,  0,  1,  0,  0,  2,\n",
       "         4,  1,  4,  0,  1,  0,  4,  3,  0,  0,  1,  1,  0,  1,  0,  2,  4,\n",
       "         3,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  4,  8,  1,  0,\n",
       "         1,  1,  3,  0,  0,  1,  0,  1,  4,  0,  1,  0],\n",
       "       [ 0,  0,  1,  2,  1,  0,  3,  0,  3,  1,  3,  1,  0,  5,  0,  0,  1,\n",
       "         0,  1,  0,  2,  2,  0,  2,  1,  0,  0,  1,  2,  0,  1,  0,  0,  7,\n",
       "         4,  0,  2,  2,  1,  0,  0,  0,  1,  1,  0,  2,  1,  2,  9,  2,  3,\n",
       "         0,  0,  1,  1,  0,  2,  1,  0,  0,  1,  5,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  0,  0,  1,  0,  2,  2,  0,\n",
       "         0,  0,  1,  0,  3,  0,  2,  4,  3,  0,  0,  2,  0,  0,  0,  0,  0,\n",
       "         1,  2,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  1,  2,  5,  0,  0,\n",
       "         0,  0,  3,  1,  0,  0,  0,  0,  1,  0,  6,  1],\n",
       "       [ 1,  1,  1,  1,  2,  0,  1,  0,  1,  2,  0,  0,  2,  0,  0,  1,  4,\n",
       "         0,  0,  0,  2,  1,  2,  2,  3,  1,  1,  0,  2,  1,  0,  1,  0,  0,\n",
       "         7,  1,  0,  1,  0,  0,  1,  1,  0,  0,  0,  0,  0,  4,  6,  0,  1,\n",
       "         0,  2,  4,  1,  1,  0,  1,  0,  2,  1,  2,  0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from hedgeable_ai.functions.preprocessing.basic_processor import BasicProcessor\n",
    "from hedgeable_ai.functions.preprocessing.clean_text import clean_text\n",
    "\n",
    "\n",
    "class Word2IndexProcessor(BasicProcessor):\n",
    "    def __init__(self, texts, is_processed=False, num_additional=2,\n",
    "                 max_dist=0, min_word_length=1,replace_neg=False, use_unknown=False):\n",
    "        _texts = []\n",
    "        lengths = []\n",
    "        self.num_additional = num_additional\n",
    "        for text in texts:\n",
    "            if is_processed:\n",
    "                words = text\n",
    "            else:\n",
    "                words = clean_text(text.lower(), max_dist=max_dist,\n",
    "                                   min_word_length=min_word_length,\n",
    "                                   replace_neg=replace_neg, use_unknown=use_unknown)\n",
    "            if len(words) > 0:\n",
    "                _texts.extend(words)\n",
    "                lengths.append(len(words))\n",
    "        lengths = list(np.cumsum(lengths))\n",
    "        lengths.insert(0, 0)\n",
    "        self.encoder = preprocessing.LabelEncoder()\n",
    "        # 0 adn 1 are taken for padding and <eos> respectively\n",
    "        indices = self.encoder.fit_transform(_texts) + self.num_additional\n",
    "        # split to sentences\n",
    "        self.data = np.array([indices[lengths[i]:lengths[i+1]] for i in range(len(lengths) - 1)])\n",
    "        \n",
    "    def encode(self, text):\n",
    "        words = clean_text(text.lower(), max_dist=0, min_word_length=1,replace_neg=False, use_unknown=False)\n",
    "        words = [word for word in words if word in self.encoder.classes_]\n",
    "        if words:\n",
    "            index = np.array(self.encoder.transform(words)) + self.num_additional\n",
    "        else:\n",
    "            index = np.array([])\n",
    "        return index\n",
    "    \n",
    "    def decode(self, index):\n",
    "        return [self.encoder.inverse_transform(i-self.num_additional) for i in index if i >=self.num_additional]    \n",
    "            \n",
    "    \n",
    "    def batch_process_test(self, X, y=None):\n",
    "        if y is None:\n",
    "            return np.array([self.encode(x_i) for x_i in X])\n",
    "        else:\n",
    "            return np.array([self.encode(x_i) for x_i in X]), np.array(y)\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.encoder.classes_)\n",
    "\n",
    "    def batch_padding(self, inputs, max_sequence_length=None, pad_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs:\n",
    "                list of sentences (integer lists)\n",
    "            max_sequence_length:\n",
    "                integer specifying how large should `max_time` dimension be.\n",
    "                If None, maximum sequence length would be used\n",
    "    \n",
    "        Returns:\n",
    "            inputs_time_major:\n",
    "                input sentences transformed into time-major matrix \n",
    "                (shape [max_time, batch_size]) padded with 0s\n",
    "            sequence_lengths:\n",
    "                batch-sized list of integers specifying amount of active \n",
    "                time steps in each input sequence\n",
    "        \"\"\"\n",
    "    \n",
    "        sequence_lengths = [len(seq) for seq in inputs]\n",
    "        batch_size = len(inputs)\n",
    "    \n",
    "        if max_sequence_length is None:\n",
    "            max_sequence_length = max(sequence_lengths)\n",
    "        sequence_lengths = [min(length, max_sequence_length) for length in sequence_lengths]\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "        inputs_batch_major = pad_idx * np.ones([batch_size, max_sequence_length], dtype=np.int32)\n",
    "    \n",
    "        for i, seq in enumerate(inputs):\n",
    "            seq = seq[:max_sequence_length]\n",
    "            for j, element in enumerate(seq):\n",
    "                inputs_batch_major[i, j] = element\n",
    "\n",
    "        return inputs_batch_major, sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c42ca2e45e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from hedgeable_ai.functions.preprocessing.word2index import Word2IndexProcessor\\nN = 100\\ntexts = questions[:100] + answers[:100]\\nprocessor = Word2IndexProcessor(texts, is_processed=False)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomoaki/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/tomoaki/anaconda3/lib/python3.6/site-packages/hedgeable_ai-0.1.2-py3.6.egg/hedgeable_ai/functions/preprocessing/word2index.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhedgeable_ai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_processor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasicProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhedgeable_ai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tomoaki/anaconda3/lib/python3.6/site-packages/hedgeable_ai-0.1.2-py3.6.egg/hedgeable_ai/functions/preprocessing/clean_text.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0medit_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'enchant'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from hedgeable_ai.functions.preprocessing.word2index import Word2IndexProcessor\n",
    "N = 100\n",
    "texts = questions[:100] + answers[:100]\n",
    "processor = Word2IndexProcessor(texts, is_processed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "def layer_normalization(inputs, epsilon=1e-8, scope=\"layer_normalization\", reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        hidden_dim = shape[-1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        gamma = tf.get_variable(\"gamma\", hidden_dim, initializer=tf.ones_initializer(tf.float32))\n",
    "        beta = tf.get_variable(\"beta\", hidden_dim, initializer=tf.zeros_initializer(tf.float32))\n",
    "        normalized_inputs = (inputs - mean) / tf.sqrt(variance + epsilon)\n",
    "        outputs = gamma * normalized_inputs + beta\n",
    "    return outputs\n",
    "\n",
    "def embedding(inputs,vocab_size, num_units, \n",
    "              zero_pad=True, scale=True,\n",
    "              scope=\"embedding\", reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "        \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]    \n",
    "    ```    \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        \n",
    "        if scale:\n",
    "            outputs = outputs * tf.sqrt(num_units) \n",
    "            \n",
    "    return outputs\n",
    "    \n",
    "def multihead_attention(queries, \n",
    "                        keys,\n",
    "                        values,\n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        drop_rate=0,\n",
    "                        training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      values: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      drop_rate: A floating point number.\n",
    "      training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list()[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
    "        V = tf.layers.dense(values, num_units, activation=None) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        # The size will be [N * num_heads, T_k, C_k/num_heads]\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        alignments = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        d = tf.constant(K_.get_shape().as_list()[-1], tf.float32)\n",
    "        alignments = tf.nn.softmax(alignments / tf.sqrt(d))\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            shape = tf.shape(alignments)\n",
    "            masks = tf.ones(shape[1:]) # (T_q, T_k)\n",
    "            masks = tf.contrib.linalg.LinearOperatorTriL(masks).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(masks, 0), [shape[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "            paddings = tf.ones_like(masks)*(-2**32+1) # minimum value for float\n",
    "            alignments = tf.where(tf.equal(masks, 0), paddings, alignments) # (h*N, T_q, T_k)\n",
    "          \n",
    "        # Dropouts\n",
    "        alignments = tf.layers.dropout(alignments, rate=drop_rate, training=training)\n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(alignments, V_) # ( h*N, T_q, C/h)\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2) # (N, T_q, C)\n",
    "        # Residual connection\n",
    "        outputs += queries \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs) # (N, T_q, C)\n",
    "    return outputs\n",
    "\n",
    "def feedforward(inputs, \n",
    "                num_units=[2048, 512],\n",
    "                scope=\"multihead_attention\", \n",
    "                reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```    \n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, conv2d\n",
    "from tensorflow.contrib.layers import conv2d_transpose, flatten\n",
    "\n",
    "from hedgeable_ai.models.nn import BaseModel\n",
    "        \n",
    "class FeedForward(BaseModel):\n",
    "    def __init__(self, model_params, scope_name, *args, **kwargs):\n",
    "        super().__init__(model_params, scope_name, *args, **kwargs)\n",
    "    \n",
    "    def __call__(self, x, training=True):\n",
    "        with tf.variable_scope(self.scope_name, reuse=self.reuse):\n",
    "            for i, params in enumerate(self.model_params):\n",
    "                with tf.variable_scope('layer_' + str(i)):\n",
    "                    if \"is_flatten\" in params and params[\"is_flatten\"]:\n",
    "                        x = flatten(x)\n",
    "                    if \"drop_rate\" in params:\n",
    "                        x = tf.layers.dropout(x, rate=params[\"drop_rate\"], training=training)\n",
    "                    # demtermine which layer to use\n",
    "                    if params[\"name\"] == \"dense\":\n",
    "                        x = fully_connected(x, params[\"num_hidden\"], activation_fn=None,\n",
    "                                            reuse=self.reuse, scope=\"dense\")\n",
    "                    elif params[\"name\"] == \"conv2d\":\n",
    "                        x =  conv2d(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                    params[\"stride\"], params[\"padding\"], \n",
    "                                    scope=\"conv2d\", reuse=self.reuse, activation_fn=None)\n",
    "                    elif params[\"name\"] == \"deconv2d\":\n",
    "                        x =  conv2d_transpose(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                              params[\"stride\"], params[\"padding\"], \n",
    "                                              scope=\"deconv2d\", reuse=self.reuse, activation_fn=None)\n",
    "                    elif params[\"name\"] == \"reshape\":\n",
    "                        x = tf.reshape(x, (-1,) + params[\"reshape_size\"])\n",
    "                    elif params[\"name\"] == \"pooling\":\n",
    "                        del params[\"name\"]\n",
    "                        x = tf.nn.pool(x, **params)\n",
    "                    elif params[\"name\"] == None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"No implementation for 'name'={}\".format(params[\"name\"]))         \n",
    "                    if \"is_batch\" in params and params[\"is_batch\"]:\n",
    "                        x = tf.layers.batch_normalization(x, training=training, momentum=0.9,\n",
    "                                                          reuse=self.reuse, name=\"batch_norm\")\n",
    "                    if \"activation\" in params:\n",
    "                        x = params[\"activation\"](x)\n",
    "            if self.reuse is False:\n",
    "                self.global_scope_name = tf.get_variable_scope().name\n",
    "                self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.global_scope_name)\n",
    "        self.reuse = True\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from hedgeable_ai.models.nn import BaseModel, get_shape, get_length\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from hedgeable_ai.models.nn.params import nn_is_logit\n",
    "from hedgeable_ai.models.nn import BaseNN, get_shape\n",
    "\n",
    "from hedgeable_ai.models.nn.rnn import get_cell\n",
    "\n",
    "count = 0\n",
    "class DialogueAgent(BaseNN):\n",
    "    def __init__(self, processor, maxlen=20,\n",
    "                 conf=None, additional_length=3,\n",
    "                 num_hidden=512, num_heads=8, num_blocks=6, drop_rate=0.1,\n",
    "                 position_scale=1000, *args, **kwargs):\n",
    "        self.num_blocks = num_blocks\n",
    "        # leave index 0 for padding and 1 for  <eos>\n",
    "        self.vocab_size = processor.vocab_size + 2\n",
    "        self.maxlen = maxlen\n",
    "        self.additional_length = additional_length\n",
    "        self.dec_maxlen = maxlen + additional_length\n",
    "        self.position_scale = position_scale\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hidden = num_hidden\n",
    "        self.reuse = False\n",
    "        super().__init__(processor=processor, conf=conf, *args, **kwargs)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build tensorflow graph\n",
    "        \n",
    "        Note:\n",
    "            You build graphs for output and input, which will be used \n",
    "            for training and prediction.\n",
    "        \"\"\"\n",
    "        # Build Basic Netwoiork\n",
    "        self.enc_input = tf.placeholder(tf.int32, shape=(None, None), name=\"encoder_input\")\n",
    "        self.dec_input = tf.placeholder(tf.int32, shape=(None, None), name=\"decoder_input\")\n",
    "        batch_size = tf.shape(self.dec_input)[0]\n",
    "        eos_padding = tf.ones((batch_size, 1), dtype=tf.int32)\n",
    "        dec_target = self.dec_input\n",
    "        _dec_input = tf.concat((eos_padding, self.dec_input[:, :-1]), axis=1)\n",
    "        logits = self._get_output(self.enc_input, _dec_input, self.training)\n",
    "        # predictions = tf.cast(tf.arg_max(self.logits, dimension=-1), tf.int32)\n",
    "        target_smoothed = label_smoothing(tf.one_hot(dec_target, depth=self.vocab_size), epsilon=0.1)\n",
    "        _loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target_smoothed)\n",
    "        masks = tf.cast(tf.sign(dec_target), tf.float32)\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(masks * _loss, [1]))\n",
    "        \n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        # with tf.control_dependencies(update_ops):\n",
    "        self.optimizer = self._get_optimizer(self.optimizer_name, self.learning_rate_op, self.optimizer_conf)\n",
    "        grads_vars = self.optimizer.compute_gradients(self.loss)\n",
    "        if \"grad_clip\" in self.conf and self.conf[\"grad_clip\"] is not None:\n",
    "            grads_vars = [\n",
    "                (tf.clip_by_norm(gv[0], clip_norm=self.conf[\"grad_clip\"]), gv[1]) \n",
    "                    for gv in grads_vars]\n",
    "        self.train_step = self.optimizer.apply_gradients(grads_vars)\n",
    "        # prediction flow\n",
    "        # logits = self._get_output(self.enc_input, _dec_input, self.training)\n",
    "        self.predictions = tf.cast(tf.arg_max(logits, dimension=-1), tf.int32)\n",
    "        self.logits = logits\n",
    "        \n",
    "    def _get_output(self, enc_input, dec_input, training=True):\n",
    "        tensor_batch_size = tf.shape(enc_input)[0]\n",
    "        with tf.variable_scope(\"embedding\", reuse=self.reuse):\n",
    "            embeddings = tf.get_variable(\"embedding\", [self.vocab_size, self.num_hidden],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings = tf.sqrt(float(self.num_hidden)) * embeddings\n",
    "        # Encoder\n",
    "        with tf.variable_scope(\"encoder\", reuse=self.reuse):\n",
    "            enc_input_embedded = tf.nn.embedding_lookup(embeddings, self.enc_input)\n",
    "            ## Positional Encoding\n",
    "            enc_length = tf.shape(enc_input)[1]\n",
    "            position_idx = tf.range(tf.reduce_max(enc_length))\n",
    "            position_idx = tf.tile(tf.expand_dims(position_idx, 0), [tensor_batch_size, 1])\n",
    "            enc_position_embeddings = tf.get_variable(\"embedding_position\",\n",
    "                                                      [self.maxlen, self.num_hidden],\n",
    "                                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            enc_position_embedded = tf.nn.embedding_lookup(enc_position_embeddings, position_idx)\n",
    "            enc_x = enc_input_embedded + enc_position_embedded\n",
    "            ## Dropout\n",
    "            enc_x = tf.layers.dropout(enc_x, rate=self.drop_rate, training=training)\n",
    "            # Encoder blocks    \n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    enc_x = multihead_attention(queries=enc_x, \n",
    "                                                keys=enc_x, \n",
    "                                                values=enc_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=False,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"self_attention\")\n",
    "                    ### Feed Forward\n",
    "                    enc_x = feedforward(enc_x, num_units=[4*self.num_hidden, self.num_hidden])\n",
    "            \n",
    "        # Decoder\n",
    "        with tf.variable_scope(\"decoder\", reuse=self.reuse):\n",
    "            dec_input_embedded = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "            ## Positional Encoding\n",
    "            dec_length = tf.shape(dec_input)[1]\n",
    "            position_idx = tf.range(dec_length)\n",
    "            position_idx = tf.tile(tf.expand_dims(position_idx, 0), [tensor_batch_size, 1])\n",
    "            dec_position_embeddings = tf.get_variable(\"embedding_position\",\n",
    "                                                      [self.dec_maxlen, self.num_hidden],\n",
    "                                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            dec_position_embedded = tf.nn.embedding_lookup(dec_position_embeddings, position_idx)\n",
    "            dec_x = dec_input_embedded + dec_position_embedded\n",
    "            ## Dropout\n",
    "            dec_x = tf.layers.dropout(dec_x, rate=self.drop_rate, training=training)\n",
    "                \n",
    "            ## Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    dec_x = multihead_attention(queries=dec_x, \n",
    "                                                keys=dec_x, \n",
    "                                                values=dec_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=True,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"self_attention\")\n",
    "                        \n",
    "                    dec_x = multihead_attention(queries=dec_x, \n",
    "                                                keys=enc_x, \n",
    "                                                values=enc_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=False,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"vanilla_attention\")\n",
    "                    ### Feed Forward\n",
    "                    dec_x = feedforward(dec_x, num_units=[4*self.num_hidden, self.num_hidden],\n",
    "                                       scope=\"fully_connected\", reuse=self.reuse)\n",
    "                \n",
    "            # Final linear projection\n",
    "            dim_size = dec_x.get_shape().as_list()[2]\n",
    "            shape = tf.shape(dec_x)\n",
    "            dec_x = tf.reshape(dec_x, [-1, shape[2]])\n",
    "            logits = tf.matmul(dec_x, tf.transpose(embeddings))\n",
    "            logits = tf.reshape(logits, [shape[0], shape[1], self.vocab_size])\n",
    "        self.reuse = True\n",
    "        return logits\n",
    "        \n",
    "    def _optimize(self, batch_X, batch_y, *args, **kwargs):\n",
    "        global count\n",
    "        batch_X = batch_X[0]\n",
    "        batch_X, Xlen = self.processor.batch_padding(batch_X, self.maxlen)\n",
    "        length = np.max(Xlen) + self.additional_length\n",
    "        batch_y = self._batch_padding(batch_y, length)\n",
    "        feed_dict = {self.enc_input: batch_X,\n",
    "                     self.dec_input: batch_y,\n",
    "                     self.training: True}\n",
    "        \n",
    "        _, loss = self.sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "        # print(\"loss\", loss)\n",
    "        if count % 100 == 0:\n",
    "            # logits = self.sess.run(self.logits, feed_dict=feed_dict)\n",
    "            # print(\"logits\", logits)\n",
    "            sentences = texts[:5]\n",
    "            predictions = self.generate_sentences(sentences)\n",
    "            print(\"******************************************\")\n",
    "            for prediction in predictions:\n",
    "                print(prediction)\n",
    "        count += 1\n",
    "        return loss\n",
    "    \n",
    "    def generate_sentences(self, sentences):\n",
    "        X = [self.processor.encode(sentence) for sentence in sentences]\n",
    "        # X = sentences\n",
    "        # X = [x_[::-1] for x_ in X]\n",
    "        X, Xlen = self.processor.batch_padding(X, self.maxlen)\n",
    "        batch_size = X.shape[0]\n",
    "        y = np.array([[] for _ in range(batch_size)], dtype=int)\n",
    "        not_finished = np.array([True for _ in range(batch_size)])\n",
    "        for i in range(self.dec_maxlen):\n",
    "            feed_dict = {self.enc_input: X,\n",
    "                         self.dec_input: y,\n",
    "                         self.training: False}\n",
    "            predictions = self.sess.run(self.predictions, feed_dict=feed_dict)\n",
    "            # print(predictions)\n",
    "            new_y = predictions[:, -1]\n",
    "            new_y = np.array([new_y[k] if not_finished[k] else 0 for k in range(batch_size)])\n",
    "            y = np.concatenate((y, new_y[:, np.newaxis]), axis=1)\n",
    "            # print(y)\n",
    "            new_not_finished = np.array(new_y > 1, dtype=int)\n",
    "            not_finished = np.array(\n",
    "                np.array(not_finished, dtype=int) * np.array(new_not_finished, dtype=int),\n",
    "                dtype=bool)\n",
    "            if np.sum(not_finished) == 0:\n",
    "                break\n",
    "        return [self.processor.decode(i) for i in y]\n",
    "    \n",
    "    def _batch_padding(self, batch, length):\n",
    "        EOS = 1\n",
    "        PAD = 0\n",
    "        padded_batch = []\n",
    "        for x in batch:\n",
    "            x = list(x)\n",
    "            if len(x) > length:\n",
    "                x = x[:length]\n",
    "            elif len(x) < length:\n",
    "                x.append(EOS)\n",
    "            while len(x) < length:\n",
    "                x.append(PAD)\n",
    "            padded_batch.append(x)\n",
    "        return np.array(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['out', 'out', 'you', 'you', 'you']\n",
      "['out', 'out', 'you', 'you', 'you', 'you']\n",
      "['out', 'out', 'you', 'you', 'you', 'you']\n",
      "['out', 'out', 'you', 'you', 'you', 'you']\n",
      "['out', 'out', 'i', 'you', 'you']\n",
      "******************************************\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', '.']\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', '.', 'the', 'the', 'to', 'a', 'to', 'a', 'to', 'a', 'to', 'a', 'to', 'a']\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', '.', 'the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'a', 'a', 'a']\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', '.', 'to', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'a', 'a', 'a']\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'a']\n",
      "******************************************\n",
      "['men', 'men', '.', 'have', 'have', 'have', 'of', 'of', 'to', 'to', 'you', 'of', 'of', '.', '.']\n",
      "['men', 'men', '.', 'have', 'have', 'have', 'of', 'of', 'to', 'to', 'you', 'of', 'of', '.', '.']\n",
      "['now', 'now', '.', 'have', '.', 'have', 'you', 'of', 'of', 'to', 'to', 'be', 'be', 'now', 'of', '.', '.']\n",
      "['now', 'now', '.', 'have', '.', 'have', 'you', 'of', 'of', 'to', 'to', 'be', 'be', 'now', 'of', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', '.', '.', 'you', 'you', 'of', 'to', 'to', 'be', 'be', 'of', 'now', '.', '.']\n",
      "******************************************\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.', 'i', 'i', 'you', 'you', 'the', 'the', 'it', 'this', '.']\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.', 'i', 'i', 'you', 'you', 'the', 'the', 'it', 'this', '.']\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.', 'i', 'i', 'you', 'you', 'the', 'the', 'it', 'this', 'and']\n",
      "******************************************\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', 'is', 'and', 'i', 'i', 'not']\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', 'is', 'and', 'i', 'i', 'not']\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', 'is', 'and', 'i', 'i', 'not']\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', 'is', 'and', 'i', 'i', 'not']\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', '.', 'and', 'and', 'i', 'make']\n",
      "******************************************\n",
      "['a', 'a', 'with', 'a', 'with', 'a', 'a', 'we', 'we', 'a', 'a', 'that', '.', 'we']\n",
      "['a', 'a', 'with', 'a', 'with', 'with', 'a', 'a', 'we', '.', 'will']\n",
      "['a', 'a', 'with', 'a', 'with', 'with', 'a', 'a', 'we', '.', 'will']\n",
      "['a', 'a', 'with', 'a', 'with', 'with', 'a', 'a', 'we', '.', 'will']\n",
      "['a', 'a', 'with', 'a', 'with', 'with', 'a', 'a', 'we', '.', 'will']\n",
      "******************************************\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "******************************************\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', 'the', '.', '.', 'the', 'the']\n",
      "['the', 'the', '.', '.', 'the', 'the', 'the', '.', '.', 'the', 'the']\n",
      "['the', 'the', '.', '.', 'the', 'the', 'the', '.', '.', 'the', 'the']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/100 [04:09<6:51:22, 249.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "******************************************\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|         | 2/100 [08:18<6:47:15, 249.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|         | 3/100 [12:26<6:42:34, 249.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['to', 'to']\n",
      "['to', 'to']\n",
      "['to', 'to']\n",
      "['to', 'to']\n",
      "['to', 'to']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|         | 4/100 [16:29<6:35:13, 247.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "conf = {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"learning_rate_minimum\": 1e-4,\n",
    "        \"learning_rate_decay\": 0.5,\n",
    "        \"learning_rate_decay_step\": 1,\n",
    "        \"batch_size\": 64,\n",
    "        \"model_dir\": \"./attention_logs\",\n",
    "        \"load_file_path\": None,\n",
    "        \"save_file_path\": None,\n",
    "        \"log_freq\": 1,\n",
    "        \"grad_clip\":None,\n",
    "        \"optimizer\":\"adam\",\n",
    "}\n",
    "\n",
    "train_X = processor.data[:-1]\n",
    "train_y = processor.data[1:]\n",
    "# with tf.device('/gpu:0'):\n",
    "tf.reset_default_graph()\n",
    "agent = DialogueAgent(processor, maxlen=20, conf=conf, additional_length=3)\n",
    "agent.fit(train_X, train_y, num_epochs=100, batch_bar=False, log_freq=1, batch_log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "[]\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "predictions = agent.generate_sentences(texts[:10])\n",
    "for prediction in predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "Model saved in file: params/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "agent.fit(train_X, train_y, num_epochs=100, batch_bar=False, log_freq=1, batch_log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([True, True, True, False], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.Variable(np.ones((10, 10)))\n",
    "with tf.variable_scope(\"hello\", reuse=False):\n",
    "    x = tf.contrib.layers.fully_connected(x, 10)\n",
    "    # x = tf.contrib.layers.fully_connected(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"hello\", reuse=True):\n",
    "    x = tf.contrib.layers.fully_connected(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
