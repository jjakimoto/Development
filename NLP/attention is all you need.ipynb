{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 368 ms, total: 664 ms\n",
      "Wall time: 298 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "# texts = clean_movie_data(data)\n",
    "df = pd.read_csv(\"data/All-seasons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Chef</td>\n",
       "      <td>I'm sorry boys.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
       "1     10       1      Kyle                        Going away? For how long?\\n\n",
       "2     10       1      Stan                                         Forever.\\n\n",
       "3     10       1      Chef                                  I'm sorry boys.\\n\n",
       "4     10       1      Stan  Chef said he's been bored, so he joining a gro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = df[\"Line\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = open(\"data/cornell movie-dialogs corpus/movie_lines.txt\", \"r\",encoding='utf-8', errors='ignore')\n",
    "data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_movie_data(data):\n",
    "    pattern = \" \\+\\+\\+\\$\\+\\+\\+ u[0-9] \\+\\+\\+\\$\\+\\+\\+ m[0-9] \\+\\+\\+\\$\\+\\+\\+ \\w* \\+\\+\\+\\$\\+\\+\\+ \"\n",
    "    lines = re.split(pattern, data.lower())\n",
    "    line_words = []\n",
    "    for line in tqdm(lines):\n",
    "        try:\n",
    "            cleaned_line = clean_text(line, max_dist=0)\n",
    "            if len(cleaned_line) >= 1:\n",
    "                line_words.append(cleaned_line)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "    return line_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['You guys, you guys! Chef is going away. \\n',\n",
       "       'Going away? For how long?\\n', 'Forever.\\n', \"I'm sorry boys.\\n\",\n",
       "       \"Chef said he's been bored, so he joining a group called the Super Adventure Club. \\n\",\n",
       "       'Wow!\\n',\n",
       "       'Chef?? What kind of questions do you think adventuring around the world is gonna answer?!\\n',\n",
       "       \"What's the meaning of life? Why are we here?\\n\",\n",
       "       \"I hope you're making the right choice.\\n\",\n",
       "       \"I'm gonna miss him.  I'm gonna miss Chef and I...and I don't know how to tell him! \\n\"], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41min 47s, sys: 25.3 s, total: 42min 13s\n",
      "Wall time: 41min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from hedgeable_ai.functions.preprocessing.word2index import Word2IndexProcessor\n",
    "\n",
    "processor = Word2IndexProcessor(texts[:], is_processed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "def layer_normalization(inputs, epsilon=1e-8, scope=\"layer_normalization\", reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        hidden_dim = shape[-1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        gamma = tf.get_variable(\"gamma\", hidden_dim, initializer=tf.ones_initializer(tf.float32))\n",
    "        beta = tf.get_variable(\"beta\", hidden_dim, initializer=tf.zeros_initializer(tf.float32))\n",
    "        normalized_inputs = (inputs - mean) / tf.sqrt(variance + epsilon)\n",
    "        outputs = gamma * normalized_inputs + beta\n",
    "    return outputs\n",
    "\n",
    "def embedding(inputs,vocab_size, num_units, \n",
    "              zero_pad=True, scale=True,\n",
    "              scope=\"embedding\", reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "        \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]    \n",
    "    ```    \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        \n",
    "        if scale:\n",
    "            outputs = outputs * tf.sqrt(num_units) \n",
    "            \n",
    "    return outputs\n",
    "    \n",
    "def multihead_attention(queries, \n",
    "                        keys,\n",
    "                        values,\n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        drop_rate=0,\n",
    "                        training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      values: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      drop_rate: A floating point number.\n",
    "      training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list()[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
    "        V = tf.layers.dense(values, num_units, activation=None) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        # The size will be [N * num_heads, T_k, C_k/num_heads]\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        alignments = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        d = tf.constant(K_.get_shape().as_list()[-1], tf.float32)\n",
    "        alignments = tf.nn.softmax(alignments / tf.sqrt(d))\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            shape = tf.shape(alignments)\n",
    "            masks = tf.ones(shape[1:]) # (T_q, T_k)\n",
    "            masks = tf.contrib.linalg.LinearOperatorTriL(masks).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(masks, 0), [shape[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "            paddings = tf.ones_like(masks)*(-2**32+1) # minimum value for float\n",
    "            alignments = tf.where(tf.equal(masks, 0), paddings, alignments) # (h*N, T_q, T_k)\n",
    "          \n",
    "        # Dropouts\n",
    "        alignments = tf.layers.dropout(alignments, rate=drop_rate, training=training)\n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(alignments, V_) # ( h*N, T_q, C/h)\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2) # (N, T_q, C)\n",
    "        # Residual connection\n",
    "        outputs += queries \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs) # (N, T_q, C)\n",
    "    return outputs\n",
    "\n",
    "def feedforward(inputs, \n",
    "                num_units=[2048, 512],\n",
    "                scope=\"multihead_attention\", \n",
    "                reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```    \n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, conv2d\n",
    "from tensorflow.contrib.layers import conv2d_transpose, flatten\n",
    "\n",
    "from hedgeable_ai.models.nn import BaseModel\n",
    "        \n",
    "class FeedForward(BaseModel):\n",
    "    def __init__(self, model_params, scope_name, *args, **kwargs):\n",
    "        super().__init__(model_params, scope_name, *args, **kwargs)\n",
    "    \n",
    "    def __call__(self, x, training=True):\n",
    "        with tf.variable_scope(self.scope_name, reuse=self.reuse):\n",
    "            for i, params in enumerate(self.model_params):\n",
    "                with tf.variable_scope('layer_' + str(i)):\n",
    "                    if \"is_flatten\" in params and params[\"is_flatten\"]:\n",
    "                        x = flatten(x)\n",
    "                    if \"drop_rate\" in params:\n",
    "                        x = tf.layers.dropout(x, rate=params[\"drop_rate\"], training=training)\n",
    "                    # demtermine which layer to use\n",
    "                    if params[\"name\"] == \"dense\":\n",
    "                        x = fully_connected(x, params[\"num_hidden\"], activation_fn=None,\n",
    "                                            reuse=self.reuse, scope=\"dense\")\n",
    "                    elif params[\"name\"] == \"conv2d\":\n",
    "                        x =  conv2d(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                    params[\"stride\"], params[\"padding\"], \n",
    "                                    scope=\"conv2d\", reuse=self.reuse, activation_fn=None)\n",
    "                    elif params[\"name\"] == \"deconv2d\":\n",
    "                        x =  conv2d_transpose(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                              params[\"stride\"], params[\"padding\"], \n",
    "                                              scope=\"deconv2d\", reuse=self.reuse, activation_fn=None)\n",
    "                    elif params[\"name\"] == \"reshape\":\n",
    "                        x = tf.reshape(x, (-1,) + params[\"reshape_size\"])\n",
    "                    elif params[\"name\"] == \"pooling\":\n",
    "                        del params[\"name\"]\n",
    "                        x = tf.nn.pool(x, **params)\n",
    "                    elif params[\"name\"] == None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"No implementation for 'name'={}\".format(params[\"name\"]))         \n",
    "                    if \"is_batch\" in params and params[\"is_batch\"]:\n",
    "                        x = tf.layers.batch_normalization(x, training=training, momentum=0.9,\n",
    "                                                          reuse=self.reuse, name=\"batch_norm\")\n",
    "                    if \"activation\" in params:\n",
    "                        x = params[\"activation\"](x)\n",
    "            if self.reuse is False:\n",
    "                self.global_scope_name = tf.get_variable_scope().name\n",
    "                self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.global_scope_name)\n",
    "        self.reuse = True\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from hedgeable_ai.models.nn import BaseModel, get_shape, get_length\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from hedgeable_ai.models.nn.params import nn_is_logit\n",
    "from hedgeable_ai.models.nn import BaseNN, get_shape\n",
    "\n",
    "from hedgeable_ai.models.nn.rnn import get_cell\n",
    "\n",
    "count = 0\n",
    "class DialogueAgent(BaseNN):\n",
    "    def __init__(self, processor, maxlen=20,\n",
    "                 conf=None, additional_length=3,\n",
    "                 num_hidden=512, num_heads=8, num_blocks=6, drop_rate=0.1,\n",
    "                 position_scale=1000, *args, **kwargs):\n",
    "        self.num_blocks = num_blocks\n",
    "        # leave index 0 for padding and 1 for  <eos>\n",
    "        self.vocab_size = processor.vocab_size + 2\n",
    "        self.maxlen = maxlen\n",
    "        self.additional_length = additional_length\n",
    "        self.dec_maxlen = maxlen + additional_length\n",
    "        self.position_scale = position_scale\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hidden = num_hidden\n",
    "        self.reuse = False\n",
    "        super().__init__(processor=processor, conf=conf, *args, **kwargs)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build tensorflow graph\n",
    "        \n",
    "        Note:\n",
    "            You build graphs for output and input, which will be used \n",
    "            for training and prediction.\n",
    "        \"\"\"\n",
    "        # Build Basic Netwoiork\n",
    "        self.enc_input = tf.placeholder(tf.int32, shape=(None, None), name=\"encoder_input\")\n",
    "        self.dec_input = tf.placeholder(tf.int32, shape=(None, None), name=\"decoder_input\")\n",
    "        batch_size = tf.shape(self.dec_input)[0]\n",
    "        eos_padding = tf.ones((batch_size, 1), dtype=tf.int32)\n",
    "        dec_target = self.dec_input\n",
    "        _dec_input = tf.concat((eos_padding, self.dec_input[:, :-1]), axis=1)\n",
    "        logits = self._get_output(self.enc_input, _dec_input, self.training)\n",
    "        # predictions = tf.cast(tf.arg_max(self.logits, dimension=-1), tf.int32)\n",
    "        target_smoothed = label_smoothing(tf.one_hot(dec_target, depth=self.vocab_size), epsilon=0.1)\n",
    "        _loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target_smoothed)\n",
    "        masks = tf.cast(tf.sign(dec_target), tf.float32)\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(masks * _loss, [1]))\n",
    "        \n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        # with tf.control_dependencies(update_ops):\n",
    "        self.optimizer = self._get_optimizer(self.optimizer_name, self.learning_rate_op, self.optimizer_conf)\n",
    "        grads_vars = self.optimizer.compute_gradients(self.loss)\n",
    "        if \"grad_clip\" in self.conf and self.conf[\"grad_clip\"] is not None:\n",
    "            grads_vars = [\n",
    "                (tf.clip_by_norm(gv[0], clip_norm=self.conf[\"grad_clip\"]), gv[1]) \n",
    "                    for gv in grads_vars]\n",
    "        self.train_step = self.optimizer.apply_gradients(grads_vars)\n",
    "        # prediction flow\n",
    "        # logits = self._get_output(self.enc_input, _dec_input, self.training)\n",
    "        self.predictions = tf.cast(tf.arg_max(logits, dimension=-1), tf.int32)\n",
    "        self.logits = logits\n",
    "        \n",
    "    def _get_output(self, enc_input, dec_input, training=True):\n",
    "        tensor_batch_size = tf.shape(enc_input)[0]\n",
    "        with tf.variable_scope(\"embedding\", reuse=self.reuse):\n",
    "            embeddings = tf.get_variable(\"embedding\", [self.vocab_size, self.num_hidden],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            embeddings = tf.sqrt(float(self.num_hidden)) * embeddings\n",
    "        # Encoder\n",
    "        with tf.variable_scope(\"encoder\", reuse=self.reuse):\n",
    "            enc_input_embedded = tf.nn.embedding_lookup(embeddings, self.enc_input)\n",
    "            ## Positional Encoding\n",
    "            enc_length = tf.shape(enc_input)[1]\n",
    "            position_idx = tf.range(tf.reduce_max(enc_length))\n",
    "            position_idx = tf.tile(tf.expand_dims(position_idx, 0), [tensor_batch_size, 1])\n",
    "            enc_position_embeddings = tf.get_variable(\"embedding_position\",\n",
    "                                                      [self.maxlen, self.num_hidden],\n",
    "                                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            enc_position_embedded = tf.nn.embedding_lookup(enc_position_embeddings, position_idx)\n",
    "            enc_x = enc_input_embedded + enc_position_embedded\n",
    "            ## Dropout\n",
    "            enc_x = tf.layers.dropout(enc_x, rate=self.drop_rate, training=training)\n",
    "            # Encoder blocks    \n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    enc_x = multihead_attention(queries=enc_x, \n",
    "                                                keys=enc_x, \n",
    "                                                values=enc_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=False,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"self_attention\")\n",
    "                    ### Feed Forward\n",
    "                    enc_x = feedforward(enc_x, num_units=[4*self.num_hidden, self.num_hidden])\n",
    "            \n",
    "        # Decoder\n",
    "        with tf.variable_scope(\"decoder\", reuse=self.reuse):\n",
    "            dec_input_embedded = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "            ## Positional Encoding\n",
    "            dec_length = tf.shape(dec_input)[1]\n",
    "            position_idx = tf.range(dec_length)\n",
    "            position_idx = tf.tile(tf.expand_dims(position_idx, 0), [tensor_batch_size, 1])\n",
    "            dec_position_embeddings = tf.get_variable(\"embedding_position\",\n",
    "                                                      [self.dec_maxlen, self.num_hidden],\n",
    "                                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            dec_position_embedded = tf.nn.embedding_lookup(dec_position_embeddings, position_idx)\n",
    "            dec_x = dec_input_embedded + dec_position_embedded\n",
    "            ## Dropout\n",
    "            dec_x = tf.layers.dropout(dec_x, rate=self.drop_rate, training=training)\n",
    "                \n",
    "            ## Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    dec_x = multihead_attention(queries=dec_x, \n",
    "                                                keys=dec_x, \n",
    "                                                values=dec_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=True,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"self_attention\")\n",
    "                        \n",
    "                    dec_x = multihead_attention(queries=dec_x, \n",
    "                                                keys=enc_x, \n",
    "                                                values=enc_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=False,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"vanilla_attention\")\n",
    "                    ### Feed Forward\n",
    "                    dec_x = feedforward(dec_x, num_units=[4*self.num_hidden, self.num_hidden],\n",
    "                                       scope=\"fully_connected\", reuse=self.reuse)\n",
    "                \n",
    "            # Final linear projection\n",
    "            dim_size = dec_x.get_shape().as_list()[2]\n",
    "            shape = tf.shape(dec_x)\n",
    "            dec_x = tf.reshape(dec_x, [-1, shape[2]])\n",
    "            logits = tf.matmul(dec_x, tf.transpose(embeddings))\n",
    "            logits = tf.reshape(logits, [shape[0], shape[1], self.vocab_size])\n",
    "        self.reuse = True\n",
    "        return logits\n",
    "        \n",
    "    def _optimize(self, batch_X, batch_y, *args, **kwargs):\n",
    "        global count\n",
    "        batch_X = batch_X[0]\n",
    "        batch_X, Xlen = self.processor.batch_padding(batch_X, self.maxlen)\n",
    "        length = np.max(Xlen) + self.additional_length\n",
    "        batch_y = self._batch_padding(batch_y, length)\n",
    "        feed_dict = {self.enc_input: batch_X,\n",
    "                     self.dec_input: batch_y,\n",
    "                     self.training: True}\n",
    "        \n",
    "        _, loss = self.sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "        # print(\"loss\", loss)\n",
    "        if count % 100 == 0:\n",
    "            # logits = self.sess.run(self.logits, feed_dict=feed_dict)\n",
    "            # print(\"logits\", logits)\n",
    "            sentences = texts[:5]\n",
    "            predictions = self.generate_sentences(sentences)\n",
    "            print(\"******************************************\")\n",
    "            for prediction in predictions:\n",
    "                print(prediction)\n",
    "        count += 1\n",
    "        return loss\n",
    "    \n",
    "    def generate_sentences(self, sentences):\n",
    "        X = [self.processor.encode(sentence) for sentence in sentences]\n",
    "        # X = sentences\n",
    "        # X = [x_[::-1] for x_ in X]\n",
    "        X, Xlen = self.processor.batch_padding(X, self.maxlen)\n",
    "        batch_size = X.shape[0]\n",
    "        y = np.array([[] for _ in range(batch_size)], dtype=int)\n",
    "        not_finished = np.array([True for _ in range(batch_size)])\n",
    "        for i in range(self.dec_maxlen):\n",
    "            feed_dict = {self.enc_input: X,\n",
    "                         self.dec_input: y,\n",
    "                         self.training: False}\n",
    "            predictions = self.sess.run(self.predictions, feed_dict=feed_dict)\n",
    "            # print(predictions)\n",
    "            new_y = predictions[:, -1]\n",
    "            new_y = np.array([new_y[k] if not_finished[k] else 0 for k in range(batch_size)])\n",
    "            y = np.concatenate((y, new_y[:, np.newaxis]), axis=1)\n",
    "            # print(y)\n",
    "            new_not_finished = np.array(new_y > 1, dtype=int)\n",
    "            not_finished = np.array(\n",
    "                np.array(not_finished, dtype=int) * np.array(new_not_finished, dtype=int),\n",
    "                dtype=bool)\n",
    "            if np.sum(not_finished) == 0:\n",
    "                break\n",
    "        return [self.processor.decode(i) for i in y]\n",
    "    \n",
    "    def _batch_padding(self, batch, length):\n",
    "        EOS = 1\n",
    "        PAD = 0\n",
    "        padded_batch = []\n",
    "        for x in batch:\n",
    "            x = list(x)\n",
    "            if len(x) > length:\n",
    "                x = x[:length]\n",
    "            elif len(x) < length:\n",
    "                x.append(EOS)\n",
    "            while len(x) < length:\n",
    "                x.append(PAD)\n",
    "            padded_batch.append(x)\n",
    "        return np.array(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['out', 'out', 'you', 'you', 'you']\n",
      "['out', 'out', 'you', 'you', 'you', 'you']\n",
      "['out', 'out', 'you', 'you', 'you', 'you']\n",
      "['out', 'out', 'you', 'you', 'you', 'you']\n",
      "['out', 'out', 'i', 'you', 'you']\n",
      "******************************************\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', '.']\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', '.', 'the', 'the', 'to', 'a', 'to', 'a', 'to', 'a', 'to', 'a', 'to', 'a']\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', '.', 'the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'a', 'a', 'a']\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', '.', 'to', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'a', 'a', 'a']\n",
      "['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', '.', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'a']\n",
      "******************************************\n",
      "['men', 'men', '.', 'have', 'have', 'have', 'of', 'of', 'to', 'to', 'you', 'of', 'of', '.', '.']\n",
      "['men', 'men', '.', 'have', 'have', 'have', 'of', 'of', 'to', 'to', 'you', 'of', 'of', '.', '.']\n",
      "['now', 'now', '.', 'have', '.', 'have', 'you', 'of', 'of', 'to', 'to', 'be', 'be', 'now', 'of', '.', '.']\n",
      "['now', 'now', '.', 'have', '.', 'have', 'you', 'of', 'of', 'to', 'to', 'be', 'be', 'now', 'of', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', '.', '.', 'you', 'you', 'of', 'to', 'to', 'be', 'be', 'of', 'now', '.', '.']\n",
      "******************************************\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.', 'i', 'i', 'you', 'you', 'the', 'the', 'it', 'this', '.']\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.', 'i', 'i', 'you', 'you', 'the', 'the', 'it', 'this', '.']\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', 'the', 'the', 'the', 'the', 'the', 'all', 'all', 'the', 'the', '.', '.', 'i', 'i', 'you', 'you', 'the', 'the', 'it', 'this', 'and']\n",
      "******************************************\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', 'is', 'and', 'i', 'i', 'not']\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', 'is', 'and', 'i', 'i', 'not']\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', 'is', 'and', 'i', 'i', 'not']\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', 'is', 'and', 'i', 'i', 'not']\n",
      "['the', 'the', '.', '.', '.', '.', '.', '.', '.', 'twig', 'twig', 'that', '.', '.', 'twig', 'twig', 'that', '.', '.', 'and', 'and', 'i', 'make']\n",
      "******************************************\n",
      "['a', 'a', 'with', 'a', 'with', 'a', 'a', 'we', 'we', 'a', 'a', 'that', '.', 'we']\n",
      "['a', 'a', 'with', 'a', 'with', 'with', 'a', 'a', 'we', '.', 'will']\n",
      "['a', 'a', 'with', 'a', 'with', 'with', 'a', 'a', 'we', '.', 'will']\n",
      "['a', 'a', 'with', 'a', 'with', 'with', 'a', 'a', 'we', '.', 'will']\n",
      "['a', 'a', 'with', 'a', 'with', 'with', 'a', 'a', 'we', '.', 'will']\n",
      "******************************************\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "['been', 'been', '.', '.', '.', '.', 'been', '.', '.']\n",
      "******************************************\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', 'the', '.', '.', 'the', 'the']\n",
      "['the', 'the', '.', '.', 'the', 'the', 'the', '.', '.', 'the', 'the']\n",
      "['the', 'the', '.', '.', 'the', 'the', 'the', '.', '.', 'the', 'the']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n",
      "['and', 'and', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'i', 'i', '.', '.', 'is', 'is', 'i', 'i', '.', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/100 [04:09<6:51:22, 249.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "['a', 'a', '.', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "['the', 'the', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'the', 'the', 'to', 'to', '.', '.']\n",
      "******************************************\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "['the', 'the', '.', '.', 'the', '.', '.', 'the', 'the', '.', '.', 'the', 'the', '.', '.', '.', 'the', 'the', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [08:18<6:47:15, 249.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "['the', 'the']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [12:26<6:42:34, 249.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['to', 'to']\n",
      "['to', 'to']\n",
      "['to', 'to']\n",
      "['to', 'to']\n",
      "['to', 'to']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [16:29<6:35:13, 247.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "******************************************\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n",
      "['.', '.']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "conf = {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"learning_rate_minimum\": 1e-4,\n",
    "        \"learning_rate_decay\": 0.5,\n",
    "        \"learning_rate_decay_step\": 1,\n",
    "        \"batch_size\": 64,\n",
    "        \"model_dir\": \"./attention_logs\",\n",
    "        \"load_file_path\": None,\n",
    "        \"save_file_path\": None,\n",
    "        \"log_freq\": 1,\n",
    "        \"grad_clip\":None,\n",
    "        \"optimizer\":\"adam\",\n",
    "}\n",
    "\n",
    "train_X = processor.data[:-1]\n",
    "train_y = processor.data[1:]\n",
    "# with tf.device('/gpu:0'):\n",
    "tf.reset_default_graph()\n",
    "agent = DialogueAgent(processor, maxlen=20, conf=conf, additional_length=3)\n",
    "agent.fit(train_X, train_y, num_epochs=100, batch_bar=False, log_freq=1, batch_log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "[]\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "predictions = agent.generate_sentences(texts[:10])\n",
    "for prediction in predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "******************************************\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "Model saved in file: params/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "agent.fit(train_X, train_y, num_epochs=100, batch_bar=False, log_freq=1, batch_log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([True, True, True, False], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.Variable(np.ones((10, 10)))\n",
    "with tf.variable_scope(\"hello\", reuse=False):\n",
    "    x = tf.contrib.layers.fully_connected(x, 10)\n",
    "    # x = tf.contrib.layers.fully_connected(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"hello\", reuse=True):\n",
    "    x = tf.contrib.layers.fully_connected(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
