{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 380 ms, sys: 52 ms, total: 432 ms\n",
      "Wall time: 437 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "# texts = clean_movie_data(data)\n",
    "df = pd.read_csv(\"data/All-seasons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Chef</td>\n",
       "      <td>I'm sorry boys.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
       "1     10       1      Kyle                        Going away? For how long?\\n\n",
       "2     10       1      Stan                                         Forever.\\n\n",
       "3     10       1      Chef                                  I'm sorry boys.\\n\n",
       "4     10       1      Stan  Chef said he's been bored, so he joining a gro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = df[\"Line\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['You guys, you guys! Chef is going away. \\n',\n",
       "       'Going away? For how long?\\n', 'Forever.\\n', \"I'm sorry boys.\\n\",\n",
       "       \"Chef said he's been bored, so he joining a group called the Super Adventure Club. \\n\",\n",
       "       'Wow!\\n',\n",
       "       'Chef?? What kind of questions do you think adventuring around the world is gonna answer?!\\n',\n",
       "       \"What's the meaning of life? Why are we here?\\n\",\n",
       "       \"I hope you're making the right choice.\\n\",\n",
       "       \"I'm gonna miss him.  I'm gonna miss Chef and I...and I don't know how to tell him! \\n\"], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.3 s, sys: 124 ms, total: 38.4 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from hedgeable_ai.functions.preprocessing.word2index import Word2IndexProcessor\n",
    "\n",
    "processor = Word2IndexProcessor(texts[:1000], is_processed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "def layer_normalization(inputs, epsilon=1e-8, scope=\"layer_normalization\", reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        hidden_dim = shape[-1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        gamma = tf.get_variable(\"gamma\", hidden_dim, initializer=tf.ones_initializer(tf.float32))\n",
    "        beta = tf.get_variable(\"beta\", hidden_dim, initializer=tf.zeros_initializer(tf.float32))\n",
    "        normalized_inputs = (inputs - mean) / tf.sqrt(variance + epsilon)\n",
    "        outputs = gamma * normalized_inputs + beta\n",
    "    return outputs\n",
    "\n",
    "def embedding(inputs,vocab_size, num_units, \n",
    "              zero_pad=True, scale=True,\n",
    "              scope=\"embedding\", reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "        \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]    \n",
    "    ```    \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        \n",
    "        if scale:\n",
    "            outputs = outputs * tf.sqrt(num_units) \n",
    "            \n",
    "    return outputs\n",
    "    \n",
    "def multihead_attention(queries, \n",
    "                        keys,\n",
    "                        values,\n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        drop_rate=0,\n",
    "                        training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      values: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      drop_rate: A floating point number.\n",
    "      training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list()[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
    "        V = tf.layers.dense(values, num_units, activation=None) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        # The size will be [N * num_heads, T_k, C_k/num_heads]\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        alignments = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        d = tf.constant(K_.get_shape().as_list()[-1], tf.float32)\n",
    "        alignments = tf.nn.softmax(alignments / tf.sqrt(d))\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            shape = tf.shape(alignments)\n",
    "            masks = tf.ones(shape[1:]) # (T_q, T_k)\n",
    "            masks = tf.contrib.linalg.LinearOperatorTriL(masks).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(masks, 0), [shape[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "            alignments = alignments * masks\n",
    "            sum_alignments = tf.reduce_sum(alignments, -1, keep_dims=True)\n",
    "            sum_alignments = tf.tile(sum_alignments, [1, 1, shape[-1]])\n",
    "            alingments = alignments / sum_alignments\n",
    "          \n",
    "        # Dropouts\n",
    "        alignments = tf.layers.dropout(alignments, rate=drop_rate, training=training)\n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(alignments, V_) # ( h*N, T_q, C/h)\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2) # (N, T_q, C)\n",
    "        # Residual connection\n",
    "        outputs += queries \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs) # (N, T_q, C)\n",
    "    return outputs\n",
    "\n",
    "def feedforward(inputs, \n",
    "                num_units=[2048, 512],\n",
    "                scope=\"multihead_attention\", \n",
    "                reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```    \n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, conv2d\n",
    "from tensorflow.contrib.layers import conv2d_transpose, flatten\n",
    "\n",
    "from hedgeable_ai.models.nn import BaseModel\n",
    "        \n",
    "class FeedForward(BaseModel):\n",
    "    def __init__(self, model_params, scope_name, *args, **kwargs):\n",
    "        super().__init__(model_params, scope_name, *args, **kwargs)\n",
    "    \n",
    "    def __call__(self, x, training=True):\n",
    "        with tf.variable_scope(self.scope_name, reuse=self.reuse):\n",
    "            for i, params in enumerate(self.model_params):\n",
    "                with tf.variable_scope('layer_' + str(i)):\n",
    "                    if \"is_flatten\" in params and params[\"is_flatten\"]:\n",
    "                        x = flatten(x)\n",
    "                    if \"drop_rate\" in params:\n",
    "                        x = tf.layers.dropout(x, rate=params[\"drop_rate\"], training=training)\n",
    "                    # demtermine which layer to use\n",
    "                    if params[\"name\"] == \"dense\":\n",
    "                        x = fully_connected(x, params[\"num_hidden\"], activation_fn=None,\n",
    "                                            reuse=self.reuse, scope=\"dense\")\n",
    "                    elif params[\"name\"] == \"conv2d\":\n",
    "                        x =  conv2d(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                    params[\"stride\"], params[\"padding\"], \n",
    "                                    scope=\"conv2d\", reuse=self.reuse, activation_fn=None)\n",
    "                    elif params[\"name\"] == \"deconv2d\":\n",
    "                        x =  conv2d_transpose(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                              params[\"stride\"], params[\"padding\"], \n",
    "                                              scope=\"deconv2d\", reuse=self.reuse, activation_fn=None)\n",
    "                    elif params[\"name\"] == \"reshape\":\n",
    "                        x = tf.reshape(x, (-1,) + params[\"reshape_size\"])\n",
    "                    elif params[\"name\"] == \"pooling\":\n",
    "                        del params[\"name\"]\n",
    "                        x = tf.nn.pool(x, **params)\n",
    "                    elif params[\"name\"] == None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"No implementation for 'name'={}\".format(params[\"name\"]))         \n",
    "                    if \"is_batch\" in params and params[\"is_batch\"]:\n",
    "                        x = tf.layers.batch_normalization(x, training=training, momentum=0.9,\n",
    "                                                          reuse=self.reuse, name=\"batch_norm\")\n",
    "                    if \"activation\" in params:\n",
    "                        x = params[\"activation\"](x)\n",
    "            if self.reuse is False:\n",
    "                self.global_scope_name = tf.get_variable_scope().name\n",
    "                self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.global_scope_name)\n",
    "        self.reuse = True\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from hedgeable_ai.models.nn import BaseModel, get_shape, get_length\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from hedgeable_ai.models.nn.params import nn_is_logit\n",
    "from hedgeable_ai.models.nn import BaseNN, get_shape\n",
    "\n",
    "from hedgeable_ai.models.nn.rnn import get_cell\n",
    "\n",
    "count = 0\n",
    "class DialogueAgent(BaseNN):\n",
    "    def __init__(self, processor, emb_size=300, maxlen=20,\n",
    "                 conf=None, additional_length=3, clip_val=5.0, \n",
    "                 num_hidden=512, num_heads=8, num_blocks=6, drop_rate=0.2,\n",
    "                 position_scale=1000, *args, **kwargs):\n",
    "        self.emb_size = emb_size\n",
    "        self.clip_val = clip_val\n",
    "        self.num_blocks = num_blocks\n",
    "        # leave index 0 for padding and 1 for  <eos>\n",
    "        self.vocab_size = processor.vocab_size + 2\n",
    "        self.maxlen = maxlen\n",
    "        self.additional_length = additional_length\n",
    "        self.position_scale = position_scale\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hidden = num_hidden\n",
    "        self.reuse = False\n",
    "        super().__init__(processor=processor, conf=conf, *args, **kwargs)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build tensorflow graph\n",
    "        \n",
    "        Note:\n",
    "            You build graphs for output and input, which will be used \n",
    "            for training and prediction.\n",
    "        \"\"\"\n",
    "        # Build Basic Netwoiork\n",
    "        self.enc_input = tf.placeholder(tf.int32, shape=(None, None), name=\"encoder_input\")\n",
    "        self.dec_input = tf.placeholder(tf.int32, shape=(None, None), name=\"decoder_input\")\n",
    "        self.dec_target = tf.placeholder(tf.int32, shape=(None, None), name=\"decoder_target\")\n",
    "        batch_size = tf.shape(self.dec_input)[0]\n",
    "        _dec_input = tf.concat((tf.ones((batch_size, 1), dtype=tf.int32), self.dec_input[:, :-1]), axis=1)\n",
    "        logits = self._get_output(self.enc_input, _dec_input, self.training)\n",
    "        # predictions = tf.cast(tf.arg_max(self.logits, dimension=-1), tf.int32)\n",
    "        target_smoothed = label_smoothing(tf.one_hot(self.dec_target, depth=self.vocab_size))\n",
    "        _loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target_smoothed)\n",
    "        masks = tf.cast(tf.sign(self.dec_target), tf.float32)\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(masks * _loss, [1]))\n",
    "        \n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimizer = self._get_optimizer(self.optimizer_name, self.learning_rate_op, self.optimizer_conf)\n",
    "            grads_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            if \"grad_clip\" in self.conf and self.conf[\"grad_clip\"] is not None:\n",
    "                grads_vars = [\n",
    "                    (tf.clip_by_norm(gv[0], clip_norm=self.conf[\"grad_clip\"]), gv[1]) \n",
    "                    for gv in grads_vars]\n",
    "            self.train_step = self.optimizer.apply_gradients(grads_vars)\n",
    "        # prediction flow\n",
    "        self.logits = self._get_output(self.enc_input, _dec_input, self.training)\n",
    "        self.predictions = tf.cast(tf.arg_max(self.logits, dimension=-1), tf.int32)\n",
    "        \n",
    "    def _get_output(self, enc_input, dec_target, training=True):\n",
    "        enc_length = get_length(enc_input)\n",
    "        dec_length = get_length(dec_target)\n",
    "        tensor_batch_size = tf.shape(enc_input)[0]\n",
    "        # Encoder\n",
    "        with tf.variable_scope(\"encoder\", reuse=self.reuse):\n",
    "            ## Embedding\n",
    "            enc_embeddings = tf.get_variable(\"embedding\", [self.vocab_size, self.num_hidden],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            enc_input_embedded = tf.nn.embedding_lookup(enc_embeddings, self.enc_input)\n",
    "            ## Positional Encoding\n",
    "            position_idx = tf.range(tf.reduce_max(enc_length))\n",
    "            position_idx = tf.tile(tf.expand_dims(position_idx, 0), [tensor_batch_size, 1])\n",
    "            enc_position_embeddings = tf.get_variable(\"embedding_position\",\n",
    "                                                      [self.maxlen, self.num_hidden],\n",
    "                                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            enc_position_embedded = tf.nn.embedding_lookup(enc_position_embeddings, position_idx)\n",
    "            enc_x = enc_input_embedded + enc_position_embedded\n",
    "            ## Dropout\n",
    "            enc_x = tf.layers.dropout(enc_x, rate=self.drop_rate, training=training)\n",
    "            # Encoder blocks    \n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    enc_x = multihead_attention(queries=enc_x, \n",
    "                                                keys=enc_x, \n",
    "                                                values=enc_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=False,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"self_attention\")\n",
    "                    ### Feed Forward\n",
    "                    enc_x = feedforward(enc_x, num_units=[4*self.num_hidden, self.num_hidden])\n",
    "            \n",
    "        # Decoder\n",
    "        with tf.variable_scope(\"decoder\", reuse=self.reuse):\n",
    "            ## Embedding\n",
    "            dec_embeddings = tf.get_variable(\"embedding\", [self.vocab_size, self.num_hidden],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            dec_input_embedded = tf.nn.embedding_lookup(dec_embeddings, self.dec_input)\n",
    "            ## Positional Encoding\n",
    "            position_idx = tf.range(tf.reduce_max(dec_length))\n",
    "            position_idx = tf.tile(tf.expand_dims(position_idx, 0), [tensor_batch_size, 1])\n",
    "            dec_position_embeddings = tf.get_variable(\"embedding_position\",\n",
    "                                                      [self.maxlen + self.additional_length, self.num_hidden],\n",
    "                                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "            dec_position_embedded = tf.nn.embedding_lookup(dec_position_embeddings, position_idx)\n",
    "            dec_x = dec_input_embedded + dec_position_embedded\n",
    "            ## Dropout\n",
    "            dec_x = tf.layers.dropout(dec_x, rate=self.drop_rate, training=training)\n",
    "                \n",
    "            ## Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    dec_x = multihead_attention(queries=dec_x, \n",
    "                                                keys=dec_x, \n",
    "                                                values=dec_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=True,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"self_attention\")\n",
    "                        \n",
    "                    dec_x = multihead_attention(queries=dec_x, \n",
    "                                                keys=enc_x, \n",
    "                                                values=enc_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=training,\n",
    "                                                causality=False,\n",
    "                                                reuse=self.reuse,\n",
    "                                                scope=\"vanilla_attention\")\n",
    "                    ### Feed Forward\n",
    "                    dec_x = feedforward(dec_x, num_units=[4*self.num_hidden, self.num_hidden],\n",
    "                                       scope=\"fully_connected\", reuse=self.reuse)\n",
    "                \n",
    "            # Final linear projection\n",
    "            logits = tf.layers.dense(dec_x, self.vocab_size, name=\"dense\", reuse=self.reuse)\n",
    "        self.reuse = True\n",
    "        return logits\n",
    "        \n",
    "    def _optimize(self, batch_X, batch_y, *args, **kwargs):\n",
    "        global count\n",
    "        batch_X = batch_X[0]\n",
    "        batch_X, Xlen = self.processor.batch_padding(batch_X, self.maxlen)\n",
    "        length = np.max(Xlen) + self.additional_length\n",
    "        batch_y = self._batch_padding(batch_y, length)\n",
    "        input_y = batch_y[:, :-1]\n",
    "        target_y = batch_y[:, 1:]\n",
    "        feed_dict = {self.enc_input: batch_X,\n",
    "                     self.dec_input: input_y,\n",
    "                     self.dec_target: target_y,\n",
    "                     self.training: True}\n",
    "        \n",
    "        _, loss = self.sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "        # print(\"loss\", loss)\n",
    "        if count % 10 == 0:\n",
    "            # logits = self.sess.run(self.logits, feed_dict=feed_dict)\n",
    "            # print(\"logits\", logits)\n",
    "            sentences = texts[:5]\n",
    "            predictions = self.generate_sentences(sentences)\n",
    "            print(predictions)\n",
    "        count += 1\n",
    "        return loss\n",
    "    \n",
    "    def generate_sentences(self, sentences):\n",
    "        X = [self.processor.encode(sentence) for sentence in sentences]\n",
    "        # X = sentences\n",
    "        X = [x_[::-1] for x_ in X]\n",
    "        X, Xlen = self.processor.batch_padding(X, self.maxlen)\n",
    "        batch_size = X.shape[0]\n",
    "        y = np.ones((batch_size,1), dtype=int)\n",
    "        for i in range(self.maxlen):\n",
    "            feed_dict = {self.enc_input: X,\n",
    "                         self.dec_input: y,\n",
    "                         self.training: False}\n",
    "            predictions = self.sess.run(self.predictions, feed_dict=feed_dict)\n",
    "            new_y = predictions[:, -1]\n",
    "            y = np.concatenate((y, new_y[:, np.newaxis]), axis=1)\n",
    "            finished = np.array(new_y > 1, dtype=int)\n",
    "            if np.sum(finished) == 0:\n",
    "                break\n",
    "        return [self.processor.decode(i) for i in y]\n",
    "    \n",
    "    def _batch_padding(self, batch, length):\n",
    "        EOS = 1\n",
    "        PAD = 0\n",
    "        padded_batch = []\n",
    "        for x in batch:\n",
    "            x = list(x)\n",
    "            if len(x) > length:\n",
    "                x = x[:length]\n",
    "            elif len(x) < length:\n",
    "                x.append(EOS)\n",
    "            while len(x) < length:\n",
    "                x.append(PAD)\n",
    "            padded_batch.append(x)\n",
    "        return np.array(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n",
      "[['reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference'], ['reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference'], ['school', 'school', 'school', 'school', 'school', 'nearing', 'school', 'school', 'school', 'school', 'school', 'school', 'school', 'school', 'school', 'school', 'school', 'school', 'school', 'school'], ['school', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference', 'reference'], ['wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly', 'wrongly']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/100 [00:08<13:36,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [00:13<11:51,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [00:18<10:37,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:27<09:01,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [00:32<08:33,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [00:37<08:12,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [00:42<07:57,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:52<07:32,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [00:57<07:24,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [01:02<07:19,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 13/100 [01:07<07:14,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [01:17<06:59,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 16/100 [01:22<06:55,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 17/100 [01:27<06:51,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 18/100 [01:32<06:47,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [01:41<06:33,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [01:46<06:30,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 22/100 [01:51<06:25,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 23/100 [01:56<06:21,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [02:06<06:08,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 26/100 [02:11<06:04,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 27/100 [02:16<06:01,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 28/100 [02:21<05:56,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [02:31<05:44,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.', '.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 31/100 [02:36<05:42,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 32/100 [02:41<05:37,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 33/100 [02:46<05:32,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [02:56<05:19,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 36/100 [03:01<05:16,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 37/100 [03:06<05:12,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 38/100 [03:11<05:07,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [03:20<04:55,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 41/100 [03:25<04:51,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 42/100 [03:30<04:47,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 43/100 [03:35<04:42,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [03:45<04:30,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 46/100 [03:50<04:26,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 47/100 [03:55<04:22,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 48/100 [04:00<04:18,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [04:10<04:06,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 51/100 [04:15<04:02,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 52/100 [04:20<03:57,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 53/100 [04:25<03:53,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['.'], ['.'], ['.'], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [04:35<03:41,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 56/100 [04:39<03:36,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 57/100 [04:44<03:31,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 58/100 [04:49<03:27,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [04:59<03:16,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 61/100 [05:04<03:11,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 62/100 [05:09<03:06,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 63/100 [05:14<03:02,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [05:24<02:51,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 66/100 [05:29<02:47,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 67/100 [05:34<02:42,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 68/100 [05:39<02:37,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [05:48<02:27,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 71/100 [05:53<02:22,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 72/100 [05:58<02:17,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 73/100 [06:03<02:13,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [06:13<02:02,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 76/100 [06:18<01:58,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 77/100 [06:23<01:53,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 78/100 [06:28<01:48,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [06:37<01:37,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 81/100 [06:42<01:33,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 82/100 [06:47<01:28,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 83/100 [06:52<01:23,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [07:02<01:13,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 86/100 [07:07<01:08,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 87/100 [07:12<01:04,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 88/100 [07:17<00:59,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [07:27<00:49,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 91/100 [07:32<00:44,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 92/100 [07:37<00:39,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 93/100 [07:42<00:34,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [07:51<00:24,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 96/100 [07:56<00:19,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 97/100 [08:01<00:14,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 98/100 [08:06<00:09,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:16<00:00,  4.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "conf = {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"learning_rate_minimum\": 1e-3,\n",
    "        \"learning_rate_decay\": 0.99,\n",
    "        \"learning_rate_decay_step\": 1,\n",
    "        \"batch_size\": 128,\n",
    "        \"model_dir\": \"./logs\",\n",
    "        \"load_file_path\": None,\n",
    "        \"save_file_path\": None,\n",
    "        \"log_freq\": 1,\n",
    "        \"grad_clip\":5.0,\n",
    "        \"optimizer\":\"gd\",\n",
    "        \"model\":[{\"name\":\"lstm\", \"num_units\":512},\n",
    "                {\"name\":\"lstm\", \"num_units\":512},],\n",
    "        # \"model\":[{\"name\":\"lstm\", \"num_units\":500},\n",
    "        #         {\"name\":\"lstm\", \"num_units\":500}],\n",
    "        \"attention_size\": 512\n",
    "}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "agent = DialogueAgent(processor, maxlen=20, conf=conf, additional_length=3)\n",
    "\n",
    "train_X = processor.data[:-1]\n",
    "train_y = processor.data[1:]\n",
    "agent.fit(train_X, train_y, num_epochs=100, batch_bar=False, log_freq=1, batch_log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = agent.generate_sentences(texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['You guys, you guys! Chef is going away. \\n',\n",
       "       'Going away? For how long?\\n'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.Variable(np.ones((10, 10)))\n",
    "with tf.variable_scope(\"hello\", reuse=False):\n",
    "    x = tf.contrib.layers.fully_connected(x, 10)\n",
    "    # x = tf.contrib.layers.fully_connected(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"hello\", reuse=True):\n",
    "    x = tf.contrib.layers.fully_connected(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ],\n",
       "       [ 0.04597573,  0.        ,  0.        ,  0.02855326,  0.        ,\n",
       "         0.30168584,  0.00910732,  0.        ,  0.90480631,  0.220675  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
