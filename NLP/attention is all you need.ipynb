{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 320 ms, sys: 368 ms, total: 688 ms\n",
      "Wall time: 317 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "# texts = clean_movie_data(data)\n",
    "df = pd.read_csv(\"data/All-seasons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Chef</td>\n",
       "      <td>I'm sorry boys.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Chef said he's been bored, so he joining a gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Season Episode Character                                               Line\n",
       "0     10       1      Stan         You guys, you guys! Chef is going away. \\n\n",
       "1     10       1      Kyle                        Going away? For how long?\\n\n",
       "2     10       1      Stan                                         Forever.\\n\n",
       "3     10       1      Chef                                  I'm sorry boys.\\n\n",
       "4     10       1      Stan  Chef said he's been bored, so he joining a gro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = df[\"Line\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['You guys, you guys! Chef is going away. \\n',\n",
       "       'Going away? For how long?\\n', 'Forever.\\n', \"I'm sorry boys.\\n\",\n",
       "       \"Chef said he's been bored, so he joining a group called the Super Adventure Club. \\n\",\n",
       "       'Wow!\\n',\n",
       "       'Chef?? What kind of questions do you think adventuring around the world is gonna answer?!\\n',\n",
       "       \"What's the meaning of life? Why are we here?\\n\",\n",
       "       \"I hope you're making the right choice.\\n\",\n",
       "       \"I'm gonna miss him.  I'm gonna miss Chef and I...and I don't know how to tell him! \\n\"], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43min 51s, sys: 15.2 s, total: 44min 6s\n",
      "Wall time: 44min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from hedgeable_ai.functions.preprocessing.word2index import Word2IndexProcessor\n",
    "\n",
    "processor = Word2IndexProcessor(texts[:], is_processed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "def layer_normalization(inputs, epsilon=1e-8, scope=\"layer_normalization\", reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        hidden_dim = shape[-1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        gamma = tf.get_variable(\"gamma\", hidden_dim, initializer=tf.ones_initializer(tf.float32))\n",
    "        beta = tf.get_variable(\"beta\", hidden_dim, initializer=tf.zeros_initializer(tf.float32))\n",
    "        normalized_inputs = (inputs - mean) / tf.sqrt(variance + epsilon)\n",
    "        outputs = gamma * normalized_inputs + beta\n",
    "    return outputs\n",
    "\n",
    "def embedding(inputs,vocab_size, num_units, \n",
    "              zero_pad=True, scale=True,\n",
    "              scope=\"embedding\", reuse=None):\n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "        \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]    \n",
    "    ```    \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        \n",
    "        if scale:\n",
    "            outputs = outputs * tf.sqrt(num_units) \n",
    "            \n",
    "    return outputs\n",
    "    \n",
    "def multihead_attention(queries, \n",
    "                        keys,\n",
    "                        values,\n",
    "                        num_units=None, \n",
    "                        num_heads=8, \n",
    "                        drop_rate=0,\n",
    "                        training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      values: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      drop_rate: A floating point number.\n",
    "      training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list()[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
    "        V = tf.layers.dense(values, num_units, activation=None) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        # The size will be [N * num_heads, T_k, C_k/num_heads]\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        alignments = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        d = tf.constant(K_.get_shape().as_list()[-1], tf.float32)\n",
    "        alignments = tf.nn.softmax(alignments / tf.sqrt(d))\n",
    "  \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            shape = tf.shape(alignments)\n",
    "            masks = tf.ones(shape[1:]) # (T_q, T_k)\n",
    "            masks = tf.contrib.linalg.LinearOperatorTriL(masks).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(masks, 0), [shape[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "            alignments = alignments * masks\n",
    "            sum_alignments = tf.reduce_sum(alignments, -1, keep_dims=True)\n",
    "            sum_alignments = tf.tile(sum_alignments, [1, 1, shape[-1]])\n",
    "            alingments = alignments / sum_alignments\n",
    "          \n",
    "        # Dropouts\n",
    "        alignments = tf.layers.dropout(alignments, rate=drop_rate, training=training)\n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(alignments, V_) # ( h*N, T_q, C/h)\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2) # (N, T_q, C)\n",
    "        # Residual connection\n",
    "        outputs += queries \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs) # (N, T_q, C)\n",
    "    return outputs\n",
    "\n",
    "def feedforward(inputs, \n",
    "                num_units=[2048, 512],\n",
    "                scope=\"multihead_attention\", \n",
    "                reuse=None):\n",
    "    '''Point-wise feed forward net.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, C].\n",
    "      num_units: A list of two integers.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A 3d tensor with shape of [N, T, V], where V is the number of vocabulary.\n",
    "      epsilon: Smoothing rate.\n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```    \n",
    "    '''\n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, conv2d\n",
    "from tensorflow.contrib.layers import conv2d_transpose, flatten\n",
    "\n",
    "from hedgeable_ai.models.nn import BaseModel\n",
    "        \n",
    "class FeedForward(BaseModel):\n",
    "    def __init__(self, model_params, scope_name, *args, **kwargs):\n",
    "        super().__init__(model_params, scope_name, *args, **kwargs)\n",
    "    \n",
    "    def __call__(self, x, training=True):\n",
    "        with tf.variable_scope(self.scope_name, reuse=self.reuse):\n",
    "            for i, params in enumerate(self.model_params):\n",
    "                with tf.variable_scope('layer_' + str(i)):\n",
    "                    if \"is_flatten\" in params and params[\"is_flatten\"]:\n",
    "                        x = flatten(x)\n",
    "                    if \"drop_rate\" in params:\n",
    "                        x = tf.layers.dropout(x, rate=params[\"drop_rate\"], training=training)\n",
    "                    # demtermine which layer to use\n",
    "                    if params[\"name\"] == \"dense\":\n",
    "                        x = fully_connected(x, params[\"num_hidden\"], activation_fn=None)\n",
    "                    elif params[\"name\"] == \"conv2d\":\n",
    "                        x =  conv2d(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                          params[\"stride\"], params[\"padding\"], activation_fn=None)\n",
    "                    elif params[\"name\"] == \"deconv2d\":\n",
    "                        x =  conv2d_transpose(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                          params[\"stride\"], params[\"padding\"], activation_fn=None)\n",
    "                    elif params[\"name\"] == \"reshape\":\n",
    "                        x = tf.reshape(x, (-1,) + params[\"reshape_size\"])\n",
    "                    elif params[\"name\"] == \"pooling\":\n",
    "                        del params[\"name\"]\n",
    "                        x = tf.nn.pool(x, **params)\n",
    "                    elif params[\"name\"] == None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"No implementation for 'name'={}\".format(params[\"name\"]))         \n",
    "                    if \"is_batch\" in params and params[\"is_batch\"]:\n",
    "                        x = tf.layers.batch_normalization(x, training=training, momentum=0.9)\n",
    "                    if \"activation\" in params:\n",
    "                        x = params[\"activation\"](x)\n",
    "            if self.reuse is False:\n",
    "                self.global_scope_name = tf.get_variable_scope().name\n",
    "                self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.global_scope_name)\n",
    "        self.reuse = True\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from hedgeable_ai.models.nn import BaseModel, get_shape, get_length\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from hedgeable_ai.models.nn.params import nn_is_logit\n",
    "from hedgeable_ai.models.nn import BaseNN, get_shape\n",
    "\n",
    "from hedgeable_ai.models.nn.rnn import get_cell\n",
    "\n",
    "count = 0\n",
    "class DialogueAgent(BaseNN):\n",
    "    def __init__(self, processor, emb_size=300, maxlen=20,\n",
    "                 conf=None, additional_length=3, clip_val=5.0, \n",
    "                 num_hidden=512, num_heads=8, num_blocks=6, drop_rate=0.2,\n",
    "                 position_scale=1000, *args, **kwargs):\n",
    "        self.emb_size = emb_size\n",
    "        self.clip_val = clip_val\n",
    "        self.num_blocks = num_blocks\n",
    "        # leave index 0 for padding and 1 for  <eos>\n",
    "        self.vocab_size = processor.vocab_size + 2\n",
    "        self.maxlen = maxlen\n",
    "        self.additional_length = additional_length\n",
    "        self.position_scale = position_scale\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hidden = num_hidden\n",
    "        super().__init__(processor=processor, conf=conf, *args, **kwargs)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build tensorflow graph\n",
    "        \n",
    "        Note:\n",
    "            You build graphs for output and input, which will be used \n",
    "            for training and prediction.\n",
    "        \"\"\"\n",
    "        # Build Basic Netwoiork\n",
    "        self.enc_input = tf.placeholder(tf.int32, shape=(None, None), name=\"encoder_input\")\n",
    "        self.dec_input = tf.placeholder(tf.int32, shape=(None, None), name=\"decoder_input\")\n",
    "        self.dec_target = tf.placeholder(tf.int32, shape=(None, None), name=\"decoder_target\")\n",
    "        self.training = tf.placeholder(tf.bool, (), name=\"trainig\")\n",
    "        enc_length = get_length(self.enc_input)\n",
    "        dec_length = get_length(self.dec_target)\n",
    "        tensor_batch_size = tf.shape(self.enc_input)[0]\n",
    "        # Encoder\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            ## Embedding\n",
    "            enc_embeddings = tf.get_variable(\"embedding\", [self.vocab_size, self.num_hidden],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            enc_input_embedded = tf.nn.embedding_lookup(enc_embeddings, self.enc_input)\n",
    "            ## Positional Encoding\n",
    "            position_idx = tf.range(tf.reduce_max(enc_length))\n",
    "            position_idx = tf.tile(tf.expand_dims(position_idx, 0), [tensor_batch_size, 1])\n",
    "            enc_position_embeddings = tf.get_variable(\"embedding_position\", [self.vocab_size, self.num_hidden],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            enc_position_embedded = tf.nn.embedding_lookup(enc_position_embeddings, position_idx)\n",
    "            enc_x = enc_input_embedded + enc_position_embedded\n",
    "            ## Dropout\n",
    "            enc_x = tf.layers.dropout(enc_x, rate=self.drop_rate, training=self.training)\n",
    "            # Encoder blocks    \n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    enc_x = multihead_attention(queries=enc_x, \n",
    "                                                keys=enc_x, \n",
    "                                                values=enc_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=self.training,\n",
    "                                                causality=False,\n",
    "                                                scope=\"self_attention\")\n",
    "                    ### Feed Forward\n",
    "                    enc_x = feedforward(enc_x, num_units=[4*self.num_hidden, self.num_hidden])\n",
    "            \n",
    "        # Decoder\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            ## Embedding\n",
    "            dec_embeddings = tf.get_variable(\"embedding\", [self.vocab_size, self.num_hidden],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            dec_input_embedded = tf.nn.embedding_lookup(dec_embeddings, self.dec_input)\n",
    "            ## Positional Encoding\n",
    "            position_idx = tf.range(tf.reduce_max(dec_length))\n",
    "            position_idx = tf.tile(tf.expand_dims(position_idx, 0), [tensor_batch_size, 1])\n",
    "            dec_position_embeddings = tf.get_variable(\"embedding_position\", [self.vocab_size, self.num_hidden],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            dec_position_embedded = tf.nn.embedding_lookup(dec_position_embeddings, position_idx)\n",
    "            dec_x = dec_input_embedded + dec_position_embedded\n",
    "            ## Dropout\n",
    "            dec_x = tf.layers.dropout(dec_x, rate=self.drop_rate, training=self.training)\n",
    "                \n",
    "            ## Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                    ### Multihead Attention\n",
    "                    dec_x = multihead_attention(queries=dec_x, \n",
    "                                                keys=dec_x, \n",
    "                                                values=dec_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=self.training,\n",
    "                                                causality=True,\n",
    "                                                scope=\"self_attention\")\n",
    "                        \n",
    "                    dec_x = multihead_attention(queries=dec_x, \n",
    "                                                keys=enc_x, \n",
    "                                                values=enc_x,\n",
    "                                                num_units=self.num_hidden,\n",
    "                                                num_heads=self.num_heads, \n",
    "                                                drop_rate=self.drop_rate,\n",
    "                                                training=self.training,\n",
    "                                                causality=False,\n",
    "                                                scope=\"vanilla_attention\")\n",
    "                    ### Feed Forward\n",
    "                    dec_x = feedforward(dec_x, num_units=[4*self.num_hidden, self.num_hidden])\n",
    "                \n",
    "        # Final linear projection\n",
    "        self.logits = tf.layers.dense(dec_x, self.vocab_size)\n",
    "        self.predictions = tf.cast(tf.arg_max(self.logits, dimension=-1), tf.int32)\n",
    "        target_smoothed = label_smoothing(tf.one_hot(self.dec_target, depth=self.vocab_size))\n",
    "        _loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=target_smoothed)\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(_loss, [1]))\n",
    "        \n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimizer = self._get_optimizer(self.optimizer_name, self.learning_rate_op, self.optimizer_conf)\n",
    "            grads_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            clipped_grads_vars = [\n",
    "                (tf.clip_by_norm(gv[0], clip_norm=self.clip_val), gv[1]) \n",
    "                for gv in grads_vars]\n",
    "            self.train_step = self.optimizer.apply_gradients(clipped_grads_vars)\n",
    "        \n",
    "    def _optimize(self, batch_X, batch_y, *args, **kwargs):\n",
    "        global count\n",
    "        batch_X = batch_X[0]\n",
    "        batch_X, Xlen = self.processor.batch_padding(batch_X, self.maxlen)\n",
    "        length = np.max(Xlen) + self.additional_length\n",
    "        batch_y = self._batch_padding(batch_y, length)\n",
    "        input_y = batch_y[:, :-1]\n",
    "        target_y = batch_y[:, 1:]\n",
    "        feed_dict = {self.enc_input: batch_X,\n",
    "                     self.dec_input: input_y,\n",
    "                     self.dec_target: target_y,\n",
    "                     self.training: True}\n",
    "        \n",
    "        _, loss = self.sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "        # print(\"loss\", loss)\n",
    "        if count % 1000 == 0:\n",
    "            sentences = [\"how are you?\", \"what is your name?\", \"you should kill youself.\"]\n",
    "            predictions = self.generate_sentences(sentences)\n",
    "            print(predictions)\n",
    "        count += 1\n",
    "        return loss\n",
    "    \n",
    "    def generate_sentences(self, sentences):\n",
    "        X = [self.processor.encode(sentence) for sentence in sentences]\n",
    "        # X = sentences\n",
    "        X = [x_[::-1] for x_ in X]\n",
    "        X, Xlen = self.processor.batch_padding(X, self.maxlen)\n",
    "        batch_size = X.shape[0]\n",
    "        y = np.ones((batch_size,1), dtype=int)\n",
    "        for i in range(self.maxlen):\n",
    "            feed_dict = {self.enc_input: X,\n",
    "                         self.dec_input: y,\n",
    "                         self.dec_target:y,\n",
    "                         self.training: False}\n",
    "            predictions = self.sess.run(self.predictions, feed_dict=feed_dict)\n",
    "            new_y = predictions[:, -1]\n",
    "            y = np.concatenate((y, new_y[:, np.newaxis]), axis=1)\n",
    "            finished = np.array(new_y > 1, dtype=int)\n",
    "            if np.sum(finished) == 0:\n",
    "                break\n",
    "        return [self.processor.decode(i) for i in y]\n",
    "    \n",
    "    def _batch_padding(self, batch, length):\n",
    "        EOS = 1\n",
    "        PAD = 0\n",
    "        padded_batch = []\n",
    "        for x in batch:\n",
    "            x = list(x)\n",
    "            if len(x) > length:\n",
    "                x = x[:length]\n",
    "            elif len(x) < length:\n",
    "                x.append(EOS)\n",
    "            while len(x) < length:\n",
    "                x.append(PAD)\n",
    "            padded_batch.append(x)\n",
    "        return np.array(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 1/10000 [03:29<582:46:32, 209.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.', 'the'], ['.', 'the'], ['.', 'the']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 2/10000 [07:02<585:23:16, 210.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 3/10000 [10:36<587:51:56, 211.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'i'], ['i', 'i'], ['i', 'i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 4/10000 [14:11<590:28:04, 212.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 5/10000 [17:46<592:06:03, 213.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['is'], ['is'], ['is']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 6/10000 [21:21<593:35:52, 213.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 7/10000 [24:56<594:32:40, 214.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 8/10000 [28:31<595:24:24, 214.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 9/10000 [32:06<595:57:40, 214.74s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 10/10000 [35:42<596:30:33, 214.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 11/10000 [39:17<596:48:49, 215.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 12/10000 [42:53<597:09:31, 215.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 13/10000 [46:28<597:19:42, 215.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 14/10000 [50:04<597:22:12, 215.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 15/10000 [53:40<597:43:54, 215.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 16/10000 [57:16<597:58:26, 215.62s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 17/10000 [1:00:52<598:19:28, 215.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 18/10000 [1:04:28<598:19:20, 215.78s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 19/10000 [1:08:04<598:25:49, 215.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 20/10000 [1:11:40<598:30:17, 215.89s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 21/10000 [1:15:16<598:33:21, 215.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 22/10000 [1:18:52<598:41:51, 216.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 23/10000 [1:22:28<598:54:43, 216.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 24/10000 [1:26:04<599:02:43, 216.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 25/10000 [1:29:41<599:01:22, 216.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 26/10000 [1:33:17<598:51:53, 216.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 27/10000 [1:36:53<599:01:58, 216.24s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 28/10000 [1:40:29<599:05:44, 216.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 29/10000 [1:44:06<599:12:30, 216.34s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 30/10000 [1:47:43<599:22:15, 216.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 31/10000 [1:51:19<599:20:59, 216.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 32/10000 [1:54:56<599:22:58, 216.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 33/10000 [1:58:32<599:22:40, 216.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 34/10000 [2:02:09<599:38:13, 216.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 35/10000 [2:05:45<599:21:54, 216.53s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 36/10000 [2:09:22<599:35:51, 216.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 37/10000 [2:12:59<599:39:17, 216.68s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i'], ['i'], ['i']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 38/10000 [2:16:36<599:46:03, 216.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 39/10000 [2:20:13<599:39:04, 216.72s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 40/10000 [2:23:50<599:51:26, 216.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 41/10000 [2:27:26<599:38:57, 216.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 42/10000 [2:31:03<599:41:14, 216.80s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 43/10000 [2:34:40<599:54:53, 216.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 44/10000 [2:38:17<599:58:32, 216.95s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 45/10000 [2:41:55<600:11:20, 217.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 46/10000 [2:45:31<599:55:04, 216.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 47/10000 [2:49:08<599:56:29, 217.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 48/10000 [2:52:45<599:53:18, 217.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 49/10000 [2:56:23<599:52:20, 217.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 50/10000 [3:00:00<599:55:07, 217.06s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 51/10000 [3:03:37<599:57:09, 217.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 52/10000 [3:07:14<600:03:30, 217.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 53/10000 [3:10:52<600:10:58, 217.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 54/10000 [3:14:29<600:12:35, 217.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 55/10000 [3:18:06<600:23:51, 217.34s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 56/10000 [3:21:44<600:26:50, 217.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 57/10000 [3:25:21<600:20:15, 217.36s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 58/10000 [3:28:59<600:16:48, 217.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 59/10000 [3:32:36<600:11:03, 217.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 60/10000 [3:36:13<600:15:48, 217.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 61/10000 [3:39:51<600:04:17, 217.35s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 62/10000 [3:43:28<600:19:06, 217.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 63/10000 [3:47:06<600:15:04, 217.46s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 64/10000 [3:50:43<599:56:42, 217.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 65/10000 [3:54:21<600:05:42, 217.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 66/10000 [3:57:58<600:08:01, 217.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 67/10000 [4:01:36<599:58:30, 217.45s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 68/10000 [4:05:13<600:09:38, 217.54s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 69/10000 [4:08:51<600:03:07, 217.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 70/10000 [4:12:28<600:06:43, 217.56s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 71/10000 [4:16:06<600:11:57, 217.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 72/10000 [4:19:44<600:21:20, 217.70s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 73/10000 [4:23:22<600:22:48, 217.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 74/10000 [4:27:00<600:24:04, 217.76s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 75/10000 [4:30:38<600:28:34, 217.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 76/10000 [4:34:15<600:21:08, 217.78s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 77/10000 [4:37:53<600:25:56, 217.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 78/10000 [4:41:31<600:21:38, 217.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 79/10000 [4:45:09<600:20:51, 217.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 80/10000 [4:48:47<600:14:12, 217.83s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 81/10000 [4:52:24<600:04:17, 217.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 82/10000 [4:56:02<600:09:29, 217.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 83/10000 [4:59:40<600:16:18, 217.91s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 84/10000 [5:03:19<600:22:27, 217.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 85/10000 [5:06:57<600:26:20, 218.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 86/10000 [5:10:35<600:36:57, 218.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 87/10000 [5:14:13<600:29:36, 218.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 88/10000 [5:17:52<601:04:00, 218.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 89/10000 [5:21:30<601:00:52, 218.31s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 90/10000 [5:25:09<601:09:03, 218.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 91/10000 [5:28:47<600:37:50, 218.21s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 92/10000 [5:32:25<600:27:52, 218.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 93/10000 [5:36:03<600:25:30, 218.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  1%|          | 94/10000 [5:39:41<600:23:43, 218.19s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "conf = {\n",
    "        \"learning_rate\": 1e-1,\n",
    "        \"learning_rate_minimum\": 1e-3,\n",
    "        \"learning_rate_decay\": 0.99,\n",
    "        \"learning_rate_decay_step\": 1,\n",
    "        \"batch_size\": 128,\n",
    "        \"model_dir\": \"./logs\",\n",
    "        \"load_file_path\": None,\n",
    "        \"save_file_path\": None,\n",
    "        \"log_freq\": 1,\n",
    "        \"optimizer\":\"gd\",\n",
    "        \"model\":[{\"name\":\"lstm\", \"num_units\":512},\n",
    "                {\"name\":\"lstm\", \"num_units\":512},],\n",
    "        # \"model\":[{\"name\":\"lstm\", \"num_units\":500},\n",
    "        #         {\"name\":\"lstm\", \"num_units\":500}],\n",
    "        \"attention_size\": 512\n",
    "}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "agent = DialogueAgent(processor, maxlen=20, conf=conf, additional_length=3)\n",
    "\n",
    "train_X = processor.data[:-1]\n",
    "train_y = processor.data[1:]\n",
    "agent.fit(train_X, train_y, num_epochs=10000, batch_bar=False, log_freq=1, batch_log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = agent.generate_sentences(texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['anyhow', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse'], ['aloha', 'aloha', 'flushing', 'flushing', 'flushing', 'flushing', 'flushing', 'aloha', 'flushing', 'aloha'], ['legitimate', 'best-known', 'best-known', 'best-known', 'best-known', 'best-known', 'best-known', 'best-known', 'best-known', 'best-known'], ['ended', 'chi-chis', 'chi-chis', 'chi-chis', 'chi-chis', 'chi-chis', 'chi-chis', 'chi-chis', 'chi-chis', 'chi-chis'], ['warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse'], ['legitimate', 'legitimate', 'legitimate', 'legitimate', 'legitimate', 'legitimate', 'legitimate', 'legitimate', 'legitimate', 'legitimate'], ['anyhow', 'anyhow', 'anyhow', 'anyhow', 'anyhow', 'anyhow', 'anyhow', 'anyhow', 'anyhow', 'anyhow'], ['warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse'], ['warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse', 'warehouse'], ['pie-hole', 'pie-hole', 'pie-hole', 'pie-hole', 'pie-hole', 'pie-hole', 'pie-hole', 'pie-hole', 'pie-hole', 'pie-hole']]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17960"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You guys, you guys! Chef is going away. \\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
