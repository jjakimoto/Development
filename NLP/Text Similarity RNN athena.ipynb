{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = df.dropna()\n",
    "X1 = df[\"question1\"].values\n",
    "X2 = df[\"question2\"].values\n",
    "y = df[\"is_duplicate\"].values\n",
    "X= [X1, X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "length = [len(x.split()) for x in X[0]]\n",
    "length.extend([len(x.split()) for x in X[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 2 * int(np.mean(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 43s, sys: 21.6 s, total: 2min 5s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "padding = \"post\"\n",
    "truncating = \"post\"\n",
    "pad_val = 0\n",
    "maxlen = None\n",
    "\n",
    "sentence_params = {\"emb_mode\": 0, \"emb_dim\" : 300,\n",
    "                   \"remove_stopwords\": False, \"remove_special\": False}\n",
    "vector_file_path = None\n",
    "\n",
    "def pad_sequences(sequences, maxlen=maxlen, \n",
    "                  padding=padding, truncating=truncating, value=pad_val):\n",
    "    \"\"\"Pad sequences to get the same length sequences\n",
    "\n",
    "    Args:\n",
    "        sequences: list(array-like), each element has to satisfy\n",
    "            shape = (length,) + element_shape\n",
    "        maxlen: int, optional, if not specified, maximum length among\n",
    "            sequences will be set as maxlen\n",
    "        padding: str, the methods of padding: 'post' or 'pre'\n",
    "        value: int or float, optional value to be used for padding\n",
    "    \n",
    "    Returns:\n",
    "        array-like, shape = (len(sequences), maxlen,) + element_shape\n",
    "    \"\"\"\n",
    "    dtype = np.array(sequences[0]).dtype\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max([len(seq) for seq in sequences])\n",
    "    for seq in sequences:\n",
    "        if list(seq):\n",
    "            shape = np.array(seq).shape\n",
    "            break\n",
    "    num_dim = len(shape) - 1\n",
    "    value = (value, value)\n",
    "    processed_sequences = []\n",
    "    for seq in sequences:\n",
    "        if not list(seq):\n",
    "            seq = np.zeros((maxlen,) + shape[1:])\n",
    "        elif len(seq) >= maxlen:\n",
    "            if padding == \"post\":\n",
    "                seq = seq[:maxlen]\n",
    "            else:\n",
    "                seq = seq[-maxlen:]\n",
    "        else:\n",
    "            if padding == \"post\":\n",
    "                pad_conf = ((0, maxlen - len(seq)),)\n",
    "            else:\n",
    "                pad_conf = ((maxlen - len(seq), 0),)\n",
    "            for i in range(num_dim):\n",
    "                pad_conf += ((0, 0),)\n",
    "            seq = np.pad(np.array(seq), pad_conf,\n",
    "                         mode=\"constant\", constant_values=value)\n",
    "        processed_sequences.append(seq)\n",
    "    return np.array(processed_sequences, dtype=dtype)\n",
    "\n",
    "class BasicProcessor(object):\n",
    "    \"\"\"Process data for estimators.\"\"\"\n",
    "        \n",
    "    def batch_process(self, X, y=None):\n",
    "        \"\"\"Make sure to have numpy data for input and target\"\"\"\n",
    "        if y is None:\n",
    "            return np.array(X)\n",
    "        else:\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "    def batch_process_y(self, y):\n",
    "        return np.array(y)\n",
    "\n",
    "class SentenceProcessor(BasicProcessor):\n",
    "    \"\"\"Processor class to embed text data into numeric vectors.\n",
    "\n",
    "    This class embeds text data through Bag of Words, Word2Vec, and\n",
    "    Doc2Vec, Word2vec and Doc2Vec are based on the following papers:\n",
    "\n",
    "    https://arxiv.org/pdf/1301.3781.pdf\n",
    "    https://arxiv.org/pdf/1405.4053.pdf\n",
    "\n",
    "    They are implemented through python package gensim:\n",
    "\n",
    "    https://radimrehurek.com/gensim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences=None, embedding=\"w2v\", emb_mode=sentence_params[\"emb_mode\"], \n",
    "                 emb_dim=sentence_params[\"emb_dim\"],\n",
    "                 remove_stopwords=sentence_params[\"remove_stopwords\"], \n",
    "                 remove_special=sentence_params[\"remove_special\"],\n",
    "                 vector_file_path=vector_file_path, maxlen=None):\n",
    "        \"\"\"Initialize a processor for text data.\n",
    "\n",
    "        Args:\n",
    "            sentences: list(str)\n",
    "            embedding: str, name of methods of mebeddings; we\n",
    "                currently have 'w2v', 'w2v_avg', 'w2v_index',\n",
    "                'd2v', and 'bow'.\n",
    "            emb_mode: int, mode of vector representation. if 2\n",
    "                is chosen, emb_dim becomes double\n",
    "            emb_dim: int, dimention of embedding; this is used\n",
    "                all of methods except for 'bow'.\n",
    "            revmoe_stopswords: bool, if True, remove stopwords\n",
    "                downloaded by nltk\n",
    "            remove_sords: bool, if True, remove charcters other than\n",
    "                alphabets and numbers\n",
    "            vector_file_path: str, path to use pretrained word2vec;\n",
    "                this is used only for 'w2v', 's2v_avg', and\n",
    "                'w2v_index'.\n",
    "        \"\"\"\n",
    "        self._initialize_processor(sentences, embedding, emb_mode, emb_dim,\n",
    "                                   remove_stopwords, remove_special,\n",
    "                                   maxlen, vector_file_path)\n",
    "        \n",
    "    def batch_process(self, sentences, labels=None):\n",
    "        if labels is None:\n",
    "            return self.sentence2vec(sentences)\n",
    "        else:\n",
    "            return self.sentence2vec(sentences), np.array(labels)\n",
    "\n",
    "    def sentence2vec(self, sentences, remove_stopwords=None,\n",
    "                     remove_special=None):\n",
    "        \"\"\"Convert text data into numeric.\n",
    "\n",
    "        Args:\n",
    "            sentences: list(str)\n",
    "            embedding: str, name of methods of mebeddings; we\n",
    "                currently have 'w2v', 'w2v_avg', 'w2v_index',\n",
    "                'd2v', and 'bow'.\n",
    "            emb_dim: int, dimention of embedding; this is used\n",
    "                all of methods except for 'bow'.\n",
    "            revmoe_stopswords: bool, if True, remove stopwords\n",
    "                downloaded by nltk\n",
    "            remove_sords: bool, if True, remove charcters other than\n",
    "                alphabets and numbers\n",
    "            file_path: str, path to down load pretrained word2vec;\n",
    "                this is used only for 'w2v', 's2v_avg', and\n",
    "                'w2v_index'.\n",
    "\n",
    "        Returns:\n",
    "            np.array: embbeded vecotors or indices of words\n",
    "        \"\"\"\n",
    "        if not remove_stopwords:\n",
    "            remove_stopwords = self.remove_stopwords\n",
    "        if not remove_special:\n",
    "            remove_special = self.remove_special\n",
    "        # Get text data in the format, list(str).\n",
    "        sentences = self._preprocess_sentence(\n",
    "                sentences, remove_stopwords, remove_special)\n",
    "        # Convert into numeric data\n",
    "        # Get bag of words.\n",
    "        if self.embedding == \"bow\":\n",
    "            _vectors = self.bow.transform(sentences).toarray()\n",
    "        # Get word2vec and pad it with 0 vetors to have the fix length\n",
    "        # and deal with unkonw words\n",
    "        elif self.embedding == \"w2v\":\n",
    "            _vectors = []\n",
    "            for sentence in sentences:\n",
    "                if self.emb_mode == 2:\n",
    "                    _vector = [\n",
    "                        np.concatenate((self.w2v[0][w], self.w2v[1][w]), axis=0)\n",
    "                        for w in sentence if w in self.vocab.keys()]\n",
    "                else:\n",
    "                    _vector = [self.w2v[0][w] for w in sentence if w in self.vocab.keys()]\n",
    "                _vectors.append(_vector)\n",
    "            _vectors = pad_sequences(_vectors, maxlen=self.maxlen)\n",
    "        # Get index to look up parameters for word2vec.\n",
    "        elif self.embedding == \"w2v_index\":\n",
    "            _vectors = []\n",
    "            for sentence in sentences:\n",
    "                _vector = []\n",
    "                for w in sentence:\n",
    "                    if w not in self.vocab.keys():\n",
    "                        _vector.append(0)\n",
    "                    else:\n",
    "                        _vector.append(self.vocab[w])\n",
    "                _vectors.append(_vector)\n",
    "            # Unkown words and padding refer to 0 index, which returns\n",
    "            # 0 vectors.\n",
    "            _vectors = pad_sequences(_vectors, maxlen=self.maxlen)\n",
    "        # Get word2vec averaged over each sentence\n",
    "        elif self.embedding == \"w2v_avg\":\n",
    "            _vectors = []\n",
    "            # To use both of cbow and skip gram, we have 2*emb_dim\n",
    "            # dimension\n",
    "            zerovec = np.zeros(self.get_dim())\n",
    "            for sentence in sentences:\n",
    "                if self.emb_mode == 2:\n",
    "                    _vector = [\n",
    "                        np.concatenate((self.w2v[0][w], self.w2v[1][w]), axis=0)\n",
    "                        for w in sentence if w in self.vocab.keys()]\n",
    "                else:\n",
    "                    _vector = [\n",
    "                        self.w2v[0][w] for w in sentence if w in self.vocab.keys()]\n",
    "                if not _vector:\n",
    "                    _vector = [zerovec,]\n",
    "                _vector = np.mean(_vector, axis=0)\n",
    "                _vectors.append(_vector)\n",
    "        # Get doc2vec, which combines two types doc2vec\n",
    "        elif self.embedding == \"d2v\":\n",
    "            _vectors = []\n",
    "            for sentence in sentences:\n",
    "                if self.emb_mode == 2:\n",
    "                    _vector = np.concatenate(\n",
    "                        (self.d2v[0].infer_vector(sentence.words),\n",
    "                         self.d2v[1].infer_vector(sentence.words)),\n",
    "                         axis=0)\n",
    "                else:\n",
    "                    _vector = self.d2v[0].infer_vector(sentence.words)\n",
    "                _vectors.append(_vector)\n",
    "        return np.array(_vectors)\n",
    "\n",
    "    def get_params(self):\n",
    "        weights0 = np.zeros((1, self.emb_dim))\n",
    "        if self.embedding == \"w2v\":\n",
    "            # add poarameters for index 0\n",
    "            params = [\n",
    "                np.concatenate((vec.syn0, weights0), axis=0) for vec in self.w2v]\n",
    "        elif self.embedding == \"d2v\":\n",
    "            params = [\n",
    "                np.concatenate((vec.syn0, weights0), axis=0) for vec in self.d2v]\n",
    "        else:\n",
    "            raise ValueError(\"We do not have parameters for\"\n",
    "                             \"embedding={}\".format(self.embedding))\n",
    "        return params\n",
    "    \n",
    "    def get_dim(self):\n",
    "        if self.embedding == \"w2v\"\\\n",
    "                or self.embedding == \"w2v_avg\"\\\n",
    "                or self.embedding == \"d2v\":\n",
    "            if self.emb_mode == 0 or self.emb_mode == 1:\n",
    "                return self.emb_dim\n",
    "            else:\n",
    "                return 2 * self.emb_dim\n",
    "        elif self.embedding == \"w2v\":\n",
    "            return 0\n",
    "        elif self.embedding == \"bow\":\n",
    "            return None            \n",
    "\n",
    "    def _preprocess_sentence(self, sentences,\n",
    "                             remove_stopwords=None, remove_special=None):\n",
    "        \"\"\"Remove stopwords or special characters if necessary\n",
    "\n",
    "        Args:\n",
    "            sentences: list(str)\n",
    "            revmoe_stopswords: bool, if True, remove stopwords\n",
    "                downloaded by nltk\n",
    "            remove_sords: bool, if True, remove charcters other than\n",
    "                alphabets and numbers\n",
    "\n",
    "        Returns:\n",
    "            list(sentences)\n",
    "        \"\"\"\n",
    "        _sentences = []\n",
    "        if not remove_stopwords:\n",
    "            remove_stopwords = self.remove_stopwords\n",
    "        if not remove_special:\n",
    "            remove_special = self.remove_special\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence = nltk.word_tokenize(sentence)\n",
    "            sentence = \" \".join(sentence)\n",
    "            # Remove special characters and split a sentence into\n",
    "            # a list of words\n",
    "            if remove_special:\n",
    "                _sentence = re.sub(r\"[^a-z]\", \" \", sentence).split()\n",
    "            else:\n",
    "                _sentence = sentence.split()\n",
    "            # Remove stopwords, e.g., he, she, you, ...\n",
    "            if remove_stopwords:\n",
    "                stops = set(stopwords.words(\"english\"))\n",
    "                _sentence = [w for w in _sentence if w not in stops]\n",
    "            if self.embedding == \"bow\":\n",
    "                _sentence = \" \".join(_sentence)\n",
    "            elif self.embedding == \"w2v\"\\\n",
    "                    or self.embedding == \"w2v_avg\"\\\n",
    "                    or self.embedding == \"w2v_index\":\n",
    "                # Word2vec use a list of words as input\n",
    "                pass\n",
    "            elif self.embedding == \"d2v\":\n",
    "                tag = \"SENT_\" + str(i)\n",
    "                _sentence = TaggedDocument(words=_sentence, tags=[tag])\n",
    "            else:\n",
    "                raise ValueError(\"We do not have implementation for\"\n",
    "                                 \"embedding={}\".format(embedding))\n",
    "            _sentences.append(_sentence)\n",
    "        return _sentences\n",
    "\n",
    "    def _initialize_processor(self, sentences, embedding, emb_mode, emb_dim,\n",
    "                              remove_stopwords, remove_special, maxlen, file_path):\n",
    "        \"\"\"Initialize a processor and keep parameters as attirbutes.\n",
    "\n",
    "        Args:\n",
    "            sentences: list(str)\n",
    "            embedding: str, name of methods of mebeddings; we\n",
    "                currently have 'w2v', 'w2v_avg', 'w2v_index',\n",
    "                'd2v', and 'bow'.\n",
    "            emb_mode: int, mode of vector representation. If 2\n",
    "                is chosen, emb_dim becomes double\n",
    "            emb_dim: int, dimention of embedding; this is used\n",
    "                all of methods except for 'bow'.\n",
    "            revmoe_stopswords: bool, if True, remove stopwords\n",
    "                downloaded by nltk\n",
    "            remove_sords: bool, if True, remove charcters other than\n",
    "                alphabets and numbers\n",
    "            file_path: str, path to use pretrained word2vec;\n",
    "                this is used only for 'w2v', 's2v_avg', and\n",
    "                'w2v_index'.\n",
    "        \"\"\"\n",
    "        # Initialize parameters and keep them as attributes.\n",
    "        self.embedding = embedding\n",
    "        self.emb_mode = emb_mode\n",
    "        self.emb_dim = emb_dim\n",
    "        self.file_path = file_path\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_special = remove_special\n",
    "        self.maxlen = maxlen\n",
    "        if sentences is not None:\n",
    "            lengthes = [len(x) for x in sentences]\n",
    "            self.maxlen = int(np.mean(lengthes))\n",
    "            sentences = self._preprocess_sentence(sentences,\n",
    "                                              remove_stopwords,\n",
    "                                              remove_special)\n",
    "        if embedding == \"bow\":\n",
    "            self.bow = CountVectorizer(analyzer=\"word\", tokenizer=None,\n",
    "                                       preprocessor=None, stop_words=None,\n",
    "                                       max_features=5000)\n",
    "            self.bow.fit(sentences)\n",
    "        elif embedding == \"w2v\"\\\n",
    "                or embedding == \"w2v_avg\"\\\n",
    "                or embedding == \"w2v_index\":\n",
    "            # mode=1: cbow, mode=1: skip-gram, and mode=2: both\n",
    "            self.w2v = []\n",
    "            if self.file_path is not None:\n",
    "                vec = KeyedVectors.load_word2vec_format(self.file_path,\n",
    "                                                        binary=True)\n",
    "                self.w2v.append(vec)\n",
    "                self.emb_mode = 0\n",
    "            else:\n",
    "                if self.emb_mode == 0 or self.emb_mode == 2:\n",
    "                    cbow = gensim.models.word2vec.Word2Vec(\n",
    "                    sentences, iter=6, size=emb_dim,\n",
    "                    window=5, min_count=3, workers=4, max_vocab_size=50000, sg=0)\n",
    "                    self.w2v.append(cbow)\n",
    "                if self.emb_mode == 1 or self.emb_mode == 2:\n",
    "                    skpg = gensim.models.word2vec.Word2Vec(\n",
    "                        sentences, iter=6, size=emb_dim,\n",
    "                        window=5, min_count=3, workers=4, max_vocab_size=50000, sg=1)\n",
    "                    self.w2v.append(skpg)\n",
    "            # Keep index 0 for unknown or padding.\n",
    "            self.vocab = {}\n",
    "            for word in iter(self.w2v[0].wv.vocab.keys()):\n",
    "                self.vocab[word] = self.w2v[0].wv.vocab[word].index + 1\n",
    "        elif embedding == \"d2v\":\n",
    "            # mode=0: pvdbow, mode=1: pvdm, and mode=2: both\n",
    "            self.d2v = []\n",
    "            if self.emb_mode == 0 or self.emb_mode == 2:\n",
    "                if file_path is None:\n",
    "                    pvdbow = Doc2Vec(sentences, size=emb_dim, dm=0, alpha=1e-2,\n",
    "                                 min_alpha=1e-3, sample=1e-5, \n",
    "                                window=15, min_count=3, workers=4,\n",
    "                                max_vocab_size=50000, iter=6)\n",
    "                else:\n",
    "                    pvdbow = Doc2Vec.load(file_path)\n",
    "                    self.emb_mode = 0\n",
    "                self.d2v.append(pvdbow)\n",
    "            if self.emb_mode == 1 or self.emb_mode == 2:\n",
    "                if file_path is None:\n",
    "                    pvdm = Doc2Vec(sentences, size=emb_dim, dm=1, alpha=1e-2,\n",
    "                                min_alpha=1e-3, sample=1e-6, \n",
    "                                window=5, min_count=3, workers=4,\n",
    "                                max_vocab_size=50000, iter=6)\n",
    "                else:\n",
    "                    pvdm = Doc2Vec.load(file_path)\n",
    "                self.d2v.append(pvdm)\n",
    "\n",
    "processor = SentenceProcessor(None, \"w2v\", maxlen=maxlen, vector_file_path=\"data/GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What is the step by step guide to invest in share market?',\n",
       "       'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?',\n",
       "       'How can Internet speed be increased by hacking through DNS?', ...,\n",
       "       \"What's this coin?\",\n",
       "       'I am having little hairfall problem but I want to use hair styling product. Which one should I prefer out of gel, wax and clay?',\n",
       "       'What is it like to have sex with your cousin?'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_sess = None\n",
    "load_file_path = None\n",
    "save_file_path = None\n",
    "overwrite = True\n",
    "\n",
    "\n",
    "nn_conf = {\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"learning_rate_minimum\": 1e-5,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 10,\n",
    "        \"batch_size\": 32,\n",
    "        \"model_dir\": \"./logs\",\n",
    "        \"load_file_path\": None,\n",
    "        \"save_file_path\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import tensorflow as tf\n",
    "from time import time \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from logging import getLogger\n",
    "from copy import deepcopy\n",
    "from tqdm import trange\n",
    "import gc\n",
    "\n",
    "is_processed, is_training = False, True\n",
    "\n",
    "class BaseEstimator(object):\n",
    "    \"\"\"Abstract class for all estimators\n",
    "    \n",
    "    Each estimator is used with some mixin class, e.g., ClassifierMixin\n",
    "    RegressorMixin. Do not use this abstract base class directly but \n",
    "    instead use one of the concrete estimators implemented.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor):\n",
    "        \"\"\"Initialize an estimators\n",
    "        \n",
    "        Args:\n",
    "            model_params: dictionary, parameters of estimators, \n",
    "                e.g., output_dim and input_shape\n",
    "            processor: helper class\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        # Check if a model is optimized\n",
    "        self.is_trained = False\n",
    "\n",
    "    def load_params(self, filepath):\n",
    "        \"\"\"Loads parameters of an estimator from a file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: str, The path to the file.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def save_params(self, filepath, overwrite):\n",
    "        \"\"\"Saves parameters of an estimator as a file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: str, The path to where \n",
    "                the parameters should be saved.\n",
    "            overwrite: bool, If `False` and `filepath` \n",
    "                already exists, raises an error.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _calc_output(self, X, *args, **kwargs):\n",
    "        \"\"\"Return output of estimator\n",
    "        \n",
    "        Note:\n",
    "            output of estimator is not necessary \n",
    "                prediction of target values.\n",
    "            This value can be values necessary \n",
    "                for transformation, e.g. logit\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class NNTrainMixin(object):\n",
    "    def train(self, train_X, train_y=None, valid_X=None, valid_y=None, num_epochs=100, \n",
    "              valid_freq=1, epoch_bar=True, batch_bar=False, log_freq=1, batch_log_freq=None,\n",
    "              img_log_freq=None, plot_X=None, is_processed=False, \n",
    "              is_memory_intensive=False, overwrite=True, save_file_path=None, *args, **kwargs):\n",
    "        \"\"\"Train neural network model\n",
    "        \n",
    "        Args:\n",
    "            train_X, train_y: list(array-like), this is preprocessed by \n",
    "                self.processor.batch_process\n",
    "            valid_X, valid_y: array-like(optional), if they are feeded, they\n",
    "                will be used as validation set\n",
    "            is_processed: bool, if True, skip preprocessing\n",
    "            args, kwargs: parameters for score function\n",
    "        \"\"\"\n",
    "        train_X, train_y = self._preprocess(train_X, train_y)\n",
    "        valid_X, valid_y = self._preprocess(valid_X, valid_y)\n",
    "        # check if there is already the same name file\n",
    "        self.save_params(self.save_file_path, overwrite)\n",
    "        self.is_trained = True\n",
    "        num_train = len(train_X[0])\n",
    "        indices = np.arange(num_train)\n",
    "        try:\n",
    "            global_batch_step = 0\n",
    "            if epoch_bar:\n",
    "                epoch_iter = trange(num_epochs)\n",
    "            else:\n",
    "                epoch_iter  = range(num_epochs)\n",
    "            for i in epoch_iter:\n",
    "                np.random.shuffle(indices)\n",
    "                self.sess.run(self.update_step_op);\n",
    "                epoch_loss = []\n",
    "                epoch_score = []\n",
    "                batch_loss_list = []\n",
    "                batch_score_list = []\n",
    "                batch_step = 0\n",
    "                total_batch = num_train//self.batch_size + 1\n",
    "                if batch_bar:\n",
    "                    batch_iter = trange(total_batch)\n",
    "                else:\n",
    "                    batch_iter = range(total_batch)\n",
    "                for batch_i in batch_iter:\n",
    "                    batch_step += 1\n",
    "                    global_batch_step += 1\n",
    "                    batch_idx = self._get_batch_idx(batch_i, indices)\n",
    "                    batch_X = [X[batch_idx] for X in train_X]\n",
    "                    if not is_processed:\n",
    "                        batch_X = [self.processor.batch_process(X_i) for X_i in batch_X]\n",
    "                    if train_y is not None:\n",
    "                        batch_y = train_y[batch_idx]\n",
    "                    else:\n",
    "                        batch_y = None\n",
    "                    batch_loss = self._optimize(batch_X, batch_y, num_data=num_train)\n",
    "                    if batch_y is not None:\n",
    "                        batch_score = self.score(batch_X, batch_y, is_training=False, is_processed=True)\n",
    "                        batch_score_list.append(batch_score)\n",
    "                        epoch_score.append(batch_score)\n",
    "                    if batch_log_freq is not None: \n",
    "                        batch_loss_list.append(batch_loss)\n",
    "                        if batch_step % batch_log_freq == 0:\n",
    "                            tag_dict = {\"batch_loss\": np.mean(batch_loss_list)}\n",
    "                            if batch_score_list:\n",
    "                                tag_dict[\"batch_score\"] = np.mean(batch_score_list)\n",
    "                            self._inject_summary(tag_dict, global_batch_step)\n",
    "                            batch_loss_list = []\n",
    "                            batch_score_list = []\n",
    "                    epoch_loss.append(np.mean(batch_loss))\n",
    "                    if is_memory_intensive:\n",
    "                        del batch_X\n",
    "                        del batch_y\n",
    "                        gc.collect()\n",
    "                step = self.global_step.eval(session=self.sess)\n",
    "                if step % log_freq == 0:\n",
    "                    lr_val = self.learning_rate_op.eval(session=self.sess)\n",
    "                    tag_dict = {'loss': np.mean(epoch_loss), \"learning_rate\":lr_val}\n",
    "                    if epoch_score:\n",
    "                        tag_dict[\"score\"] = np.mean(epoch_score)\n",
    "                    self._inject_summary(tag_dict)\n",
    "                # self._epoch_func(X=train_X, y=train_y, is_processed=is_processed)\n",
    "                accuracies = []\n",
    "                if valid_y is not None:\n",
    "                    num_valid = len(valid_y)\n",
    "                    # check accuracy every print_freq epoch\n",
    "                    if step % valid_freq == 0:\n",
    "                        accuracies = []\n",
    "                        logits_list = []\n",
    "                        for batch_i in range(num_valid//self.batch_size + 1):\n",
    "                            batch_idx = self._get_batch_idx(batch_i)\n",
    "                            batch_X = [X[batch_idx] for X in valid_X]\n",
    "                            batch_y = valid_y[batch_idx]\n",
    "                            _score = self.score(batch_X, batch_y, is_training=False,\n",
    "                                                is_processed=True, *args, **kwargs)\n",
    "                            accuracies.append(_score)\n",
    "                        accuracy = np.mean(accuracies)\n",
    "                        print(\"accuracy: \", accuracy)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Save model parameters before finishing training...\")\n",
    "        finally:\n",
    "            self.save_params(save_file_path, overwrite=True)\n",
    "        print(\"finished training\")\n",
    "        \n",
    "    def _optimize(self, batch_X, batch_y, *args, **kwargs):\n",
    "        feed_dict={\n",
    "                self.input: batch_X[0],\n",
    "                self.training: True}\n",
    "        if batch_y is not None:\n",
    "            feed_dict[self.target] = batch_y\n",
    "        _, loss = self.sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def _epoch_func(self, X, y, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def _gen_plot(self, X):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _inject_summary(self, tag_dict, step=None):\n",
    "        if step is None:\n",
    "            step = self.global_step.eval(session=self.sess)\n",
    "        summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dict.keys()], {\n",
    "          self.summary_placeholders[tag]: value for tag, value in tag_dict.items()\n",
    "        })\n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str, step)\n",
    "            \n",
    "    def _get_tag_dict(self, epoch_loss, *args, **kwargs):\n",
    "        lr_val = self.learning_rate_op.eval(session=self.sess)\n",
    "        tag_dict = {'loss': np.mean(epoch_loss), \"learning_rate\":lr_val}\n",
    "        return tag_dict\n",
    "    \n",
    "    def _preprocess(self, X, y):\n",
    "        return [X], y\n",
    "    \n",
    "    def _get_batch_idx(self, idx, indices):\n",
    "        return indices[idx*self.batch_size: (idx+1)*self.batch_size]        \n",
    "\n",
    "class SaveLoadMixin(object):\n",
    "\n",
    "    def load_params(self, file_path):\n",
    "        \"\"\"Loads parameters of an estimator from a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: str, The path to the file.\n",
    "        \"\"\"\n",
    "        self.saver.restore(self.sess, file_path)\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "    def save_params(self, file_path=None, overwrite=True):\n",
    "        \"\"\"Saves parameters of an estimator as a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: str, The path to where the parameters should be saved.\n",
    "            overwrite: bool, If `False` and `file_path` already exists, raises an error.\n",
    "        \"\"\"\n",
    "        if file_path is None:\n",
    "            if not os.path.isdir(\"params\"):\n",
    "                os.mkdir(\"params\")\n",
    "            file_path = \"params/model.ckpt\"\n",
    "        if not overwrite:\n",
    "            _path = \".\".join([file_path, \"meta\"])\n",
    "            if os.path.isfile(_path):\n",
    "                raise NameError(\"%s already exists.\" % file_path)\n",
    "        save_path = self.saver.save(self.sess, file_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "class BaseNN(SaveLoadMixin, NNTrainMixin, BaseEstimator):\n",
    "    \"\"\"Abstract class for neural networks.\n",
    "    \n",
    "    Do not use this class directly. You should use this class with other mixin \n",
    "    class, e.g., NNClassifier and  NNRegressor.     \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model, conf=None,\n",
    "                 sess=None, default_conf=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conf = conf\n",
    "        if sess is None:\n",
    "            sess = tf.Session()\n",
    "        self.sess = sess\n",
    "        # configuration for not specified features\n",
    "        if default_conf is None:\n",
    "            default_conf = nn_conf\n",
    "        self.default_conf = default_conf\n",
    "        self.conf = self._set_conf(conf)\n",
    "        self.model = model(output_dim, self.conf[\"model\"], sess=self.sess, *args, **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # configure for learning schedule\n",
    "        self.learning_rate = self.conf[\"learning_rate\"]\n",
    "        self.learning_rate_decay_step = self.conf[\"learning_rate_decay_step\"]\n",
    "        self.learning_rate_decay = self.conf[\"learning_rate_decay\"]\n",
    "        self.learning_rate_minimum = self.conf[\"learning_rate_minimum\"]\n",
    "        self.batch_size = self.conf[\"batch_size\"]\n",
    "        # directory for tensorboard\n",
    "        self.model_dir = self.conf[\"model_dir\"]\n",
    "        print(\"Building tensorflow graph...\")\n",
    "        st = time()\n",
    "        with self.sess.as_default():\n",
    "            # build tensorflow graph\n",
    "            self.global_step = tf.Variable(0, name=\"epoch\", trainable=False)\n",
    "            self.update_step_op = self._get_update_step_op()\n",
    "            self._build_graph()\n",
    "            self.saver = tf.train.Saver()\n",
    "        print(\"Finished building tensorflow graph, spent time:\", time() - st)\n",
    "        self.save_file_path = self.conf[\"save_file_path\"]\n",
    "        load_file_path = self.conf[\"load_file_path\"]\n",
    "        if load_file_path is not None:\n",
    "            self.load_params(load_file_path)\n",
    "            self.is_trained = True\n",
    "\n",
    "    def _build_graph(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _get_learning_rate(self):\n",
    "        learning_rate_op = tf.maximum(self.learning_rate_minimum,\n",
    "              tf.train.exponential_decay(\n",
    "              self.learning_rate,\n",
    "              self.global_step,\n",
    "              self.learning_rate_decay_step,\n",
    "              self.learning_rate_decay,\n",
    "              staircase=False))\n",
    "        return learning_rate_op\n",
    "\n",
    "    def _calc_output(self, X, is_training=is_training, is_processed=is_processed):\n",
    "        \"\"\"Return model output\n",
    "        \n",
    "        Returns:\n",
    "            array-like\n",
    "        \"\"\"\n",
    "        # Check the shape\n",
    "        if isinstance(self.input_dim, int):\n",
    "            dim_len = 1 + 1\n",
    "        else:\n",
    "            dim_len = len(self.input_dim) + 1\n",
    "        if len(np.array(X).shape) > dim_len:\n",
    "            X = X[0]\n",
    "        if not is_processed:\n",
    "            X = self.processor.batch_process(X)\n",
    "        num_data = len(X)\n",
    "        output_list = []\n",
    "        for batch_i in range(num_data // self.batch_size + 1):\n",
    "            batch_X = X[self.batch_size * batch_i : self.batch_size * (batch_i+1)]\n",
    "            feed_dict = {self.training: is_training, self.input: batch_X}\n",
    "            output_list.extend(self.sess.run(self.output, feed_dict=feed_dict))\n",
    "        return np.array(output_list)\n",
    "    \n",
    "    def _set_conf(self, conf):\n",
    "        conf = deepcopy(conf)\n",
    "        for key in self.default_conf.keys():\n",
    "            if key not in conf:\n",
    "                conf[key] = self.default_conf[key]\n",
    "        return conf\n",
    "    \n",
    "    def _get_update_step_op(self):\n",
    "        return tf.assign(self.global_step, self.global_step + 1)\n",
    "    \n",
    "    def _build_summaries(self):\n",
    "        self.writer = tf.summary.FileWriter(self.model_dir, self.sess.graph)\n",
    "        self.summary_placeholders = {}\n",
    "        self.summary_ops = {}\n",
    "        scalar_summary_tags = ['loss', 'score', 'learning_rate', 'batch_loss', 'batch_score']\n",
    "        for tag in scalar_summary_tags:\n",
    "            self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag.replace(' ', '_'))\n",
    "            self.summary_ops[tag] =\\\n",
    "                tf.summary.scalar(\"%s\" % tag, self.summary_placeholders[tag])\n",
    "                \n",
    "class MultiNN(BaseNN):\n",
    "    def __init__(self, input_dim, output_dim, input_model, output_model, conf=None,\n",
    "                 sess=None, default_conf=None, *args, **kwargs):\n",
    "        super().__init__(input_dim, output_dim, output_model, conf,\n",
    "                 sess, default_conf, *args, **kwargs)\n",
    "        \n",
    "    def _calc_output(self, X, is_training=is_training, is_processed=is_processed):\n",
    "        \"\"\"Return model output\n",
    "        \n",
    "        Returns:\n",
    "            list(array-like)\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise NotTrainedError(\"Train model by 'train' beforehand\")\n",
    "        if not is_processed:\n",
    "            X = [self.processor.batch_process(_X) for _X in X]\n",
    "        num_data = len(X[0])\n",
    "        output_list = []\n",
    "        for batch_i in range(num_data // self.batch_size + 1):\n",
    "            feed_dict = {self.training: is_training}\n",
    "            for _input, _X in zip(self.input, X):\n",
    "                batch_X = _X[self.batch_size * batch_i : self.batch_size * (batch_i+1)]\n",
    "                feed_dict[_input] = batch_X\n",
    "            output_list.extend(self.sess.run(self.output, feed_dict=feed_dict))\n",
    "        return np.array(output_list)\n",
    "    \n",
    "    def _optimize(self, batch_X, batch_y, *args, **kwargs):\n",
    "        feed_dict={self.training: True}\n",
    "        if batch_y is not None:\n",
    "            feed_dict[self.target] = batch_y\n",
    "        for input_, X in zip(self.input, batch_X):\n",
    "            feed_dict[input_] = X\n",
    "        _, loss = self.sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def _preprocess(self, X, y):\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, conv2d\n",
    "from tensorflow.contrib.layers import conv2d_transpose, flatten\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tensorflow.contrib.rnn import MultiRNNCell\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.rnn import *\n",
    "\n",
    "class BaseModel(object):\n",
    "    def __init__(self, model_params, scope_name, *args, **kwargs):\n",
    "        self.model_params = model_params\n",
    "        self.scope_name = scope_name\n",
    "        self.reuse = False\n",
    "        \n",
    "    def __call__(self, x, training=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_input(self):\n",
    "        return self.input\n",
    "    \n",
    "    def get_output(self):\n",
    "        return self.output\n",
    "\n",
    "def get_shape(input_shape, is_batch=True, is_sequence=False, maxlen=None):\n",
    "    \"\"\"Get shape of batch input for model\n",
    "    \n",
    "    Args:\n",
    "        input_shape: int or tuple, shape of input \n",
    "        is_batch: bool, if True, we add another batch\n",
    "            dimension, None\n",
    "    \n",
    "    Returns:\n",
    "        tuple: if is_batch is False, shape=(None, *input_shape)\n",
    "    \"\"\"\n",
    "    if isinstance(input_shape, int):\n",
    "        input_shape = (input_shape,)\n",
    "    elif isinstance(input_shape, list):\n",
    "        input_shape = tuple(input_shape)\n",
    "    if is_sequence:\n",
    "        input_shape = (maxlen,) + input_shape\n",
    "    if is_batch:\n",
    "        input_shape = (None,) + input_shape\n",
    "    return input_shape\n",
    "\n",
    "\n",
    "def get_length(sequence):\n",
    "    \"\"\"Get length of each instance in batch\n",
    "    \n",
    "    Args:\n",
    "        sequence: tensor, shape = (batch_size, length)\n",
    "            or shape = (batch_size, length) + shape\n",
    "            \n",
    "    Returns:\n",
    "        tensor: shape = (None,), length of each instance\n",
    "    \"\"\"\n",
    "    shape = sequence.get_shape().as_list()\n",
    "    if len(shape) < 2:\n",
    "        used = tf.sign(tf.abs(sequence))\n",
    "    else:\n",
    "        reduction_indices = list(range(2, len(shape)))\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(sequence),\n",
    "                       reduction_indices=reduction_indices))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "\n",
    "        \n",
    "class FeedForward(BaseModel):\n",
    "    def __init__(self, model_params, scope_name, *args, **kwargs):\n",
    "        super().__init__(model_params, scope_name, *args, **kwargs)\n",
    "    \n",
    "    def __call__(self, x, training=True):\n",
    "        with tf.variable_scope(self.scope_name, reuse=self.reuse):\n",
    "            for i, params in enumerate(self.model_params):\n",
    "                with tf.variable_scope('layer_' + str(i)):\n",
    "                    if \"is_flatten\" in params and params[\"is_flatten\"]:\n",
    "                        x = flatten(x)\n",
    "                    if \"drop_rate\" in params:\n",
    "                        x = tf.layers.dropout(x, rate=params[\"drop_rate\"], training=training)\n",
    "                    # demtermine which layer to use\n",
    "                    if params[\"name\"] == \"dense\":\n",
    "                        x = fully_connected(x, params[\"num_hidden\"], activation_fn=None)\n",
    "                    elif params[\"name\"] == \"conv2d\":\n",
    "                        x =  conv2d(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                          params[\"stride\"], params[\"padding\"], activation_fn=None)\n",
    "                    elif params[\"name\"] == \"deconv2d\":\n",
    "                        x =  conv2d_transpose(x, params[\"num_filter\"], params[\"kernel_size\"],\n",
    "                                          params[\"stride\"], params[\"padding\"], activation_fn=None)\n",
    "                    elif params[\"name\"] == \"reshape\":\n",
    "                        x = tf.reshape(x, (-1,) + params[\"reshape_size\"])\n",
    "                    elif params[\"name\"] == \"pooling\":\n",
    "                        del params[\"name\"]\n",
    "                        x = tf.nn.pool(x, **params)\n",
    "                    else:\n",
    "                        raise NotImplementedError(\"No implementation for 'name'={}\".format(params[\"name\"]))         \n",
    "                    if \"is_batch\" in params and params[\"is_batch\"]:\n",
    "                        x = tf.layers.batch_normalization(x, training=training, momentum=0.9)\n",
    "                    if \"activation\" in params:\n",
    "                        x = params[\"activation\"](x)\n",
    "            if self.reuse is False:\n",
    "                self.global_scope_name = tf.get_variable_scope().name\n",
    "                self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.global_scope_name)\n",
    "        self.reuse = True\n",
    "        return x\n",
    "\n",
    "\n",
    "lnlstm_cell_params = {\"forget_bias\": 1.0, \"activation\": tf.nn.tanh,\n",
    "                      \"layer_norm\": True, \"norm_gain\": 1.0, \"norm_shift\": 0.0,\n",
    "                      \"drop_rate\": 0.0, \"dropout_seed\": None, \n",
    "                      \"reuse\": None}\n",
    "\n",
    "maxlen=None\n",
    "\n",
    "def get_cell(params):\n",
    "    _params = deepcopy(params)\n",
    "    if _params[\"name\"] == \"lnlstm\":\n",
    "        del _params[\"name\"]\n",
    "        return LNLSTMCell(**_params)\n",
    "    elif _params[\"name\"] == \"lstm\":\n",
    "        del _params[\"name\"]\n",
    "        return LSTMCell(**_params)\n",
    "    elif _params[\"name\"] == \"gru\":\n",
    "        del _params[\"name\"]\n",
    "        return GRUCell(**_params)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "class LNLSTMCell(rnn.RNNCell):\n",
    "    def __init__(self, num_units, forget_bias=lnlstm_cell_params[\"forget_bias\"],\n",
    "                 activation=lnlstm_cell_params[\"activation\"],\n",
    "                 layer_norm=lnlstm_cell_params[\"layer_norm\"], \n",
    "                 norm_gain=lnlstm_cell_params[\"norm_gain\"], \n",
    "                 norm_shift=lnlstm_cell_params[\"norm_shift\"],\n",
    "                 drop_rate=lnlstm_cell_params[\"drop_rate\"], \n",
    "                 dropout_seed=lnlstm_cell_params[\"dropout_seed\"], \n",
    "                 reuse=lnlstm_cell_params[\"reuse\"]):\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation\n",
    "        self._forget_bias = forget_bias\n",
    "        self._layer_norm = layer_norm\n",
    "        self._keep_prob = 1 - drop_rate\n",
    "        self._seed = dropout_seed\n",
    "        self._g = norm_gain\n",
    "        self._b = norm_shift\n",
    "        self._reuse = reuse\n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return rnn.LSTMStateTuple(self._num_units, self._num_units)\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "    \n",
    "    def _norm(self, input_, scope):\n",
    "        # Keep list format\n",
    "        shape = input_.get_shape()[-1:]\n",
    "        gamma_init = tf.constant_initializer(self._g)\n",
    "        beta_init = tf.constant_initializer(self._b)\n",
    "        with tf.variable_scope(scope):\n",
    "            tf.get_variable(\"gamma\", shape=shape,\n",
    "                            initializer=gamma_init, dtype=tf.float32)\n",
    "            tf.get_variable(\"beta\", shape=shape,\n",
    "                            initializer=beta_init, dtype=tf.float32)\n",
    "        normalized = tf.contrib.layers.layer_norm(input_, reuse=True,\n",
    "                                                  scope=scope)\n",
    "        return normalized\n",
    "    \n",
    "    def _linear(self, x):\n",
    "        out_size = 4 * self._num_units\n",
    "        proj_size = x.get_shape()[-1]\n",
    "        weights = tf.get_variable(\"kernel\", [proj_size, out_size],\n",
    "                                  dtype=tf.float32)\n",
    "        out = tf.matmul(x, weights)\n",
    "        if not self._layer_norm:\n",
    "            bias = tf.get_variable(\"bias\", [out_size])\n",
    "            out = tf.nn.bias_add(out, bias)\n",
    "        return out\n",
    "    \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        c, h = state\n",
    "        xh = tf.concat([inputs, h], 1)\n",
    "        out_size = 4 * self._num_units\n",
    "        xh = self._linear(xh)\n",
    "        \n",
    "        i, j, f, o = tf.split(value=xh, num_or_size_splits=4, axis=1)\n",
    "        if self._layer_norm:\n",
    "            i = self._norm(i, \"input\")\n",
    "            j = self._norm(j, \"transform\")\n",
    "            f = self._norm(f, \"forget\")\n",
    "            o = self._norm(o, \"output\")\n",
    "        \n",
    "        g = self._activation(j)\n",
    "        if (not isinstance(self._keep_prob, float)) or self._keep_prob < 1:\n",
    "            g = tf.nn.dropout(g, self._keep_prob, seed=self._seed)\n",
    "            \n",
    "        new_c = (c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) * g)\n",
    "        \n",
    "        if self._layer_norm:\n",
    "            new_c = self._norm(new_c, \"state\")\n",
    "        new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "        new_state = rnn.LSTMStateTuple(new_c, new_h)\n",
    "        return new_h, new_state\n",
    "\n",
    "class BaseRNN(BaseModel):\n",
    "    def __init__(self, model_params, scope_name, full_output=False, *args, **kwargs):\n",
    "        super().__init__(model_params, scope_name, *args, **kwargs)\n",
    "        self.full_output = full_output\n",
    "    \n",
    "    def __call__(self, x, training=True):\n",
    "        with tf.variable_scope(self.scope_name, reuse=self.reuse):\n",
    "            if isinstance(self.model_params, list) or isinstance(self.model_params, tuple):\n",
    "                cells = []\n",
    "                for i, params in enumerate(self.model_params):\n",
    "                    with tf.variable_scope('cell_' + str(i)):\n",
    "                        cells.append(get_cell(params))\n",
    "                cell = MultiRNNCell(cells)\n",
    "            else:\n",
    "                cell = get_cell(self.model_params)\n",
    "            outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32, sequence_length=get_length(x))\n",
    "            if self.reuse is False:\n",
    "                self.global_scope_name = tf.get_variable_scope().name\n",
    "                self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.global_scope_name)\n",
    "        self.reuse = True\n",
    "        if self.full_output:\n",
    "            return outputs\n",
    "        else:\n",
    "            return state.h\n",
    "        \n",
    "class RNNModel(BaseRNN):\n",
    "    def __init__(self, output_dim=None, model_params=None, scope_name=None, *args, **kwargs):\n",
    "        if model_params is None:\n",
    "            model_params = rnn_conf[\"model\"]\n",
    "        if scope_name is None:\n",
    "            scope_name = \"rnn\"\n",
    "        self.rnn_model = BaseRNN(model_params, scope_name, *args, **kwargs)\n",
    "        if output_dim is not None:\n",
    "            self.ff_model = FeedForward([{\"name\": \"dense\", \"num_hidden\": output_dim},], scope_name)\n",
    "        else:\n",
    "            self.ff_model = None\n",
    "        super().__init__(model_params, scope_name, *args, **kwargs)   \n",
    "        \n",
    "    def __call__(self, x, training=True):\n",
    "        x = self.rnn_model(x, training)\n",
    "        if self.ff_model is not None:\n",
    "            x = self.ff_model(x, training)\n",
    "        return x\n",
    "    \n",
    "class MLPModel(FeedForward):\n",
    "    def __init__(self, output_dim=None, model_params=None, scope_name=None, *args, **kwargs):\n",
    "        if model_params is None:\n",
    "            model_params = mlp_conf[\"model\"]\n",
    "        if scope_name is None:\n",
    "            scope_name = \"mlp\"\n",
    "        if output_dim  is not None:\n",
    "            model_params.append({\"name\": \"dense\", \"num_hidden\": output_dim})\n",
    "        super().__init__(model_params, scope_name, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y, y_pred, *args, **kwargs):\n",
    "    y = np.squeeze(y)\n",
    "    y_pred = np.squeeze(y_pred)\n",
    "    return np.mean(np.equal(y, y_pred))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    shape = x.shape\n",
    "    z = np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    z = np.tile(z, (1, shape[1]))\n",
    "    return np.exp(x) / z\n",
    "\n",
    "\n",
    "class ClassifierMixin(object):\n",
    "    \"\"\"Mixin class for all classifiers\n",
    "    \n",
    "    Do not use this mixin class directly.\n",
    "    \"\"\"\n",
    "    _estimator_type = \"classifier\"\n",
    "\n",
    "    def score(self, X, y, score_func=accuracy_score, *args, **kwargs):\n",
    "        \"\"\"Returns the score on the input data and labels.\n",
    "        \n",
    "        If score_func is not specified, we use accuracy function.\n",
    "        \n",
    "        Args:\n",
    "            X : array-like\n",
    "            y : array-like, shape = (n_samples) True labels for X.\n",
    "            score_func: optional, metric function\n",
    "            is_training: bool, output mode\n",
    "                this will be used when using certain regularizers like\n",
    "                dropout\n",
    "            args, kwargs: additional parameters for self.predict\n",
    "        \n",
    "        Returns:\n",
    "            float\n",
    "        \"\"\"\n",
    "        return score_func(y, self.predict(X, *args, **kwargs))\n",
    "    \n",
    "    def predict_conf(self, X, *args, **kwargs):\n",
    "        \"\"\"Return confidence of predictions\n",
    "        \n",
    "        Binary or multi classification will be specified by \n",
    "        self.output_dim. If you want to use output of an estimator\n",
    "        as confidence directly, you should set 'self.is_logit=False'.\n",
    "        \n",
    "        Args:\n",
    "            X : array-like\n",
    "            is_training: bool, output mode\n",
    "                this will be used when using certain regularizers like \n",
    "                dropout\n",
    "            args, kwargs: additional parameters for self._calc_output\n",
    "        \n",
    "        Returns:\n",
    "            array_like, shape = (n_samples, self.output_dim)\n",
    "        \"\"\"\n",
    "        if self.is_logit:\n",
    "            logit = self._calc_output(X, *args, **kwargs)\n",
    "            if self.output_dim == 1:\n",
    "                prediction = sigmoid(logit)\n",
    "            else:\n",
    "                prediction = softmax(logit)\n",
    "        else:\n",
    "            prediction = self._calc_output(X, *args, **kwargs)\n",
    "        # Keep shape = (n_samples, self.output_dim)\n",
    "        if len(prediction.shape) == 1:\n",
    "            prediction = prediction[:, np.newaxis]\n",
    "        return prediction\n",
    "    \n",
    "    def predict(self, X, threshold=0.5, *args, **kwargs):\n",
    "        \"\"\"Return predictions of an estimator\n",
    "        \n",
    "        Binary or multi classification will be specified by\n",
    "        self.output_dim. If labels are binary, we assigne 0 to negative \n",
    "        and 1 to positive isntances. If you want to use output of\n",
    "        an estimator as confidence directly, you should set \n",
    "        'self.is_logit=False'.\n",
    "        \n",
    "        Args:\n",
    "            X : array-like\n",
    "            is_training: bool, output mode\n",
    "                this will be used when using certain regularizers like \n",
    "                dropout\n",
    "            threshold: float, this will be used only for binary \n",
    "                classification\n",
    "            args, kwargs: additional parameters for self._calc_output\n",
    "        \n",
    "        Returns:\n",
    "            array_like, shape = (n_samples, self.output_dim)\n",
    "        \"\"\"\n",
    "        conf = self.predict_conf(X, *args, **kwargs)\n",
    "        output_shape = conf.shape\n",
    "        if output_shape[-1] == 1:\n",
    "            prediction = np.array([1 if x[0] > threshold else 0 for x in conf])\n",
    "        else:\n",
    "            prediction = np.argmax(conf, axis=1)\n",
    "        return prediction\n",
    "\n",
    "    \n",
    "nn_is_logit = True\n",
    "\n",
    "class NNClassifier(BaseNN, ClassifierMixin):\n",
    "    \"\"\"Classifier based on neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, model, conf,\n",
    "                 is_sequence, is_logit=nn_is_logit, *args, **kwargs):\n",
    "        \"\"\"Initialize classifier with nerual network model\n",
    "        \n",
    "        Args:\n",
    "            model: class, neural network class\n",
    "            is_logit: bool(optional), if False, output of an estimator\n",
    "                as prediction directly\n",
    "            args, kwargs: parameters for parents class\n",
    "        \"\"\"\n",
    "        self.is_logit = is_logit\n",
    "        self.is_sequence = is_sequence\n",
    "        super().__init__(input_dim, output_dim, model=model,\n",
    "                         conf=conf, *args, **kwargs)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build tensorflow graph\n",
    "        \n",
    "        Note:\n",
    "            You build graphs for output and input, which will be used \n",
    "            for training and prediction.\n",
    "        \"\"\"\n",
    "        _input_dim = get_shape(self.input_dim, is_sequence=self.is_sequence)\n",
    "        self.epoch = tf.Variable(0, name=\"epoch\", trainable=False)\n",
    "        self.input = tf.placeholder(tf.float32, shape=_input_dim)\n",
    "        self.target = tf.placeholder(tf.int32, shape=(None,), name=\"target\")\n",
    "        self.training = tf.placeholder(tf.bool, (), name=\"training\")\n",
    "        self.output = self.model(self.input, self.training)\n",
    "        \n",
    "        # build optimizer\n",
    "        if self.is_logit:\n",
    "            if self.output_dim==1:\n",
    "                _output = tf.squeeze(self.output)\n",
    "                _loss =\\\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        labels=tf.cast(self.target, tf.float32),\n",
    "                        logits=_output)\n",
    "            else:\n",
    "                _target = tf.one_hot(self.target, self.output_dim)\n",
    "                _loss =\\\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=tf.cast(_target, tf.float32),\n",
    "                        logits=self.output)\n",
    "            self.loss = tf.reduce_mean(_loss)\n",
    "        else:\n",
    "            raise NotImplementedError(\"We have not implemeted non logit output model\")\n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        self.train_step =\\\n",
    "            tf.train.AdamOptimizer(self.learning_rate_op).minimize(self.loss)\n",
    "        # Build tensorboad graph\n",
    "        with tf.name_scope(\"summary\"):\n",
    "            self._build_summaries()\n",
    "        \n",
    "        # initialize graph\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "class MultiNNClassifier(MultiNN, ClassifierMixin):\n",
    "    \"\"\"Classifier based on neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, input_model, output_model, conf,\n",
    "                 is_sequence, is_logit=nn_is_logit, *args, **kwargs):\n",
    "        \"\"\"Initialize classifier with nerual network model\n",
    "        \n",
    "        Args:\n",
    "            model: class, neural network class\n",
    "            is_logit: bool(optional), if False, output of an estimator\n",
    "                as prediction directly\n",
    "            args, kwargs: parameters for parents class\n",
    "        \"\"\"\n",
    "        self.is_logit = is_logit\n",
    "        self.is_sequence = is_sequence\n",
    "        self.input_model = []\n",
    "        for i, model in enumerate(input_model):\n",
    "            self.input_model.append(model(None, conf[\"input_model\"][i], \"input_model_%d\" % i))\n",
    "        self.output_model = output_model(output_dim, conf[\"model\"], \"output_model\")\n",
    "        super().__init__(input_dim, output_dim, input_model=input_model,\n",
    "                         output_model=output_model, conf=conf, *args, **kwargs)\n",
    "        \n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build tensorflow graph\n",
    "        \n",
    "        Note:\n",
    "            You build graphs for output and input, which will be used \n",
    "            for training and prediction.\n",
    "        \"\"\"\n",
    "        self.epoch = tf.Variable(0, name=\"epoch\", trainable=False)\n",
    "        self.input = []\n",
    "        for _input_dim in self.input_dim:\n",
    "            _input_dim = get_shape(_input_dim, is_sequence=self.is_sequence)\n",
    "            self.input.append(tf.placeholder(tf.float32, shape=_input_dim))\n",
    "        self.target = tf.placeholder(tf.int32, shape=(None,), name=\"target\")\n",
    "        self.training = tf.placeholder(tf.bool, (), name=\"training\")\n",
    "        outputs = []\n",
    "        for _input, model in zip(self.input, self.input_model):\n",
    "            outputs.append(model(_input, self.training))\n",
    "        output_input = tf.concat(outputs, axis=-1)\n",
    "        self.output = self.output_model(output_input)\n",
    "        \n",
    "        # build optimizer\n",
    "        if self.is_logit:\n",
    "            if self.output_dim==1:\n",
    "                _output = tf.squeeze(self.output)\n",
    "                _loss =\\\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                        labels=tf.cast(self.target, tf.float32),\n",
    "                        logits=_output)\n",
    "            else:\n",
    "                _target = tf.one_hot(self.target, self.output_dim)\n",
    "                _loss =\\\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=tf.cast(_target, tf.float32),\n",
    "                        logits=self.output)\n",
    "            self.loss = tf.reduce_mean(_loss)\n",
    "        else:\n",
    "            raise NotImplementedError(\"We have not implemeted non logit output model\")\n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        self.train_step =\\\n",
    "            tf.train.AdamOptimizer(self.learning_rate_op).minimize(self.loss)\n",
    "        # Build tensorboad graph\n",
    "        with tf.name_scope(\"summary\"):\n",
    "            self._build_summaries()\n",
    "        \n",
    "        # initialize graph\n",
    "        self.sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_conf = {\n",
    "    \"model\": [\n",
    "        {\"name\": \"dense\", \"num_hidden\": 100, \"is_batch\":True, 'activation': tf.tanh},\n",
    "        {\"name\": \"dense\", \"num_hidden\": 100, \"drop_rate\": 0.5, \"is_batch\":True, 'activation': tf.tanh}, \n",
    "        {\"name\": \"dense\", \"num_hidden\": 10, \"drop_rate\": 0.5, \"is_batch\":True, 'activation': tf.tanh}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "rnn_conf = {\"model\":{\"name\":\"lnlstm\", \"num_units\":100}}\n",
    "\n",
    "lnlstm_cell_params = {\"forget_bias\": 1.0, \"activation\": tf.nn.tanh,\n",
    "                      \"layer_norm\": True, \"norm_gain\": 1.0, \"norm_shift\": 0.0,\n",
    "                      \"drop_rate\": 0.0, \"dropout_seed\": None, \n",
    "                      \"reuse\": None}\n",
    "\n",
    "maxlen=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tensorflow graph...\n",
      "Finished building tensorflow graph, spent time: 24.54978585243225\n",
      "INFO:tensorflow:Restoring parameters from ./lstm_params/model.ckpt\n",
      "Model restored.\n",
      "CPU times: user 20.7 s, sys: 3.47 s, total: 24.1 s\n",
      "Wall time: 25.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "conf = {\"model\":[{\"name\": \"dense\", \"num_hidden\": 10, \"is_batch\": True, \"activation\": tf.nn.relu}],\n",
    "        \"input_model\":[{\"name\":\"lnlstm\", \"num_units\":256, \"activation\":tf.nn.tanh,\n",
    "                         \"layer_norm\":True, \"drop_rate\":0.0},\n",
    "                       {\"name\":\"lnlstm\", \"num_units\":256, \"activation\":tf.nn.tanh,\n",
    "                         \"layer_norm\":True, \"drop_rate\":0.0},],\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"learning_rate_decay_step\": 10,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_minimum\": 1e-4,\n",
    "        \"batch_size\":128,\n",
    "        \"load_file_path\": \"./lstm_params/model.ckpt\"\n",
    "       }\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "input_model = [RNNModel, RNNModel]\n",
    "output_model = MLPModel\n",
    "model = MultiNNClassifier([300, 300], 1, input_model, output_model, conf, processor=processor, is_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65194649]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_conf([[X[0][0],], [X[1][0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market in india?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market in india?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
