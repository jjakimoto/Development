{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from  sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.827s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "num_data = 2000\n",
    "data_samples = dataset.data[:num_data]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.265s.\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=50000,\n",
    "                                stop_words='english',\n",
    "                                analyzer=\"word\")\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.max(tf.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.021s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lda = LatentDirichletAllocation(n_topics=10, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(tf[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'doc_topic_prior': None,\n",
       " 'evaluate_every': -1,\n",
       " 'learning_decay': 0.7,\n",
       " 'learning_method': 'online',\n",
       " 'learning_offset': 50.0,\n",
       " 'max_doc_update_iter': 100,\n",
       " 'max_iter': 5,\n",
       " 'mean_change_tol': 0.001,\n",
       " 'n_jobs': 1,\n",
       " 'n_topics': 10,\n",
       " 'perp_tol': 0.1,\n",
       " 'random_state': 0,\n",
       " 'topic_word_prior': None,\n",
       " 'total_samples': 1000000.0,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class TopicModeling(LatentDirichletAllocation):\n",
    "    def __init__(self, num_topics, num_latent=None, *args, **kwargs):\n",
    "        if num_latent is None:\n",
    "            num_latent = num_topics\n",
    "        self.num_topics = num_topics\n",
    "        super().__init__(n_topics=num_latent, *args, **kwargs)\n",
    "        self.label_model = LinearRegression(fit_intercept=False)\n",
    "        self.vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=50000,\n",
    "                                stop_words='english',\n",
    "                                analyzer=\"word\")\n",
    "        \n",
    "    def fit_label(self, X, y):\n",
    "        X = self.transform(X)\n",
    "        y = self._get_onehot(y)\n",
    "        self.label_model.fit(X, y)\n",
    "        \n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        X = self.vectorizer.fit_transform(X, *args, **kwargs)\n",
    "        super().fit(X)\n",
    "        \n",
    "    def transform(self, X, *args, **kwargs):\n",
    "        X = self.vectorizer.transform(X)\n",
    "        return super().transform(X, *args, **kwargs)\n",
    "    \n",
    "    def fit_transform(self, X, *args, **kwargs):\n",
    "        self.fit(X, *args, **kwargs)\n",
    "        return self.transform(X, *args, **kwargs)\n",
    "        \n",
    "    def predict_topic(self, X):\n",
    "        X = self.transform(X)\n",
    "        y = self.label_model.predict(X)\n",
    "        return (y.T / np.sum(y, axis=1)).T\n",
    "        \n",
    "    def _get_onehot(self, y):\n",
    "        return np.eye(self.num_topics)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomoaki/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model = TopicModeling(10)\n",
    "model.fit(data_samples)\n",
    "model.fit_label(data_samples[:10], np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -1.62897468e-11,  -7.46222628e-11, ...,\n",
       "         -3.58103304e-13,  -2.07112051e-13,  -3.52361201e-11],\n",
       "       [ -7.35034256e-12,   1.00000000e+00,  -6.39864273e-11, ...,\n",
       "         -8.97837360e-13,  -6.31023012e-14,   2.74937711e-11],\n",
       "       [  3.60676272e-11,   1.28103111e-12,   1.00000000e+00, ...,\n",
       "         -3.65146021e-12,   9.45245076e-13,  -9.62640993e-11],\n",
       "       ..., \n",
       "       [  8.97256173e+05,  -2.13116826e+05,  -1.24604573e+06, ...,\n",
       "         -1.43554633e+05,   1.61083721e+04,  -7.08308609e+05],\n",
       "       [ -2.87794768e-01,  -9.84626770e-02,   5.14627146e-01, ...,\n",
       "          1.22039714e+00,  -6.67368569e-02,   5.83717091e-01],\n",
       "       [  1.45443262e+00,   7.76819021e-01,   1.10297278e-01, ...,\n",
       "          5.23250733e-01,  -5.87202587e-02,  -5.06815838e+00]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_topic(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
