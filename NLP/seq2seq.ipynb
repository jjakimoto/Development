{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = df.dropna()\n",
    "X1 = df[\"question1\"].values\n",
    "X2 = df[\"question2\"].values\n",
    "y = df[\"is_duplicate\"].values\n",
    "X= [X1, X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch(inputs, max_sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs:\n",
    "            list of sentences (integer lists)\n",
    "        max_sequence_length:\n",
    "            integer specifying how large should `max_time` dimension be.\n",
    "            If None, maximum sequence length would be used\n",
    "    \n",
    "    Outputs:\n",
    "        inputs_time_major:\n",
    "            input sentences transformed into time-major matrix \n",
    "            (shape [max_time, batch_size]) padded with 0s\n",
    "        sequence_lengths:\n",
    "            batch-sized list of integers specifying amount of active \n",
    "            time steps in each input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    # [batch_size, max_time] -> [max_time, batch_size]\n",
    "    # inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_batch_major, sequence_lengths\n",
    "\n",
    "\n",
    "def random_sequences(length_from, length_to,\n",
    "                     vocab_lower, vocab_upper,\n",
    "                     batch_size):\n",
    "    \"\"\" Generates batches of random integer sequences,\n",
    "        sequence length in [length_from, length_to],\n",
    "        vocabulary in [vocab_lower, vocab_upper]\n",
    "    \"\"\"\n",
    "    if length_from > length_to:\n",
    "            raise ValueError('length_from > length_to')\n",
    "\n",
    "    def random_length():\n",
    "        if length_from == length_to:\n",
    "            return length_from\n",
    "        return np.random.randint(length_from, length_to + 1)\n",
    "    \n",
    "    while True:\n",
    "        yield [\n",
    "            np.random.randint(low=vocab_lower,\n",
    "                              high=vocab_upper,\n",
    "                              size=random_length()).tolist()\n",
    "            for _ in range(batch_size)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from hedgeable_ai.models.nn import BaseModel, get_shape, get_length\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from hedgeable_ai.models.nn.params import nn_is_logit\n",
    "from hedgeable_ai.models.nn import BaseNN, get_shape\n",
    "\n",
    "from hedgeable_ai.models.nn.rnn import get_cell\n",
    "\n",
    "\n",
    "class DialogueAgent(BaseNN):\n",
    "    def __init__(self, processor, conf=None, *args, **kwargs):\n",
    "        self.emb_size = 300\n",
    "        # add padding index 0 and index 1 for <eos>\n",
    "        self.vocab_size = processor.vocab_size + 2\n",
    "        super().__init__(processor=processor, conf=conf, *args, **kwargs)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build tensorflow graph\n",
    "        \n",
    "        Note:\n",
    "            You build graphs for output and input, which will be used \n",
    "            for training and prediction.\n",
    "        \"\"\"\n",
    "        self.encoder_input = tf.placeholder(tf.int32, shape=(None, None), name=\"encoder_input\")\n",
    "        self.encoder_input_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_input_length')\n",
    "        self.decoder_target = tf.placeholder(tf.int32, shape=(None, None), name=\"decoder_target\")\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_cell = get_cell(self.conf[\"model_encoder\"])\n",
    "        embeddings = tf.Variable(tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0, dtype=tf.float32))\n",
    "        encoder_input_embedded = tf.nn.embedding_lookup(embeddings, self.encoder_input)\n",
    "        encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "            encoder_cell, encoder_input_embedded, dtype=tf.float32, \n",
    "            time_major=False, scope=\"encoder\")\n",
    "        # We do not need encoder outputs\n",
    "        del encoder_outputs\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_cell = get_cell(self.conf[\"model_decoder\"])\n",
    "        batch_size, encoder_max_time = tf.unstack(tf.shape(self.encoder_input))\n",
    "        decoder_output_size = self._get_output_size(self.conf[\"model_decoder\"])\n",
    "        # We use a decoder length 2 words longer than encoder\n",
    "        decoder_length = self.encoder_input_length + 3\n",
    "        W = tf.Variable(tf.random_uniform([decoder_output_size, self.vocab_size], -1, 1), dtype=tf.float32)\n",
    "        b = tf.Variable(tf.zeros([self.vocab_size]), dtype=tf.float32)\n",
    "        # Prepare for padding and EOS\n",
    "        eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "        pad_time_slice = tf.zeros([batch_size],  dtype=tf.int32, name='PAD')\n",
    "        eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "        pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "        \n",
    "        def loop_fn_initial():\n",
    "            initial_elements_finished = (0 >= decoder_length)\n",
    "            initial_input = eos_step_embedded\n",
    "            initial_cell_state = encoder_final_state\n",
    "            initial_cell_output  = None\n",
    "            initial_loop_state = None\n",
    "            return (initial_elements_finished,\n",
    "                initial_input,\n",
    "                initial_cell_state,\n",
    "                initial_cell_output,\n",
    "                initial_loop_state)\n",
    "        \n",
    "        def loop_fn_transition(time, previous_output,  previous_state, previous_loop_state):\n",
    "            \n",
    "            def get_next_input():\n",
    "                output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "                prediction  = tf.argmax(output_logits, axis=1)\n",
    "                next_input = tf.nn.embedding_lookup(embeddings,  prediction)\n",
    "                return next_input\n",
    "            \n",
    "            elements_finished = (time >= decoder_length)\n",
    "            finished = tf.reduce_all(elements_finished)\n",
    "            input_ = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "            state  = previous_state\n",
    "            output = previous_output\n",
    "            loop_state = None\n",
    "            return (elements_finished, \n",
    "                input_,\n",
    "                state,\n",
    "                output,\n",
    "                loop_state)\n",
    "        \n",
    "        def loop_fn(time, previous_output, previous_state,  previous_loop_state):\n",
    "            if previous_state is None:\n",
    "                assert previous_output is None and previous_state is None\n",
    "                return loop_fn_initial()\n",
    "            else:\n",
    "                return loop_fn_transition(time, previous_output, previous_state,  previous_loop_state)\n",
    "        \n",
    "        decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "        decoder_outputs = decoder_outputs_ta.stack()\n",
    "        decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "        decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_output_size))\n",
    "        decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "        decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, self.vocab_size))\n",
    "        decoder_logits = tf.transpose(decoder_logits, [1, 0, 2])\n",
    "        self.decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "        \n",
    "        # Optimization\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.one_hot(self.decoder_target, depth=self.vocab_size, dtype=tf.float32),\n",
    "            logits=decoder_logits)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimizer = self._get_optimizer(self.optimizer_name, self.learning_rate_op, self.optimizer_conf)\n",
    "            self.train_step = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def _optimize(self, batch_X, batch_y, *args, **kwargs):\n",
    "        batch_X, Xlen = batch(batch_X[0])\n",
    "        length = np.max(Xlen) + 3\n",
    "        batch_y = self._batch_padding(batch_y, length)\n",
    "        feed_dict = {self.encoder_input: batch_X,\n",
    "                     self.decoder_target: batch_y,\n",
    "                     self.encoder_input_length: Xlen,\n",
    "                     self.training: True}\n",
    "        _, loss = self.sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def _get_output_size(self, conf):\n",
    "        if isinstance(conf, list) or isinstance(conf, tuple):\n",
    "            x = conf[-1]\n",
    "        else:\n",
    "            x = conf\n",
    "        return x[\"num_units\"]\n",
    "    \n",
    "    def generate_sentences(self, sentences):\n",
    "        words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "        X = [self.processor.encode(word) for word in words]\n",
    "        X, Xlen = batch(X)\n",
    "        feed_dict = {self.encoder_input: X,\n",
    "                     self.encoder_input_length: Xlen,\n",
    "                     self.training: False}\n",
    "        word_idx = self.sess.run(self.decoder_prediction, feed_dict=feed_dict)\n",
    "        return [self.processor.decode(i) for i in word_idx]\n",
    "    \n",
    "    def _batch_padding(self, batch, length):\n",
    "        EOS = 1\n",
    "        PAD = 0\n",
    "        padded_batch = []\n",
    "        for x in batch:\n",
    "            x = list(x)\n",
    "            if len(x) < length:\n",
    "                x.append(EOS)\n",
    "            while len(x) < length:\n",
    "                x.append(PAD)\n",
    "            padded_batch.append(x)\n",
    "        return np.array(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "class BasicProcessor(object):\n",
    "    \"\"\"Process data for estimators.\"\"\"\n",
    "        \n",
    "    def batch_process(self, X, y=None):\n",
    "        \"\"\"Make sure to have numpy data for input and target\"\"\"\n",
    "        if y is None:\n",
    "            return np.array(X)\n",
    "        else:\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "    def batch_process_y(self, y):\n",
    "        return np.array(y)\n",
    "    \n",
    "class Word2IndexProcessor(BasicProcessor):\n",
    "    def __init__(self,  texts):\n",
    "        _texts = []\n",
    "        lengths = []\n",
    "        for text in texts:\n",
    "            words = nltk.word_tokenize(text)\n",
    "            _texts.extend(words)\n",
    "            lengths.append(len(words))\n",
    "        lengths = list(np.cumsum(lengths))\n",
    "        lengths.insert(0, 0)\n",
    "        self.encoder = preprocessing.LabelEncoder()\n",
    "        indices = self.encoder.fit_transform(_texts)\n",
    "        # split to sentences\n",
    "        self.data = np.array([indices[lengths[i]:lengths[i+1]] for i in range(len(lengths) - 1)])\n",
    "        \n",
    "    def encode(self, text):\n",
    "        text_ = [x for x in text if x in self.encoder.classes_]\n",
    "        # text_ = text.split()\n",
    "        return self.encoder.transform(text_) + 2\n",
    "    \n",
    "    def decode(self, index):\n",
    "        return [self.encoder.inverse_transform(i-2) for i in index if i >=2]    \n",
    "    \n",
    "    def batch_process(self, X, y=None):\n",
    "        X, xlen = batch(X)\n",
    "        if y is None:\n",
    "            return np.array(X)\n",
    "        else:\n",
    "            y, ylen = batch(y)\n",
    "            return np.array(X), np.array(y)\n",
    "        \n",
    "    def batch_process_y(self, y):\n",
    "        y, ylen = batch(y)\n",
    "        return np.array(y)\n",
    "            \n",
    "    \n",
    "    def batch_process_test(self, X, y=None):\n",
    "        if y is None:\n",
    "            return np.array([self.encode(x_i) for x_i in X])\n",
    "        else:\n",
    "            return np.array([self.encode(x_i) for x_i in X]), np.array(y)\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 13.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processor = Word2IndexProcessor(X[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "conf = {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"learning_rate_minimum\": 1e-5,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 10,\n",
    "        \"batch_size\": 128,\n",
    "        \"model_dir\": \"./logs\",\n",
    "        \"load_file_path\": None,\n",
    "        \"save_file_path\": None,\n",
    "        \"log_freq\": 1,\n",
    "        \"model_encoder\":{\"name\":\"lnlstm\", \"num_units\":100},\n",
    "        \"model_decoder\":{\"name\":\"lnlstm\", \"num_units\":100},\n",
    "        \n",
    "}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "agent = DialogueAgent(processor, conf=conf)\n",
    "train_X = processor.data[:50]\n",
    "train_y = processor.data[50:100]\n",
    "agent.fit(train_X, train_y, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['play',\n",
       "  'political',\n",
       "  'play',\n",
       "  'abstract',\n",
       "  'political',\n",
       "  'political',\n",
       "  's',\n",
       "  'questions',\n",
       "  'Child',\n",
       "  'review',\n",
       "  'Germany',\n",
       "  'than',\n",
       "  'guide',\n",
       "  'guide',\n",
       "  'status',\n",
       "  'ring',\n",
       "  'effects',\n",
       "  'States'],\n",
       " ['email',\n",
       "  'hack',\n",
       "  'reply',\n",
       "  'immunity',\n",
       "  'lightning',\n",
       "  'F1',\n",
       "  'hack',\n",
       "  'States',\n",
       "  'not',\n",
       "  'not',\n",
       "  'aircraft',\n",
       "  'aircraft',\n",
       "  'States',\n",
       "  'kickass'],\n",
       " ['immunity',\n",
       "  'F1',\n",
       "  'hack',\n",
       "  'lightning',\n",
       "  'tips',\n",
       "  'eaten',\n",
       "  'nose',\n",
       "  'share',\n",
       "  'share',\n",
       "  'horcrux',\n",
       "  'jealous',\n",
       "  'Maul',\n",
       "  '60k',\n",
       "  'my',\n",
       "  'dance',\n",
       "  '60k',\n",
       "  'back',\n",
       "  'log'],\n",
       " ['email',\n",
       "  'hack',\n",
       "  'remember',\n",
       "  'immunity',\n",
       "  'the',\n",
       "  'not',\n",
       "  'not',\n",
       "  'Rohingya',\n",
       "  'not',\n",
       "  'people',\n",
       "  'master',\n",
       "  'master',\n",
       "  'master',\n",
       "  'master',\n",
       "  'trading',\n",
       "  'green'],\n",
       " ['play',\n",
       "  'the',\n",
       "  'f-14',\n",
       "  'immunity',\n",
       "  'salary',\n",
       "  'better',\n",
       "  'manipulation',\n",
       "  'earthquake',\n",
       "  'after',\n",
       "  'How',\n",
       "  'Warrior',\n",
       "  'Warrior',\n",
       "  'buy',\n",
       "  'confirmation',\n",
       "  'experience',\n",
       "  'manager',\n",
       "  'oxide',\n",
       "  'Ray',\n",
       "  'abstract'],\n",
       " ['a',\n",
       "  'a',\n",
       "  'abstract',\n",
       "  'up',\n",
       "  'share',\n",
       "  'share',\n",
       "  'male',\n",
       "  'from',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'cost',\n",
       "  'cost',\n",
       "  'interview',\n",
       "  'English',\n",
       "  'by',\n",
       "  'Programming',\n",
       "  '50,000',\n",
       "  'tiago',\n",
       "  'Does',\n",
       "  'Does',\n",
       "  'AOA',\n",
       "  'so',\n",
       "  'What'],\n",
       " ['play',\n",
       "  'memorable',\n",
       "  'memorable',\n",
       "  'player',\n",
       "  'citizens',\n",
       "  'hack',\n",
       "  'my',\n",
       "  'status'],\n",
       " ['play',\n",
       "  'memorable',\n",
       "  'my',\n",
       "  'should',\n",
       "  'email',\n",
       "  'immunity',\n",
       "  'not',\n",
       "  'not',\n",
       "  'not',\n",
       "  'laptop',\n",
       "  '('],\n",
       " ['email',\n",
       "  'immunity',\n",
       "  'US',\n",
       "  'not',\n",
       "  'not',\n",
       "  'Rohingya',\n",
       "  'not',\n",
       "  'Rohingya',\n",
       "  'Cap',\n",
       "  'master',\n",
       "  'master',\n",
       "  'cares'],\n",
       " ['play',\n",
       "  'speed',\n",
       "  'status',\n",
       "  'does',\n",
       "  'status',\n",
       "  'status',\n",
       "  'lost',\n",
       "  'Christians',\n",
       "  'States',\n",
       "  'Christians',\n",
       "  \"'m\",\n",
       "  'kickass',\n",
       "  'kickass',\n",
       "  'she',\n",
       "  'trading',\n",
       "  'Which']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.generate_sentences(X[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ array([133, 331, 513, 497, 194, 497, 300, 524, 329, 319, 474, 372, 319,\n",
       "       322,  24]),\n",
       "       array([133, 331, 513, 500, 400,  82,   5,  81,   6,  50,  24]),\n",
       "       array([ 67, 197,  68, 321, 513, 490, 400, 387, 326, 219, 553, 536, 143,\n",
       "       129,  24]),\n",
       "       array([138, 154,  68, 378, 537, 354,  24,  67, 197,  68, 483, 333,  24]),\n",
       "       array([136, 406, 239, 319, 544, 440, 506,   7, 469,   7, 380, 156, 200,\n",
       "       233, 411,  24]),\n",
       "       array([ 28,  22,  68, 154, 143,  40, 117,  39, 383, 156, 199, 462,  10,\n",
       "       551, 241, 512, 471, 144, 375,  24]),\n",
       "       array([112,  68, 193, 521,  24]),\n",
       "       array([ 67, 197,  68, 174, 143, 296, 289,  24]),\n",
       "       array([134, 240, 565, 534, 569, 323, 400, 568,  24]),\n",
       "       array([ 91,   5, 216,   6,  22,  37,  68, 303, 387,  42,  92,  47,  24])], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = nltk.word_tokenize(X[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'step',\n",
       " 'by',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'to',\n",
       " 'invest',\n",
       " 'in',\n",
       " 'share',\n",
       " 'market',\n",
       " 'in',\n",
       " 'india',\n",
       " '?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 263 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index = processor.batch_process(processor.data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt, xlen = batch(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 12, 16, 14, 17, 21, 6, 9, 10, 14]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.seq2seq' has no attribute 'basic_rnn_seq2seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c221f39d3a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_rnn_seq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.seq2seq' has no attribute 'basic_rnn_seq2seq'"
     ]
    }
   ],
   "source": [
    "seq2seq.basic_rnn_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(10))\n",
    "a.insert(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
