{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = df.dropna()\n",
    "X1 = df[\"question1\"].values\n",
    "X2 = df[\"question2\"].values\n",
    "y = df[\"is_duplicate\"].values\n",
    "X= [X1, X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch(inputs, max_sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs:\n",
    "            list of sentences (integer lists)\n",
    "        max_sequence_length:\n",
    "            integer specifying how large should `max_time` dimension be.\n",
    "            If None, maximum sequence length would be used\n",
    "    \n",
    "    Outputs:\n",
    "        inputs_time_major:\n",
    "            input sentences transformed into time-major matrix \n",
    "            (shape [max_time, batch_size]) padded with 0s\n",
    "        sequence_lengths:\n",
    "            batch-sized list of integers specifying amount of active \n",
    "            time steps in each input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    # [batch_size, max_time] -> [max_time, batch_size]\n",
    "    # inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_batch_major, sequence_lengths\n",
    "\n",
    "\n",
    "def random_sequences(length_from, length_to,\n",
    "                     vocab_lower, vocab_upper,\n",
    "                     batch_size):\n",
    "    \"\"\" Generates batches of random integer sequences,\n",
    "        sequence length in [length_from, length_to],\n",
    "        vocabulary in [vocab_lower, vocab_upper]\n",
    "    \"\"\"\n",
    "    if length_from > length_to:\n",
    "            raise ValueError('length_from > length_to')\n",
    "\n",
    "    def random_length():\n",
    "        if length_from == length_to:\n",
    "            return length_from\n",
    "        return np.random.randint(length_from, length_to + 1)\n",
    "    \n",
    "    while True:\n",
    "        yield [\n",
    "            np.random.randint(low=vocab_lower,\n",
    "                              high=vocab_upper,\n",
    "                              size=random_length()).tolist()\n",
    "            for _ in range(batch_size)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from hedgeable_ai.models.nn import BaseModel, get_shape, get_length\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from hedgeable_ai.models.nn.params import nn_is_logit\n",
    "from hedgeable_ai.models.nn import MultiNN, get_shape\n",
    "\n",
    "\n",
    "\n",
    "class DialogueAgent(MultiNN):\n",
    "    def __init__(self, processor, *args, **kwargs):\n",
    "        self.emb_size = 300\n",
    "        # add padding index 0\n",
    "        self.vocab_size = processor.vocab_size + 1\n",
    "        super().__init__(processor=processor, *args, **kwargs)\n",
    "   \n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build tensorflow graph\n",
    "        \n",
    "        Note:\n",
    "            You build graphs for output and input, which will be used \n",
    "            for training and prediction.\n",
    "        \"\"\"\n",
    "        self.encoder_input = tf.placeholder(tf.int32, shape=(None, None), name=\"encoder_input\")\n",
    "        self.encoder_input_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_input_length')\n",
    "        self.decoder_input = tf.placeholder(tf.int32, shape=(None, None), name=\"decoder_input\")\n",
    "        self.target = tf.placeholder(tf.int32, shape=(None, None), name=\"target\")\n",
    "        word_embeddings = tf.get_variable(\"word_embeddings\", [self.vocab_size, self.emb_size])\n",
    "        embedded_encoder = tf.gather(word_embeddings, self.encoder_input)\n",
    "        embedded_decoder = tf.gather(word_embeddings, self.decoder_input)\n",
    "        #  Build Seq2Seq Model\n",
    "        cell = [rnn.LSTMCell(512) for _ in range(3)]\n",
    "        cell = rnn.MultiRNNCell(cell)\n",
    "        \n",
    "        # Embedding\n",
    "        embeddings = tf.Variable(tf.random_uniform([self.vocab_size, self.emb_size], -1.0, 1.0, dtype=tf.float32))\n",
    "        encoder_input_embedded = tf.nn.embedding_lookup(embeddings, self.encoder_input)\n",
    "        decoder_input_embedded = tf.nn.embedding_lookup(embeddings, self.decoder_input)\n",
    "        encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "            cell, encoder_input_embedded, dtype=tf.float32, \n",
    "            time_major=False, scope=\"encoder\")\n",
    "        # We do not need encoder outputs\n",
    "        del encoder_outputs\n",
    "        \n",
    "        # Decoder\n",
    "        batch_size, encoder_max_time = tf.unstack(tf.shape(self.encoder_input))\n",
    "        decoder_length = self.encoder_input_length + 3\n",
    "        W = tf.Variable(tf.random_uniform([512, self.vocab_size], -1, 1), dtype=tf.float32)\n",
    "        b = tf.Variable(tf.zeros([self.vocab_size]), dtype=tf.ffloat32)\n",
    "        eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "        pad_time_slice = tf.zeros([batch_size],  dtype=tf.int32, name='PAD')\n",
    "        \n",
    "        eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "        pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n",
    "        \n",
    "        def loop_fn_initial():\n",
    "            initial_elements_finished = (0 >= decoder_length)\n",
    "            initial_input = eos_step_embedded\n",
    "            initial_cell_state = encoder_final_state\n",
    "            initial_cell_output  = None\n",
    "            initial_loop_state = None\n",
    "            return (initial_elements_finished,\n",
    "                initial_input,\n",
    "                initial_cell_state,\n",
    "                initial_cell_output,\n",
    "                initial_loop_state)\n",
    "        \n",
    "        def loop_fn_transition(time, previous_output,  previous_state, previous_loop_state):\n",
    "            \n",
    "            def get_next_input():\n",
    "                output_logits - tf.add(tf.matmul(previous_output, W), b)\n",
    "                prediction  = tf.argmax(output_logits,  axis=1)\n",
    "                next_input = tf.nn.embedding_lookup(embeddings,  prediction)\n",
    "                return next_input\n",
    "            \n",
    "            elements_finished = (time >= decoder_length)\n",
    "            \n",
    "            finished = tf.reduce_all(elements_finished)\n",
    "            input_ = tf.cond(finished, lambda pad_step_embedded, get_next_input)\n",
    "            state  = previous_state\n",
    "            output =  previous_output\n",
    "            loop_state = None\n",
    "            return (elements_finished, \n",
    "                input_,\n",
    "                state,\n",
    "                output,\n",
    "                loop_state)\n",
    "        finished, next_input, next_cell_state, emit_output, next_loop_state\n",
    "        \n",
    "        def loop_fn(time, previous_output, previous_state,  previous_loop_state):\n",
    "            if previous_state is None:\n",
    "                assert previous_output is None and previous_state is None\n",
    "                return loop_fn_initial()\n",
    "            else:\n",
    "                return loop_fn_transition(time, previous_output, previous_state,  previous_loop_state)\n",
    "            \n",
    "        \n",
    "        decoder_outputs_ta, decoder_final_state = tf.nn.raw_rnn(cell, loop_fn)\n",
    "        decoder_outputs = decoder_outputs_ta.stack()\n",
    "        decoder_logits = tf.contrib.layers.fully_connected(\n",
    "            decoder_outputs, self.vocab_size, activation_fn=None)\n",
    "        decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "        decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "        decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "        decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "        self.decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "        \n",
    "        # Optimization\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.one_hot(self.target, depth=self.vocab_size, dtype=tf.float32),\n",
    "            logits=decoder_logits)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        self.learning_rate_op = self._get_learning_rate()\n",
    "        self.train_step =\\\n",
    "            tf.train.AdamOptimizer(self.learning_rate_op).minimize(self.loss)\n",
    "        # Build tensorboad graph\n",
    "        with tf.name_scope(\"summary\"):\n",
    "            self._build_summaries()\n",
    "        \n",
    "        # initialize graph\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _optimize(self, batch_X, batch_y, *args, **kwargs):\n",
    "        feed_dict={\n",
    "                self.encoder_input: batch_X[0],\n",
    "                self.decoder_input: batch_X[1],\n",
    "                self.training: True,\n",
    "                self.target: batch_y}\n",
    "        _, loss = self.sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "class BasicProcessor(object):\n",
    "    \"\"\"Process data for estimators.\"\"\"\n",
    "        \n",
    "    def batch_process(self, X, y=None):\n",
    "        \"\"\"Make sure to have numpy data for input and target\"\"\"\n",
    "        if y is None:\n",
    "            return np.array(X)\n",
    "        else:\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "    def batch_process_y(self, y):\n",
    "        return np.array(y)\n",
    "    \n",
    "class Word2IndexProcessor(BasicProcessor):\n",
    "    def __init__(self,  texts):\n",
    "        _texts = []\n",
    "        for text in texts:\n",
    "            words = nltk.word_tokenize(text)\n",
    "            words.append(\"<eos>\")\n",
    "            _texts.extend(words)\n",
    "        self.encoder = preprocessing.LabelEncoder()\n",
    "        indices = self.encoder.fit_transform(_texts)\n",
    "        eos_idx = self.encoder.transform([\"<eos>\"])[0]\n",
    "        # split to sentences\n",
    "        end_idx = [0,]\n",
    "        for i, idx in enumerate(indices):\n",
    "            if idx == eos_idx:\n",
    "                end_idx.append(i+1)\n",
    "        self.data = np.array([indices[end_idx[i]:end_idx[i+1]] for i in range(len(end_idx) - 1)])\n",
    "        \n",
    "    def _encode(self, text):\n",
    "        text_ = [x for x in text if x in self.encoder.classes_]\n",
    "        # text_ = text.split()\n",
    "        return self.encoder.transform(text_) + 1\n",
    "    \n",
    "    def _decode(self, index):\n",
    "        return [self.encoder.inverse_transform(i-1) for i in index]    \n",
    "    \n",
    "    def batch_process(self, X, y=None):\n",
    "        X, xlen = batch(X)\n",
    "        if y is None:\n",
    "            return np.array(X)\n",
    "        else:\n",
    "            y, ylen = batch(y)\n",
    "            return np.array(X), np.array(y)\n",
    "        \n",
    "    def batch_process_y(self, y):\n",
    "        y, ylen = batch(y)\n",
    "        return np.array(y)\n",
    "            \n",
    "    \n",
    "    def batch_process_test(self, X, y=None):\n",
    "        if y is None:\n",
    "            return np.array([self._encode(x_i) for x_i in X])\n",
    "        else:\n",
    "            return np.array([self._encode(x_i) for x_i in X]), np.array(y)\n",
    "        \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 8 ms, total: 1.02 s\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processor = Word2IndexProcessor(X[0][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: params/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "conf = {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"learning_rate_minimum\": 1e-5,\n",
    "        \"learning_rate_decay\": 0.9,\n",
    "        \"learning_rate_decay_step\": 10,\n",
    "        \"batch_size\": 128,\n",
    "        \"model_dir\": \"./logs\",\n",
    "        \"load_file_path\": None,\n",
    "        \"save_file_path\": None,\n",
    "        \"log_freq\": 1,\n",
    "        \"model\":[{\"name\": \"conv2d\", \"kernel_size\":(5, 1), \"num_filter\":32, \"stride\":(2, 1),\n",
    "             \"padding\": 'valid', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"conv2d\", \"kernel_size\":(3, 1), \"num_filter\":64, \"stride\":(2, 1),\n",
    "             \"padding\": 'valid', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "                 {\"name\": \"conv2d\", \"kernel_size\":(3, 1), \"num_filter\":64, \"stride\":(2, 1),\n",
    "             \"padding\": 'valid', \"is_batch\":True, 'activation': tf.nn.relu},\n",
    "            {\"name\": \"dense\", \"is_flatten\":True, \"is_batch\":True, \"num_hidden\": 128, 'activation': tf.nn.relu},\n",
    "        ],\n",
    "        \n",
    "}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "agent = DialogueAgent(processor, conf=conf)\n",
    "train_X = [processor.data[:50], processor.data[50:100]]\n",
    "train_y = processor.data[50:100]\n",
    "agent.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(processor, \"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([134, 332, 514, 501, 401,  83,   5,  82,   6,  51,  25,  24]),\n",
       "       array([139, 155,  69, 379, 538, 355,  25,  68, 198,  69, 484, 334,  25,  24]),\n",
       "       array([ 29,  22,  69, 155, 144,  41, 118,  40, 384, 157, 200, 463,  10,\n",
       "       552, 242, 513, 472, 145, 376,  25,  24])], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_X[0])[[1, 3, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = nltk.word_tokenize(X[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'step',\n",
       " 'by',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'to',\n",
       " 'invest',\n",
       " 'in',\n",
       " 'share',\n",
       " 'market',\n",
       " 'in',\n",
       " 'india',\n",
       " '?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 263 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index = processor.batch_process(processor.data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt, xlen = batch(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 12, 16, 14, 17, 21, 6, 9, 10, 14]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.seq2seq' has no attribute 'basic_rnn_seq2seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c221f39d3a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_rnn_seq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.seq2seq' has no attribute 'basic_rnn_seq2seq'"
     ]
    }
   ],
   "source": [
    "seq2seq.basic_rnn_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
