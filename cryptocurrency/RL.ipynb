{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_ = {}\n",
    "\n",
    "for filename in os.listdir(\"data/Top100Cryptos/\"):\n",
    "    path = os.path.join(\"data/Top100Cryptos/\", filename)\n",
    "    try:\n",
    "        name = filename.split(\".\")[0]\n",
    "        data_[name] = pd.read_csv(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "num_data = []\n",
    "for name in data_.keys():\n",
    "    num_data.append(data_[name].shape[0])\n",
    "name_list = np.array(list(data_.keys()))[np.argsort(num_data)[::-1]]\n",
    "big_names = name_list[:10]\n",
    "\n",
    "data = dict()\n",
    "for name in big_names:\n",
    "    data[name] = data_[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time(t):\n",
    "    m = {\n",
    "        'Jan' : \"01\",\n",
    "        'Feb' : \"02\",\n",
    "        'Mar' : \"03\",\n",
    "        'Apr' : \"04\",\n",
    "        'May' : \"05\",\n",
    "        'Jun' : \"06\",\n",
    "        'June' : \"06\",\n",
    "        'Jul' : \"07\",\n",
    "        'Aug' : \"08\",\n",
    "        'Sep' : \"09\", \n",
    "        'Oct' : \"10\",\n",
    "        'Nov' : \"11\",\n",
    "        'Dec' : \"12\"\n",
    "    }\n",
    "    t_list = t.replace(\",\", \"\").split()\n",
    "    t_list[0] = m[t_list[0]]\n",
    "    return \"-\".join([t_list[2], t_list[0], t_list[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.environments import Environment\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "\n",
    "class TradeEnvironment(Environment):\n",
    "    \"\"\"Environment only for close prices\"\"\"\n",
    "    def __init__(self, data, start=None, end=None, add_cash=True):\n",
    "        time_index = set()\n",
    "        impute_data = {}\n",
    "        data = deepcopy(data)\n",
    "        for key, val in data.items():\n",
    "            dates = val[\"Date\"].values\n",
    "            dates = [convert_time(d) for d in dates]\n",
    "            impute_data[key] = dict(time_range=(dates[-1], dates[0]),\n",
    "                                    impute_val=(val.iloc[-1], val.iloc[0]))\n",
    "            data[key].index = dates\n",
    "            time_index = time_index.union(set(dates))\n",
    "        self.time_index = sorted(list(time_index))\n",
    "        if add_cash:\n",
    "            val = np.ones(len(self.time_index))\n",
    "            cash_df = pd.DataFrame({\"Open\" : val,\n",
    "                                    \"High\" : val,\n",
    "                                    \"Low\" : val,\n",
    "                                    \"Close\" : val,\n",
    "                                    \"Volume\" : val},\n",
    "                                   index=self.time_index)\n",
    "            key = \"Cash\"\n",
    "            data[key] = cash_df\n",
    "            impute_data[key] = dict(time_range=(self.time_index[-1], self.time_index[0]),\n",
    "                                    impute_val=(cash_df.iloc[-1], cash_df.iloc[0]))\n",
    "        self.impute_data  = impute_data\n",
    "        if start is None:\n",
    "            self.start = self.time_index[0]\n",
    "        else:\n",
    "            self.start = min(start, self.time_index[0])\n",
    "        if end is None:\n",
    "            self.end = self.time_index[-1]\n",
    "        else:\n",
    "            self.end = max(end, self.time_index[-1])\n",
    "        self.data = data\n",
    "        self.symbols = list(data.keys())\n",
    "        self.num_stocks = len(self.symbols)\n",
    "        self.current_time = self.start\n",
    "        self.current_step = 0\n",
    "        # Use for calculate return\n",
    "        self.prev_states = self._get_bar()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset environment and setup for new episode.\n",
    "        Returns:\n",
    "            initial state of resetted environment.\n",
    "        \"\"\"\n",
    "        self.current_time = self.start\n",
    "        self.cirrent_step = 0\n",
    "        current_states = self._get_bar()\n",
    "        states = np.array([current_states[symbol][\"Close\"] for symbol in self.symbols])\n",
    "        return states\n",
    "\n",
    "    def execute(self, actions):\n",
    "        \"\"\"\n",
    "        Executes action, observes next state(s) and reward.\n",
    "        Args:\n",
    "            actions: Actions to execute.\n",
    "        Returns:\n",
    "            (Dict of) next state(s), boolean indicating terminal, and reward signal.\n",
    "        \"\"\"\n",
    "        current_states = self._get_bar()\n",
    "        returns = []\n",
    "        for symbol in self.symbols:\n",
    "            returns.append(current_states[symbol][\"Close\"] / self.prev_states[symbol][\"Close\"] - 1)\n",
    "        # print(\"current_states\", self.current_time)\n",
    "        # Update status\n",
    "        self.prev_states = deepcopy(current_states)\n",
    "        self._update_time()\n",
    "        states = np.array([current_states[symbol][\"Close\"] for symbol in self.symbols])\n",
    "        terminal = False\n",
    "        reward = np.sum(np.array(returns) * actions)\n",
    "        return states, terminal, reward\n",
    "        \n",
    "            \n",
    "    def _update_time(self):\n",
    "        index = self.time_index.index(self.current_time)\n",
    "        self.current_time = self.time_index[index + 1]\n",
    "        self.current_step += 1\n",
    "        \n",
    "    def _get_bar(self):\n",
    "        bar = {}\n",
    "        for symbol in self.symbols:\n",
    "            min_t = self.impute_data[symbol][\"time_range\"][0]\n",
    "            max_t = self.impute_data[symbol][\"time_range\"][1]\n",
    "            if (min_t <= self.current_time) and (max_t >= self.current_time):\n",
    "                if self.current_time in self.data[symbol].index:\n",
    "                    bar[symbol] = self.data[symbol].loc[self.current_time]\n",
    "                else:\n",
    "                    bar[symbol] = deepcopy(self.impute_bar[symbol])\n",
    "            elif min_t > self.current_time:\n",
    "                bar[symbol] = self.impute_data[symbol][\"impute_val\"][0]\n",
    "            else:\n",
    "                bar[symbol] = self.impute_data[symbol][\"impute_val\"][1]\n",
    "        # Keep value for imputation\n",
    "        self.impute_bar = deepcopy(bar)\n",
    "        return bar\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        \"\"\"\n",
    "        Return the state space. Might include subdicts if multiple states are available simultaneously.\n",
    "        Returns: dict of state properties (shape and type).\n",
    "        \"\"\"\n",
    "        return {'shape': (self.num_stocks,), 'type': 'float'}\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        \"\"\"\n",
    "        Return the action space. Might include subdicts if multiple actions are available simultaneously.\n",
    "        Returns: dict of action properties (continuous, number of actions)\n",
    "        \"\"\"\n",
    "        return {\"shape\": (self.num_stocks,), \"min_value\": 0., \"max_value\": 1., \"type\": \"float\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_stats(log_return, accumulated_pv, peak_pv):\n",
    "    accumulated_pv *= np.exp(log_return)\n",
    "    if peak_pv < accumulated_pv:\n",
    "        peak_pv = accumulated_pv\n",
    "    draw_down = (peak_pv - accumulated_pv) / peak_pv\n",
    "    return_ = np.exp(log_return) - 1.\n",
    "    return return_, accumulated_pv, peak_pv, draw_down\n",
    "\n",
    "def calc_sharp_ratio(returns, bench_mark=0, eps=1e-6):\n",
    "    var = np.var(returns)\n",
    "    mean = np.mean(returns)\n",
    "    return (mean - bench_mark) / (np.sqrt(var) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "from six.moves import xrange\n",
    "from tensorforce.execution import Runner\n",
    "\n",
    "\n",
    "class TradeRunner(Runner):\n",
    "    \"\"\"\n",
    "    Simple runner for non-realtime single-process execution.\n",
    "    \"\"\"\n",
    "    def run(\n",
    "        self,\n",
    "        timesteps=None,\n",
    "        episodes=None,\n",
    "        max_episode_timesteps=None,\n",
    "        deterministic=False,\n",
    "        episode_finished=None,\n",
    "        init_pv=100.,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Runs the agent on the environment.\n",
    "\n",
    "        Args:\n",
    "            timesteps: Number of timesteps\n",
    "            episodes: Number of episodes\n",
    "            max_episode_timesteps: Max number of timesteps per episode\n",
    "            deterministic: Deterministic flag\n",
    "            episode_finished: Function handler taking a `Runner` argument and returning a boolean indicating\n",
    "                whether to continue execution. For instance, useful for reporting intermediate performance or\n",
    "                integrating termination conditions.\n",
    "        \"\"\"\n",
    "\n",
    "        # Keep track of episode reward and episode length for statistics.\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.agent.reset()\n",
    "\n",
    "        self.episode = self.agent.episode\n",
    "        if episodes is not None:\n",
    "            episodes += self.agent.episode\n",
    "\n",
    "        self.timestep = self.agent.timestep\n",
    "        if timesteps is not None:\n",
    "            timesteps += self.agent.timestep\n",
    "            \n",
    "        self.sp_list = []\n",
    "        self.mmd_list = []\n",
    "        self.pv_list = []\n",
    "        self.accumulated_pvs_list = []\n",
    "        self.draw_downs_list = []\n",
    "\n",
    "        while True:\n",
    "            episode_start_time = time.time()\n",
    "\n",
    "            self.agent.reset()\n",
    "            state = self.environment.reset()\n",
    "            accumulated_pv = init_pv\n",
    "            peak_pv = init_pv\n",
    "            self.accumulated_pvs = [accumulated_pv]\n",
    "            draw_downs = [0.]\n",
    "            returns = [0.]\n",
    "            self.episode_timestep = 0\n",
    "\n",
    "            while True:\n",
    "                action = self.agent.act(states=state, deterministic=deterministic)\n",
    "                log_return = 0\n",
    "                for repeat in xrange(self.repeat_actions):\n",
    "                    state, terminal, step_reward = self.environment.execute(actions=action)\n",
    "                    log_return += np.log(1. + step_reward)\n",
    "                    if terminal:\n",
    "                        break\n",
    "\n",
    "                if max_episode_timesteps is not None and self.episode_timestep >= max_episode_timesteps:\n",
    "                    terminal = True\n",
    "\n",
    "                self.agent.observe(terminal=terminal, reward=log_return)\n",
    "\n",
    "                self.episode_timestep += 1\n",
    "                self.timestep += 1\n",
    "                return_, accumulated_pv, peak_pv, draw_down = calc_stats(log_return, accumulated_pv, peak_pv)\n",
    "                returns.append(return_)\n",
    "                self.accumulated_pvs.append(accumulated_pv)\n",
    "                draw_downs.append(draw_down)\n",
    "                \n",
    "                if terminal or self.agent.should_stop():  # TODO: should_stop also termina?\n",
    "                    break\n",
    "                    \n",
    "            time_passed = time.time() - episode_start_time\n",
    "\n",
    "            self.pv_list.append(self.accumulated_pvs[-1])\n",
    "            self.mmd_list.append(np.max(draw_downs))\n",
    "            self.sp_list.append(calc_sharp_ratio(returns))\n",
    "            self.accumulated_pvs_list.append(self.accumulated_pvs)\n",
    "            self.draw_downs_list.append(draw_downs)\n",
    "            \n",
    "            self.episode_timesteps.append(self.episode_timestep)\n",
    "            self.episode_times.append(time_passed)\n",
    "\n",
    "            self.episode += 1\n",
    "\n",
    "            if episode_finished and not episode_finished(self) or \\\n",
    "                    (episodes is not None and self.agent.episode >= episodes) or \\\n",
    "                    (timesteps is not None and self.agent.timestep >= timesteps) or \\\n",
    "                    self.agent.should_stop():\n",
    "                # agent.episode / agent.timestep are globally updated\n",
    "                break\n",
    "\n",
    "        self.agent.close()\n",
    "        self.environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1 after 201 timesteps (reward: 123.2162936229729)\n",
      "Finished episode 2 after 201 timesteps (reward: 168.9315061248446)\n",
      "Finished episode 3 after 201 timesteps (reward: 75.19972188695223)\n",
      "Finished episode 4 after 201 timesteps (reward: 119.30127310507494)\n",
      "Finished episode 5 after 201 timesteps (reward: 111.12543626299829)\n",
      "Finished episode 6 after 201 timesteps (reward: 124.0993520172236)\n",
      "Finished episode 7 after 201 timesteps (reward: 158.3964552070213)\n",
      "Finished episode 8 after 201 timesteps (reward: 159.02973401390656)\n",
      "Finished episode 9 after 201 timesteps (reward: 107.98572828625132)\n",
      "Finished episode 10 after 201 timesteps (reward: 102.71853604542662)\n",
      "Finished episode 11 after 201 timesteps (reward: 118.52963250756463)\n",
      "Finished episode 12 after 201 timesteps (reward: 149.50535044546365)\n",
      "Finished episode 13 after 201 timesteps (reward: 110.32936822368883)\n",
      "Finished episode 14 after 201 timesteps (reward: 140.90017079319563)\n",
      "Finished episode 15 after 201 timesteps (reward: 124.59051751470145)\n",
      "Finished episode 16 after 201 timesteps (reward: 168.46663204350423)\n",
      "Finished episode 17 after 201 timesteps (reward: 124.97602602861929)\n",
      "Finished episode 18 after 201 timesteps (reward: 133.00236815590986)\n",
      "Finished episode 19 after 201 timesteps (reward: 151.73991686313857)\n",
      "Finished episode 20 after 201 timesteps (reward: 112.78367053835589)\n",
      "Finished episode 21 after 201 timesteps (reward: 132.38326607093342)\n",
      "Finished episode 22 after 201 timesteps (reward: 138.23278251450276)\n",
      "Finished episode 23 after 201 timesteps (reward: 116.14914317447226)\n",
      "Finished episode 24 after 201 timesteps (reward: 138.0821580236216)\n",
      "Finished episode 25 after 201 timesteps (reward: 129.714866869577)\n",
      "Finished episode 26 after 201 timesteps (reward: 117.83941428745851)\n",
      "Finished episode 27 after 201 timesteps (reward: 116.6127668005888)\n",
      "Finished episode 28 after 201 timesteps (reward: 134.8876304714484)\n",
      "Finished episode 29 after 201 timesteps (reward: 202.61483592553827)\n",
      "Finished episode 30 after 201 timesteps (reward: 147.3779063569955)\n",
      "Finished episode 31 after 201 timesteps (reward: 143.23121436878645)\n",
      "Finished episode 32 after 201 timesteps (reward: 94.74603338208628)\n",
      "Finished episode 33 after 201 timesteps (reward: 104.16606697096269)\n",
      "Finished episode 34 after 201 timesteps (reward: 165.84611207039225)\n",
      "Finished episode 35 after 201 timesteps (reward: 103.55841478900618)\n",
      "Finished episode 36 after 201 timesteps (reward: 106.31900250924711)\n",
      "Finished episode 37 after 201 timesteps (reward: 105.70178562620256)\n",
      "Finished episode 38 after 201 timesteps (reward: 116.26884557745079)\n",
      "Finished episode 39 after 201 timesteps (reward: 124.67200775182893)\n",
      "Finished episode 40 after 201 timesteps (reward: 155.10743979904385)\n",
      "Finished episode 41 after 201 timesteps (reward: 152.90572362395676)\n",
      "Finished episode 42 after 201 timesteps (reward: 150.52929329461043)\n",
      "Finished episode 43 after 201 timesteps (reward: 118.09614079991758)\n",
      "Finished episode 44 after 201 timesteps (reward: 129.00555045854009)\n",
      "Finished episode 45 after 201 timesteps (reward: 98.44905863069964)\n",
      "Finished episode 46 after 201 timesteps (reward: 142.56421494110407)\n",
      "Finished episode 47 after 201 timesteps (reward: 129.62188082972108)\n",
      "Finished episode 48 after 201 timesteps (reward: 135.84855055578973)\n",
      "Finished episode 49 after 201 timesteps (reward: 146.52300887020232)\n",
      "Finished episode 50 after 201 timesteps (reward: 114.00421125421856)\n",
      "Finished episode 51 after 201 timesteps (reward: 154.58945067511777)\n",
      "Finished episode 52 after 201 timesteps (reward: 135.5484176999133)\n",
      "Finished episode 53 after 201 timesteps (reward: 115.58096633087386)\n",
      "Finished episode 54 after 201 timesteps (reward: 98.9556845965996)\n",
      "Finished episode 55 after 201 timesteps (reward: 111.31754893722359)\n",
      "Finished episode 56 after 201 timesteps (reward: 118.26641133686353)\n",
      "Finished episode 57 after 201 timesteps (reward: 177.6431706269195)\n",
      "Finished episode 58 after 201 timesteps (reward: 119.64310158617761)\n",
      "Finished episode 59 after 201 timesteps (reward: 246.65391006942863)\n",
      "Finished episode 60 after 201 timesteps (reward: 114.55710548767405)\n",
      "Finished episode 61 after 201 timesteps (reward: 135.6371029307036)\n",
      "Finished episode 62 after 201 timesteps (reward: 116.36810370665478)\n",
      "Finished episode 63 after 201 timesteps (reward: 91.75857582742391)\n",
      "Finished episode 64 after 201 timesteps (reward: 122.27639172445879)\n",
      "Finished episode 65 after 201 timesteps (reward: 93.29727999611502)\n",
      "Finished episode 66 after 201 timesteps (reward: 162.1472405013093)\n",
      "Finished episode 67 after 201 timesteps (reward: 171.57549797034082)\n",
      "Finished episode 68 after 201 timesteps (reward: 88.08412130238732)\n",
      "Finished episode 69 after 201 timesteps (reward: 126.58978845464073)\n",
      "Finished episode 70 after 201 timesteps (reward: 122.29687759418162)\n",
      "Finished episode 71 after 201 timesteps (reward: 139.71016837811567)\n",
      "Finished episode 72 after 201 timesteps (reward: 131.0705862505083)\n",
      "Finished episode 73 after 201 timesteps (reward: 118.85022076535215)\n",
      "Finished episode 74 after 201 timesteps (reward: 185.57162695725924)\n",
      "Finished episode 75 after 201 timesteps (reward: 126.51743789111487)\n",
      "Finished episode 76 after 201 timesteps (reward: 146.68001730655135)\n",
      "Finished episode 77 after 201 timesteps (reward: 164.8861790293182)\n",
      "Finished episode 78 after 201 timesteps (reward: 177.1319069664988)\n",
      "Finished episode 79 after 201 timesteps (reward: 134.46138562496554)\n",
      "Finished episode 80 after 201 timesteps (reward: 116.1411208672047)\n",
      "Finished episode 81 after 201 timesteps (reward: 116.96464060199327)\n",
      "Finished episode 82 after 201 timesteps (reward: 144.1366501979274)\n",
      "Finished episode 83 after 201 timesteps (reward: 174.89987739302998)\n",
      "Finished episode 84 after 201 timesteps (reward: 124.56139404635547)\n",
      "Finished episode 85 after 201 timesteps (reward: 118.26921821016415)\n",
      "Finished episode 86 after 201 timesteps (reward: 87.52795219235317)\n",
      "Finished episode 87 after 201 timesteps (reward: 132.37843780620113)\n",
      "Finished episode 88 after 201 timesteps (reward: 147.99666191381803)\n",
      "Finished episode 89 after 201 timesteps (reward: 142.95546322877598)\n",
      "Finished episode 90 after 201 timesteps (reward: 114.48400383580321)\n",
      "Finished episode 91 after 201 timesteps (reward: 124.41074998116407)\n",
      "Finished episode 92 after 201 timesteps (reward: 153.43814994558724)\n",
      "Finished episode 93 after 201 timesteps (reward: 139.7165674992764)\n",
      "Finished episode 94 after 201 timesteps (reward: 145.61700808140242)\n",
      "Finished episode 95 after 201 timesteps (reward: 135.57432761332436)\n",
      "Finished episode 96 after 201 timesteps (reward: 141.0415680259605)\n",
      "Finished episode 97 after 201 timesteps (reward: 135.24508964699595)\n",
      "Finished episode 98 after 201 timesteps (reward: 111.67335577280846)\n",
      "Finished episode 99 after 201 timesteps (reward: 130.08244598381916)\n",
      "Finished episode 100 after 201 timesteps (reward: 133.6351379264801)\n",
      "Finished episode 101 after 201 timesteps (reward: 118.39444276620122)\n",
      "Finished episode 102 after 201 timesteps (reward: 176.55700133686696)\n",
      "Finished episode 103 after 201 timesteps (reward: 144.03314939311016)\n",
      "Finished episode 104 after 201 timesteps (reward: 92.86854780340254)\n",
      "Finished episode 105 after 201 timesteps (reward: 195.20401025744997)\n",
      "Finished episode 106 after 201 timesteps (reward: 140.4971904599494)\n",
      "Finished episode 107 after 201 timesteps (reward: 145.9222872844858)\n",
      "Finished episode 108 after 201 timesteps (reward: 149.2239473016853)\n",
      "Finished episode 109 after 201 timesteps (reward: 101.61950920350043)\n",
      "Finished episode 110 after 201 timesteps (reward: 145.5357809187855)\n",
      "Finished episode 111 after 201 timesteps (reward: 149.32948836271223)\n",
      "Finished episode 112 after 201 timesteps (reward: 155.47153752172204)\n",
      "Finished episode 113 after 201 timesteps (reward: 118.47892094590165)\n",
      "Finished episode 114 after 201 timesteps (reward: 103.48389829302343)\n",
      "Finished episode 115 after 201 timesteps (reward: 131.55121577639062)\n",
      "Finished episode 116 after 201 timesteps (reward: 166.42392368042357)\n",
      "Finished episode 117 after 201 timesteps (reward: 108.70621509427178)\n",
      "Finished episode 118 after 201 timesteps (reward: 121.00680061465454)\n",
      "Finished episode 119 after 201 timesteps (reward: 126.65648522729491)\n",
      "Finished episode 120 after 201 timesteps (reward: 143.11236604150972)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 121 after 201 timesteps (reward: 154.7218415312743)\n",
      "Finished episode 122 after 201 timesteps (reward: 110.86422120279548)\n",
      "Finished episode 123 after 201 timesteps (reward: 127.72186681134136)\n",
      "Finished episode 124 after 201 timesteps (reward: 109.11176862039504)\n",
      "Finished episode 125 after 201 timesteps (reward: 120.92365877335929)\n",
      "Finished episode 126 after 201 timesteps (reward: 149.75359123690924)\n",
      "Finished episode 127 after 201 timesteps (reward: 106.87569365834428)\n",
      "Finished episode 128 after 201 timesteps (reward: 119.58492611066188)\n",
      "Finished episode 129 after 201 timesteps (reward: 77.84606750649006)\n",
      "Finished episode 130 after 201 timesteps (reward: 152.76018538637547)\n",
      "Finished episode 131 after 201 timesteps (reward: 86.26018713228049)\n",
      "Finished episode 132 after 201 timesteps (reward: 96.51902617122732)\n",
      "Finished episode 133 after 201 timesteps (reward: 86.50106888730463)\n",
      "Finished episode 134 after 201 timesteps (reward: 155.93018448512265)\n",
      "Finished episode 135 after 201 timesteps (reward: 185.78665017597285)\n",
      "Finished episode 136 after 201 timesteps (reward: 152.9254071460119)\n",
      "Finished episode 137 after 201 timesteps (reward: 110.25556193758554)\n",
      "Finished episode 138 after 201 timesteps (reward: 132.77137980474737)\n",
      "Finished episode 139 after 201 timesteps (reward: 164.9703931242275)\n",
      "Finished episode 140 after 201 timesteps (reward: 111.10927666177386)\n",
      "Finished episode 141 after 201 timesteps (reward: 146.02563147471048)\n",
      "Finished episode 142 after 201 timesteps (reward: 135.5012595809313)\n",
      "Finished episode 143 after 201 timesteps (reward: 152.92064510020654)\n",
      "Finished episode 144 after 201 timesteps (reward: 175.04201246992787)\n",
      "Finished episode 145 after 201 timesteps (reward: 143.29019463614205)\n",
      "Finished episode 146 after 201 timesteps (reward: 105.18937033176577)\n",
      "Finished episode 147 after 201 timesteps (reward: 100.36667528027156)\n",
      "Finished episode 148 after 201 timesteps (reward: 101.12182155927749)\n",
      "Finished episode 149 after 201 timesteps (reward: 150.04124289847147)\n",
      "Finished episode 150 after 201 timesteps (reward: 113.70362274105744)\n",
      "Finished episode 151 after 201 timesteps (reward: 129.28175508852453)\n",
      "Finished episode 152 after 201 timesteps (reward: 142.8381972248401)\n",
      "Finished episode 153 after 201 timesteps (reward: 148.19930984739642)\n",
      "Finished episode 154 after 201 timesteps (reward: 94.70119029623211)\n",
      "Finished episode 155 after 201 timesteps (reward: 164.5636404215526)\n",
      "Finished episode 156 after 201 timesteps (reward: 139.99505120301652)\n",
      "Finished episode 157 after 201 timesteps (reward: 181.47716948363984)\n",
      "Finished episode 158 after 201 timesteps (reward: 172.34706545969914)\n",
      "Finished episode 159 after 201 timesteps (reward: 135.99572198148036)\n",
      "Finished episode 160 after 201 timesteps (reward: 132.91392035890175)\n",
      "Finished episode 161 after 201 timesteps (reward: 168.53893473382516)\n",
      "Finished episode 162 after 201 timesteps (reward: 123.64615786918372)\n",
      "Finished episode 163 after 201 timesteps (reward: 117.29226281536471)\n",
      "Finished episode 164 after 201 timesteps (reward: 106.48476719864101)\n",
      "Finished episode 165 after 201 timesteps (reward: 147.5830113849243)\n",
      "Finished episode 166 after 201 timesteps (reward: 121.92186472715962)\n",
      "Finished episode 167 after 201 timesteps (reward: 109.61428841265152)\n",
      "Finished episode 168 after 201 timesteps (reward: 138.13450863935287)\n",
      "Finished episode 169 after 201 timesteps (reward: 121.33376274159562)\n",
      "Finished episode 170 after 201 timesteps (reward: 145.61966246609524)\n",
      "Finished episode 171 after 201 timesteps (reward: 152.03988024470203)\n",
      "Finished episode 172 after 201 timesteps (reward: 172.5746245137061)\n",
      "Finished episode 173 after 201 timesteps (reward: 124.52911281679626)\n",
      "Finished episode 174 after 201 timesteps (reward: 130.47591310761982)\n",
      "Finished episode 175 after 201 timesteps (reward: 119.3913101411841)\n",
      "Finished episode 176 after 201 timesteps (reward: 141.30590069952294)\n",
      "Finished episode 177 after 201 timesteps (reward: 124.13464396545182)\n",
      "Finished episode 178 after 201 timesteps (reward: 177.80450381876597)\n",
      "Finished episode 179 after 201 timesteps (reward: 153.63530952857514)\n",
      "Finished episode 180 after 201 timesteps (reward: 107.44687902043339)\n",
      "Finished episode 181 after 201 timesteps (reward: 87.64041093555478)\n",
      "Finished episode 182 after 201 timesteps (reward: 99.56945309942488)\n",
      "Finished episode 183 after 201 timesteps (reward: 152.6865346175271)\n",
      "Finished episode 184 after 201 timesteps (reward: 156.80686193690886)\n",
      "Finished episode 185 after 201 timesteps (reward: 90.88448676499189)\n",
      "Finished episode 186 after 201 timesteps (reward: 101.6906358442392)\n",
      "Finished episode 187 after 201 timesteps (reward: 101.12266655866459)\n",
      "Finished episode 188 after 201 timesteps (reward: 141.60638442206422)\n",
      "Finished episode 189 after 201 timesteps (reward: 121.77990727960068)\n",
      "Finished episode 190 after 201 timesteps (reward: 133.1687444345677)\n",
      "Finished episode 191 after 201 timesteps (reward: 105.07876741063205)\n",
      "Finished episode 192 after 201 timesteps (reward: 134.03680665454488)\n",
      "Finished episode 193 after 201 timesteps (reward: 133.04129362468436)\n",
      "Finished episode 194 after 201 timesteps (reward: 142.9218927928964)\n",
      "Finished episode 195 after 201 timesteps (reward: 95.28571610740254)\n",
      "Finished episode 196 after 201 timesteps (reward: 138.85814650800194)\n",
      "Finished episode 197 after 201 timesteps (reward: 159.88060764266865)\n",
      "Finished episode 198 after 201 timesteps (reward: 160.16944055752376)\n",
      "Finished episode 199 after 201 timesteps (reward: 97.47713410536971)\n",
      "Finished episode 200 after 201 timesteps (reward: 102.3143629560458)\n",
      "Finished episode 201 after 201 timesteps (reward: 101.7397211294584)\n",
      "Finished episode 202 after 201 timesteps (reward: 105.75189543714745)\n",
      "Finished episode 203 after 201 timesteps (reward: 88.34065470654286)\n",
      "Finished episode 204 after 201 timesteps (reward: 92.429596998599)\n",
      "Finished episode 205 after 201 timesteps (reward: 184.4682310282577)\n",
      "Finished episode 206 after 201 timesteps (reward: 156.19667343960873)\n",
      "Finished episode 207 after 201 timesteps (reward: 119.25013194305092)\n",
      "Finished episode 208 after 201 timesteps (reward: 175.96753852618238)\n",
      "Finished episode 209 after 201 timesteps (reward: 146.10613053596273)\n",
      "Finished episode 210 after 201 timesteps (reward: 130.88196227484693)\n",
      "Finished episode 211 after 201 timesteps (reward: 89.58258021129072)\n",
      "Finished episode 212 after 201 timesteps (reward: 124.99890550538662)\n",
      "Finished episode 213 after 201 timesteps (reward: 135.0440475826388)\n",
      "Finished episode 214 after 201 timesteps (reward: 104.72404776779669)\n",
      "Finished episode 215 after 201 timesteps (reward: 110.54394485593973)\n",
      "Finished episode 216 after 201 timesteps (reward: 102.7507474428272)\n",
      "Finished episode 217 after 201 timesteps (reward: 98.28821105490815)\n",
      "Finished episode 218 after 201 timesteps (reward: 98.11667068651951)\n",
      "Finished episode 219 after 201 timesteps (reward: 134.46950555151201)\n",
      "Finished episode 220 after 201 timesteps (reward: 148.6676835336552)\n",
      "Finished episode 221 after 201 timesteps (reward: 108.64179390932276)\n",
      "Finished episode 222 after 201 timesteps (reward: 128.91024799044465)\n",
      "Finished episode 223 after 201 timesteps (reward: 135.55133685864968)\n",
      "Finished episode 224 after 201 timesteps (reward: 132.46053012261117)\n",
      "Finished episode 225 after 201 timesteps (reward: 136.29281019649852)\n",
      "Finished episode 226 after 201 timesteps (reward: 106.76273821224574)\n",
      "Finished episode 227 after 201 timesteps (reward: 148.44075458883674)\n",
      "Finished episode 228 after 201 timesteps (reward: 112.07675127308178)\n",
      "Finished episode 229 after 201 timesteps (reward: 102.70384020234839)\n",
      "Finished episode 230 after 201 timesteps (reward: 143.72224609471075)\n",
      "Finished episode 231 after 201 timesteps (reward: 102.56442306762398)\n",
      "Finished episode 232 after 201 timesteps (reward: 118.18327765712074)\n",
      "Finished episode 233 after 201 timesteps (reward: 122.60021526247915)\n",
      "Finished episode 234 after 201 timesteps (reward: 153.10788141348996)\n",
      "Finished episode 235 after 201 timesteps (reward: 127.60110540596128)\n",
      "Finished episode 236 after 201 timesteps (reward: 105.94820769313199)\n",
      "Finished episode 237 after 201 timesteps (reward: 130.80554811915314)\n",
      "Finished episode 238 after 201 timesteps (reward: 141.2940297360544)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 239 after 201 timesteps (reward: 114.40075168494174)\n",
      "Finished episode 240 after 201 timesteps (reward: 122.77306948458325)\n",
      "Finished episode 241 after 201 timesteps (reward: 137.58403822495362)\n",
      "Finished episode 242 after 201 timesteps (reward: 126.49806020628058)\n",
      "Finished episode 243 after 201 timesteps (reward: 166.05486694292375)\n",
      "Finished episode 244 after 201 timesteps (reward: 139.73987625529682)\n",
      "Finished episode 245 after 201 timesteps (reward: 134.6638286436461)\n",
      "Finished episode 246 after 201 timesteps (reward: 113.18509921610134)\n",
      "Finished episode 247 after 201 timesteps (reward: 160.01353980575232)\n",
      "Finished episode 248 after 201 timesteps (reward: 110.45102882131445)\n",
      "Finished episode 249 after 201 timesteps (reward: 100.45118003686422)\n",
      "Finished episode 250 after 201 timesteps (reward: 124.55741068903147)\n",
      "Finished episode 251 after 201 timesteps (reward: 171.17273396699238)\n",
      "Finished episode 252 after 201 timesteps (reward: 159.04671141882292)\n",
      "Finished episode 253 after 201 timesteps (reward: 159.0138950556111)\n",
      "Finished episode 254 after 201 timesteps (reward: 139.84675433711274)\n",
      "Finished episode 255 after 201 timesteps (reward: 144.0763175551098)\n",
      "Finished episode 256 after 201 timesteps (reward: 84.52843818139357)\n",
      "Finished episode 257 after 201 timesteps (reward: 139.59951444104053)\n",
      "Finished episode 258 after 201 timesteps (reward: 163.38507629686734)\n",
      "Finished episode 259 after 201 timesteps (reward: 97.4572117220677)\n",
      "Finished episode 260 after 201 timesteps (reward: 116.11191117326794)\n",
      "Finished episode 261 after 201 timesteps (reward: 118.55697778473184)\n",
      "Finished episode 262 after 201 timesteps (reward: 151.24488125952948)\n",
      "Finished episode 263 after 201 timesteps (reward: 165.94295969990483)\n",
      "Finished episode 264 after 201 timesteps (reward: 143.89959989158126)\n",
      "Finished episode 265 after 201 timesteps (reward: 93.6112876843683)\n",
      "Finished episode 266 after 201 timesteps (reward: 104.95912190436276)\n",
      "Finished episode 267 after 201 timesteps (reward: 178.1655697516536)\n",
      "Finished episode 268 after 201 timesteps (reward: 119.43003534556176)\n",
      "Finished episode 269 after 201 timesteps (reward: 98.26952413722738)\n",
      "Finished episode 270 after 201 timesteps (reward: 140.88128334975545)\n",
      "Finished episode 271 after 201 timesteps (reward: 137.9867217243867)\n",
      "Finished episode 272 after 201 timesteps (reward: 93.02452235845803)\n",
      "Finished episode 273 after 201 timesteps (reward: 121.07337355164073)\n",
      "Finished episode 274 after 201 timesteps (reward: 160.657546121342)\n",
      "Finished episode 275 after 201 timesteps (reward: 129.57318397441597)\n",
      "Finished episode 276 after 201 timesteps (reward: 153.30283091573037)\n",
      "Finished episode 277 after 201 timesteps (reward: 127.16271505085207)\n",
      "Finished episode 278 after 201 timesteps (reward: 140.37079607316346)\n",
      "Finished episode 279 after 201 timesteps (reward: 111.49523191699238)\n",
      "Finished episode 280 after 201 timesteps (reward: 143.58858743998005)\n",
      "Finished episode 281 after 201 timesteps (reward: 133.31148775636458)\n",
      "Finished episode 282 after 201 timesteps (reward: 165.24395396306)\n",
      "Finished episode 283 after 201 timesteps (reward: 106.8715100075587)\n",
      "Finished episode 284 after 201 timesteps (reward: 130.81020502655718)\n",
      "Finished episode 285 after 201 timesteps (reward: 101.06921904133007)\n",
      "Finished episode 286 after 201 timesteps (reward: 140.77817401118486)\n",
      "Finished episode 287 after 201 timesteps (reward: 103.86715382127285)\n",
      "Finished episode 288 after 201 timesteps (reward: 122.59272821897623)\n",
      "Finished episode 289 after 201 timesteps (reward: 112.3088967586221)\n",
      "Finished episode 290 after 201 timesteps (reward: 184.86565341530476)\n",
      "Finished episode 291 after 201 timesteps (reward: 146.44503977687535)\n",
      "Finished episode 292 after 201 timesteps (reward: 103.91139882130526)\n",
      "Finished episode 293 after 201 timesteps (reward: 84.69350173033257)\n",
      "Finished episode 294 after 201 timesteps (reward: 140.00651212651928)\n",
      "Finished episode 295 after 201 timesteps (reward: 109.73620783562042)\n",
      "Finished episode 296 after 201 timesteps (reward: 107.77149622820764)\n",
      "Finished episode 297 after 201 timesteps (reward: 182.75347502947935)\n",
      "Finished episode 298 after 201 timesteps (reward: 139.22380365446537)\n",
      "Finished episode 299 after 201 timesteps (reward: 148.39703229608085)\n",
      "Finished episode 300 after 201 timesteps (reward: 97.97699708161456)\n",
      "Finished episode 301 after 201 timesteps (reward: 148.13010200325354)\n",
      "Finished episode 302 after 201 timesteps (reward: 131.09344500888463)\n",
      "Finished episode 303 after 201 timesteps (reward: 111.67533487789424)\n",
      "Finished episode 304 after 201 timesteps (reward: 156.65834956581696)\n",
      "Finished episode 305 after 201 timesteps (reward: 85.70881550924764)\n",
      "Finished episode 306 after 201 timesteps (reward: 121.44377690780907)\n",
      "Finished episode 307 after 201 timesteps (reward: 101.95350537326578)\n",
      "Finished episode 308 after 201 timesteps (reward: 129.27460653303072)\n",
      "Finished episode 309 after 201 timesteps (reward: 89.52918584135378)\n",
      "Finished episode 310 after 201 timesteps (reward: 113.18214192735405)\n",
      "Finished episode 311 after 201 timesteps (reward: 142.51382687535167)\n",
      "Finished episode 312 after 201 timesteps (reward: 162.286613854272)\n",
      "Finished episode 313 after 201 timesteps (reward: 127.2608893342572)\n",
      "Finished episode 314 after 201 timesteps (reward: 138.00986432687134)\n",
      "Finished episode 315 after 201 timesteps (reward: 186.29541398687607)\n",
      "Finished episode 316 after 201 timesteps (reward: 127.21476138278463)\n",
      "Finished episode 317 after 201 timesteps (reward: 112.06499566611498)\n",
      "Finished episode 318 after 201 timesteps (reward: 116.06028370938712)\n",
      "Finished episode 319 after 201 timesteps (reward: 130.70594183863764)\n",
      "Finished episode 320 after 201 timesteps (reward: 132.90188960717293)\n",
      "Finished episode 321 after 201 timesteps (reward: 88.69534327328128)\n",
      "Finished episode 322 after 201 timesteps (reward: 127.61992403494763)\n",
      "Finished episode 323 after 201 timesteps (reward: 94.63063100001659)\n",
      "Finished episode 324 after 201 timesteps (reward: 164.45712145536328)\n",
      "Finished episode 325 after 201 timesteps (reward: 133.93078141774583)\n",
      "Finished episode 326 after 201 timesteps (reward: 93.09684662468811)\n",
      "Finished episode 327 after 201 timesteps (reward: 119.80233519249572)\n",
      "Finished episode 328 after 201 timesteps (reward: 104.6082603493049)\n",
      "Finished episode 329 after 201 timesteps (reward: 121.33123854299019)\n",
      "Finished episode 330 after 201 timesteps (reward: 207.64678635997615)\n",
      "Finished episode 331 after 201 timesteps (reward: 169.70358570635315)\n",
      "Finished episode 332 after 201 timesteps (reward: 197.69822240718491)\n",
      "Finished episode 333 after 201 timesteps (reward: 145.3431807302355)\n",
      "Finished episode 334 after 201 timesteps (reward: 116.03760803648079)\n",
      "Finished episode 335 after 201 timesteps (reward: 135.98428881739238)\n",
      "Finished episode 336 after 201 timesteps (reward: 112.1040613974356)\n",
      "Finished episode 337 after 201 timesteps (reward: 107.99014720858571)\n",
      "Finished episode 338 after 201 timesteps (reward: 102.63826760825899)\n",
      "Finished episode 339 after 201 timesteps (reward: 112.06128610335517)\n",
      "Finished episode 340 after 201 timesteps (reward: 157.32652180663)\n",
      "Finished episode 341 after 201 timesteps (reward: 153.06599841526366)\n",
      "Finished episode 342 after 201 timesteps (reward: 128.5697092614338)\n",
      "Finished episode 343 after 201 timesteps (reward: 99.93659030397171)\n",
      "Finished episode 344 after 201 timesteps (reward: 130.41350402487996)\n",
      "Finished episode 345 after 201 timesteps (reward: 136.3409991740404)\n",
      "Finished episode 346 after 201 timesteps (reward: 105.40049628723612)\n",
      "Finished episode 347 after 201 timesteps (reward: 136.80574011274888)\n",
      "Finished episode 348 after 201 timesteps (reward: 186.7361477399871)\n",
      "Finished episode 349 after 201 timesteps (reward: 125.03536585975871)\n",
      "Finished episode 350 after 201 timesteps (reward: 95.41138762846057)\n",
      "Finished episode 351 after 201 timesteps (reward: 142.9258700030098)\n",
      "Finished episode 352 after 201 timesteps (reward: 137.85758812810732)\n",
      "Finished episode 353 after 201 timesteps (reward: 93.16853188591348)\n",
      "Finished episode 354 after 201 timesteps (reward: 133.42264421221205)\n",
      "Finished episode 355 after 201 timesteps (reward: 157.73169002553797)\n",
      "Finished episode 356 after 201 timesteps (reward: 114.20171072887447)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 357 after 201 timesteps (reward: 116.93298041582372)\n",
      "Finished episode 358 after 201 timesteps (reward: 115.80273696499046)\n",
      "Finished episode 359 after 201 timesteps (reward: 104.08217844437311)\n",
      "Finished episode 360 after 201 timesteps (reward: 126.88502482220584)\n",
      "Finished episode 361 after 201 timesteps (reward: 140.39308778887843)\n",
      "Finished episode 362 after 201 timesteps (reward: 122.77217772053616)\n",
      "Finished episode 363 after 201 timesteps (reward: 125.21118010684134)\n",
      "Finished episode 364 after 201 timesteps (reward: 107.19594936361091)\n",
      "Finished episode 365 after 201 timesteps (reward: 149.27245678330559)\n",
      "Finished episode 366 after 201 timesteps (reward: 81.89504692419112)\n",
      "Finished episode 367 after 201 timesteps (reward: 139.14346855569445)\n",
      "Finished episode 368 after 201 timesteps (reward: 128.1617954423135)\n",
      "Finished episode 369 after 201 timesteps (reward: 122.86968315689731)\n",
      "Finished episode 370 after 201 timesteps (reward: 132.22457833735186)\n",
      "Finished episode 371 after 201 timesteps (reward: 184.7861141236261)\n",
      "Finished episode 372 after 201 timesteps (reward: 113.73400340703998)\n",
      "Finished episode 373 after 201 timesteps (reward: 93.3459920532726)\n",
      "Finished episode 374 after 201 timesteps (reward: 123.62665718751205)\n",
      "Finished episode 375 after 201 timesteps (reward: 144.18694645702752)\n",
      "Finished episode 376 after 201 timesteps (reward: 104.76159068913707)\n",
      "Finished episode 377 after 201 timesteps (reward: 140.03303646232527)\n",
      "Finished episode 378 after 201 timesteps (reward: 126.188454891236)\n",
      "Finished episode 379 after 201 timesteps (reward: 144.53055025579764)\n",
      "Finished episode 380 after 201 timesteps (reward: 105.49151126467137)\n",
      "Finished episode 381 after 201 timesteps (reward: 128.30871010121825)\n",
      "Finished episode 382 after 201 timesteps (reward: 114.41215621878251)\n",
      "Finished episode 383 after 201 timesteps (reward: 156.8274211231194)\n",
      "Finished episode 384 after 201 timesteps (reward: 131.2519350075543)\n",
      "Finished episode 385 after 201 timesteps (reward: 127.4538933445596)\n",
      "Finished episode 386 after 201 timesteps (reward: 132.15167646997423)\n",
      "Finished episode 387 after 201 timesteps (reward: 96.79484733164975)\n",
      "Finished episode 388 after 201 timesteps (reward: 146.87192070609044)\n",
      "Finished episode 389 after 201 timesteps (reward: 103.229463111366)\n",
      "Finished episode 390 after 201 timesteps (reward: 163.51687522630698)\n",
      "Finished episode 391 after 201 timesteps (reward: 161.75758810876468)\n",
      "Finished episode 392 after 201 timesteps (reward: 81.92872804982731)\n",
      "Finished episode 393 after 201 timesteps (reward: 129.51555793640551)\n",
      "Finished episode 394 after 201 timesteps (reward: 110.55289637428307)\n",
      "Finished episode 395 after 201 timesteps (reward: 98.54177910952187)\n",
      "Finished episode 396 after 201 timesteps (reward: 111.01707973224202)\n",
      "Finished episode 397 after 201 timesteps (reward: 156.5587146950342)\n",
      "Finished episode 398 after 201 timesteps (reward: 121.32712461207286)\n",
      "Finished episode 399 after 201 timesteps (reward: 132.38483148083327)\n",
      "Finished episode 400 after 201 timesteps (reward: 107.10164940001894)\n",
      "Finished episode 401 after 201 timesteps (reward: 128.73347863254003)\n",
      "Finished episode 402 after 201 timesteps (reward: 98.75546298585368)\n",
      "Finished episode 403 after 201 timesteps (reward: 180.89205991296058)\n",
      "Finished episode 404 after 201 timesteps (reward: 99.49561387212067)\n",
      "Finished episode 405 after 201 timesteps (reward: 120.70102801182308)\n",
      "Finished episode 406 after 201 timesteps (reward: 78.36837986297618)\n",
      "Finished episode 407 after 201 timesteps (reward: 177.0474429732389)\n",
      "Finished episode 408 after 201 timesteps (reward: 111.66382549415609)\n",
      "Finished episode 409 after 201 timesteps (reward: 123.05896152679068)\n",
      "Finished episode 410 after 201 timesteps (reward: 249.6516060783463)\n",
      "Finished episode 411 after 201 timesteps (reward: 105.33921998875175)\n",
      "Finished episode 412 after 201 timesteps (reward: 95.8772887016566)\n",
      "Finished episode 413 after 201 timesteps (reward: 149.38274108840588)\n",
      "Finished episode 414 after 201 timesteps (reward: 158.27908242861525)\n",
      "Finished episode 415 after 201 timesteps (reward: 131.0196755989735)\n",
      "Finished episode 416 after 201 timesteps (reward: 143.5978855456623)\n",
      "Finished episode 417 after 201 timesteps (reward: 118.54705737995899)\n",
      "Finished episode 418 after 201 timesteps (reward: 106.55501633193174)\n",
      "Finished episode 419 after 201 timesteps (reward: 133.4623585277879)\n",
      "Finished episode 420 after 201 timesteps (reward: 168.37418135297128)\n",
      "Finished episode 421 after 201 timesteps (reward: 156.89383166657396)\n",
      "Finished episode 422 after 201 timesteps (reward: 112.5939807452554)\n",
      "Finished episode 423 after 201 timesteps (reward: 96.51590406325515)\n",
      "Finished episode 424 after 201 timesteps (reward: 88.4069276153913)\n",
      "Finished episode 425 after 201 timesteps (reward: 104.40368365784988)\n",
      "Finished episode 426 after 201 timesteps (reward: 110.35165964439868)\n",
      "Finished episode 427 after 201 timesteps (reward: 108.35173860664007)\n",
      "Finished episode 428 after 201 timesteps (reward: 115.87935597898351)\n",
      "Finished episode 429 after 201 timesteps (reward: 120.98969801446114)\n",
      "Finished episode 430 after 201 timesteps (reward: 118.11395591421929)\n",
      "Finished episode 431 after 201 timesteps (reward: 160.14565287673057)\n",
      "Finished episode 432 after 201 timesteps (reward: 153.40930581407713)\n",
      "Finished episode 433 after 201 timesteps (reward: 108.3998067534135)\n",
      "Finished episode 434 after 201 timesteps (reward: 138.31611940658615)\n",
      "Finished episode 435 after 201 timesteps (reward: 125.61073300915889)\n",
      "Finished episode 436 after 201 timesteps (reward: 98.96037169943041)\n",
      "Finished episode 437 after 201 timesteps (reward: 104.57467052937973)\n",
      "Finished episode 438 after 201 timesteps (reward: 130.00075099502737)\n",
      "Finished episode 439 after 201 timesteps (reward: 147.33341954527984)\n",
      "Finished episode 440 after 201 timesteps (reward: 142.8280741974566)\n",
      "Finished episode 441 after 201 timesteps (reward: 97.89920168381737)\n",
      "Finished episode 442 after 201 timesteps (reward: 138.13322866431756)\n",
      "Finished episode 443 after 201 timesteps (reward: 118.31705006421436)\n",
      "Finished episode 444 after 201 timesteps (reward: 101.74841426228535)\n",
      "Finished episode 445 after 201 timesteps (reward: 120.08358996839542)\n",
      "Finished episode 446 after 201 timesteps (reward: 129.32506262686104)\n",
      "Finished episode 447 after 201 timesteps (reward: 109.69301618314655)\n",
      "Finished episode 448 after 201 timesteps (reward: 133.35956556391736)\n",
      "Finished episode 449 after 201 timesteps (reward: 148.0583288738842)\n",
      "Finished episode 450 after 201 timesteps (reward: 124.65243337384247)\n",
      "Finished episode 451 after 201 timesteps (reward: 101.38667942443733)\n",
      "Finished episode 452 after 201 timesteps (reward: 144.64625351667036)\n",
      "Finished episode 453 after 201 timesteps (reward: 94.54802645444697)\n",
      "Finished episode 454 after 201 timesteps (reward: 119.00047243671348)\n",
      "Finished episode 455 after 201 timesteps (reward: 129.82680406427227)\n",
      "Finished episode 456 after 201 timesteps (reward: 130.71586534028887)\n",
      "Finished episode 457 after 201 timesteps (reward: 132.34474549907193)\n",
      "Finished episode 458 after 201 timesteps (reward: 175.95576986195724)\n",
      "Finished episode 459 after 201 timesteps (reward: 115.72055226325112)\n",
      "Finished episode 460 after 201 timesteps (reward: 148.62984148042096)\n",
      "Finished episode 461 after 201 timesteps (reward: 134.07531588369096)\n",
      "Finished episode 462 after 201 timesteps (reward: 130.66607407375793)\n",
      "Finished episode 463 after 201 timesteps (reward: 109.78255872725072)\n",
      "Finished episode 464 after 201 timesteps (reward: 118.54334025220686)\n",
      "Finished episode 465 after 201 timesteps (reward: 106.15428650408624)\n",
      "Finished episode 466 after 201 timesteps (reward: 112.39707597565184)\n",
      "Finished episode 467 after 201 timesteps (reward: 223.20527976877506)\n",
      "Finished episode 468 after 201 timesteps (reward: 115.61519026231785)\n",
      "Finished episode 469 after 201 timesteps (reward: 79.38669932370564)\n",
      "Finished episode 470 after 201 timesteps (reward: 127.14375399989517)\n",
      "Finished episode 471 after 201 timesteps (reward: 119.36394270992338)\n",
      "Finished episode 472 after 201 timesteps (reward: 90.35545959020872)\n",
      "Finished episode 473 after 201 timesteps (reward: 85.87716816044527)\n",
      "Finished episode 474 after 201 timesteps (reward: 164.9280870293578)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 475 after 201 timesteps (reward: 176.54712097700354)\n",
      "Finished episode 476 after 201 timesteps (reward: 86.28206994836042)\n",
      "Finished episode 477 after 201 timesteps (reward: 114.337631456259)\n",
      "Finished episode 478 after 201 timesteps (reward: 122.86159919837327)\n",
      "Finished episode 479 after 201 timesteps (reward: 174.17475463260524)\n",
      "Finished episode 480 after 201 timesteps (reward: 114.31384018618736)\n",
      "Finished episode 481 after 201 timesteps (reward: 166.6827841320274)\n",
      "Finished episode 482 after 201 timesteps (reward: 174.05678279462336)\n",
      "Finished episode 483 after 201 timesteps (reward: 114.98023802345169)\n",
      "Finished episode 484 after 201 timesteps (reward: 136.40223345896507)\n",
      "Finished episode 485 after 201 timesteps (reward: 157.5116590775483)\n",
      "Finished episode 486 after 201 timesteps (reward: 92.25498843206977)\n",
      "Finished episode 487 after 201 timesteps (reward: 175.363323722127)\n",
      "Finished episode 488 after 201 timesteps (reward: 85.48008321664796)\n",
      "Finished episode 489 after 201 timesteps (reward: 142.1153379808953)\n",
      "Finished episode 490 after 201 timesteps (reward: 172.13838696350965)\n",
      "Finished episode 491 after 201 timesteps (reward: 112.74283511456325)\n",
      "Finished episode 492 after 201 timesteps (reward: 128.60979653515847)\n",
      "Finished episode 493 after 201 timesteps (reward: 114.34135488008745)\n",
      "Finished episode 494 after 201 timesteps (reward: 137.92370642723256)\n",
      "Finished episode 495 after 201 timesteps (reward: 175.668573004435)\n",
      "Finished episode 496 after 201 timesteps (reward: 136.21379561225768)\n",
      "Finished episode 497 after 201 timesteps (reward: 125.7080728606585)\n",
      "Finished episode 498 after 201 timesteps (reward: 99.16131171571084)\n",
      "Finished episode 499 after 201 timesteps (reward: 132.56592107185494)\n",
      "Finished episode 500 after 201 timesteps (reward: 120.57988761829748)\n"
     ]
    }
   ],
   "source": [
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "\n",
    "env = TradeEnvironment(data)\n",
    "num_stocks = env.num_stocks\n",
    "# Create a Proximal Policy Optimization agent\n",
    "agent = PPOAgent(\n",
    "    states_spec=dict(type='float', shape=(num_stocks,)),\n",
    "    actions_spec=dict(type='float', shape=(num_stocks,)),\n",
    "    network_spec=[\n",
    "        dict(type=\"flatten\"),\n",
    "        dict(type='dense', size=64),\n",
    "        dict(type='dense', size=64)\n",
    "    ],\n",
    "    distributions_spec=dict(action=dict(type=\"dirichlet\",\n",
    "                                        alphas=np.zeros(num_stocks),\n",
    "                                        min_value=0.,\n",
    "                                        max_value=1.)),\n",
    "    batch_size=10,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4\n",
    "    )\n",
    ")\n",
    "\n",
    "# Poll new state from client\n",
    "state = env.reset()\n",
    "\n",
    "runner = TradeRunner(\n",
    "        agent=agent,\n",
    "        environment=env,\n",
    "        repeat_actions=1\n",
    "    )\n",
    "\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.accumulated_pvs[-1]))\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=500, max_episode_timesteps=200, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3cb8cda128>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3cb01606d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(runner.accumulated_pvs_list[-12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.core.memories import Replay\n",
    "\n",
    "\n",
    "class SeqReplay(Replay):\n",
    "    \"\"\"\n",
    "    Replay memory to store observations and sample mini batches for training from.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, states_spec, actions_spec, capacity, random_sampling=True, beta=0.1):\n",
    "        self.beta = beta\n",
    "        super(SeqReplay, self).__init__(states_spec, actions_spec, capacity, random_sampling)\n",
    "        self.capacity = capacity\n",
    "        state_shape = list(state[\"shape\"])[1:]\n",
    "        self.length = state[\"shape\"][0]\n",
    "        self.states = {name: np.zeros((capacity,) + tuple(state_shape), dtype=util.np_dtype(state['type'])) for name, state in states_spec.items()}\n",
    "        self.internals = None\n",
    "        self.actions = {name: np.zeros((capacity,) + tuple(action['shape']), dtype=util.np_dtype(action['type'])) for name, action in actions_spec.items()}\n",
    "        self.terminal = np.zeros((capacity,), dtype=util.np_dtype('bool'))\n",
    "        self.reward = np.zeros((capacity,), dtype=util.np_dtype('float'))\n",
    "\n",
    "        self.size = 0\n",
    "        self.index = 0\n",
    "        self.random_sampling = random_sampling\n",
    "        \n",
    "    def _get_seq_index(self, X, index):\n",
    "        X_seq = []\n",
    "        for i in index:\n",
    "            idx = np.arange(i - self.length + 1, i + 1) % self.size\n",
    "            X_seq.append(X.take(idx, axis=0))\n",
    "        return np.array(X_seq)\n",
    "    \n",
    "    def _sample_index(self, shape, maximum, length, start_index):\n",
    "        assert maximum > length\n",
    "        num_cand = maximum - length + 1\n",
    "        samples = (np.random.geometric(self.beta, shape) - 1) % num_cand\n",
    "        samples = (start_index - samples) % self.capacity\n",
    "        return samples\n",
    "\n",
    "    def get_batch(self, batch_size, next_states=False, keep_terminal_states=True):\n",
    "        \"\"\"\n",
    "        Samples a batch of the specified size by selecting a random start/end point and returning\n",
    "        the contained sequence or random indices depending on the field 'random_sampling'.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: The batch size\n",
    "            next_states: A boolean flag indicating whether 'next_states' values should be included\n",
    "            keep_terminal_states: A boolean flag indicating whether to keep terminal states when\n",
    "                `next_states` are requested. In this case, the next state is not from the same episode\n",
    "                and should probably not be used to learn a model of the environment. However, if the\n",
    "                environment produces sparse rewards (i.e. only one reward at the end of the episode) we\n",
    "                cannot exclude terminal states, as otherwise there would never be a reward to learn from.\n",
    "        Returns: A dict containing states, actions, rewards, terminals, internal states (and next states)\n",
    "        \"\"\"\n",
    "        if self.random_sampling:\n",
    "            if next_states:\n",
    "                indices = self._sample_index(batch_size, self.size - 1, self.length, self.index - 1)\n",
    "                terminal = self.terminal.take(indices)\n",
    "                if not keep_terminal_states:\n",
    "                    while np.any(terminal):\n",
    "                        alternative = np.random.randint(self.size - 1, size=batch_size)\n",
    "                        indices = np.where(terminal, alternative, indices)\n",
    "                        terminal = self.terminal.take(indices)\n",
    "            else:\n",
    "                indices = self._sample_index(batch_size, self.size - 1, self.length, self.index - 1)\n",
    "\n",
    "            states = {name: self._get_seq_index(state, indices) for name, state in self.states.items()}\n",
    "            internals = [self._get_seq_index(internal, indices) for internal in self.internals]\n",
    "            actions = {name: self._get_seq_index(action, indices) for name, action in self.actions.items()}\n",
    "            terminal = self._get_seq_index(self.terminal, indices)\n",
    "            reward = self._get_seq_index(self.reward, indices)\n",
    "            if next_states:\n",
    "                indices = (indices + 1) % self.capacity\n",
    "                next_states = {name: self._get_seq_index(state, indices) for name, state in self.states.items()}\n",
    "                next_internals = [self._get_seq_index(internal, indices) for internal in self.internals]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        batch = dict(states=states, internals=internals, actions=actions, terminal=terminal, reward=reward)\n",
    "        if next_states:\n",
    "            batch['next_states'] = next_states\n",
    "            batch['next_internals'] = next_internals\n",
    "        return batch\n",
    "\n",
    "    def update_batch(self, loss_per_instance):\n",
    "        pass\n",
    "\n",
    "    def set_memory(self, states, internals, actions, terminal, reward):\n",
    "        \"\"\"\n",
    "        Convenience function to set whole batches as memory content to bypass\n",
    "        calling the insert function for every single experience.\n",
    "        Args:\n",
    "            states:\n",
    "            internals:\n",
    "            actions:\n",
    "            terminal:\n",
    "            reward:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.size = len(terminal)\n",
    "\n",
    "        if len(terminal) == self.capacity:\n",
    "            # Assign directly if capacity matches size.\n",
    "            for name, state in states.items():\n",
    "                self.states[name] = np.asarray(state)\n",
    "            self.internals = [np.asarray(internal) for internal in internals]\n",
    "            for name, action in actions.items():\n",
    "                self.actions[name] = np.asarray(action)\n",
    "            self.terminal = np.asarray(terminal)\n",
    "            self.reward = np.asarray(reward)\n",
    "            # Filled capacity to point of index wrap\n",
    "            self.index = 0\n",
    "\n",
    "        else:\n",
    "            # Otherwise partial assignment.\n",
    "            if self.internals is None and internals is not None:\n",
    "                self.internals = [np.zeros((self.capacity,) + internal.shape, internal.dtype) for internal\n",
    "                                  in internals]\n",
    "\n",
    "            for name, state in states.items():\n",
    "                self.states[name][:len(state)] = state\n",
    "            for n, internal in enumerate(internals):\n",
    "                self.internals[n][:len(internal)] = internal\n",
    "            for name, action in actions.items():\n",
    "                self.actions[name][:len(action)] = action\n",
    "            self.terminal[:len(terminal)] = terminal\n",
    "            self.reward[:len(reward)] = reward\n",
    "            self.index = len(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorforce import util, TensorForceError\n",
    "from tensorforce.models import DistributionModel\n",
    "from tensorforce.core.networks import Network\n",
    "from tensorforce.core.optimizers import Synchronization\n",
    "\n",
    "\n",
    "class DDPGModel(DistributionModel):\n",
    "    \"\"\"\n",
    "    Q-value model.\n",
    "    \"\"\"\n",
    "    def tf_actions_and_internals(self, states, internals, update, deterministic):\n",
    "        \"\"\"You have to implement to have connections between actions and input\"\"\"\n",
    "        embedding, internals = self.network.apply(x=states,\n",
    "                                                  internals=internals,\n",
    "                                                  update=update,\n",
    "                                                  return_internals=True)\n",
    "        actions = dict()\n",
    "        for name, distribution in self.distributions.items():\n",
    "            distr_params = distribution.parameterize(x=embedding)\n",
    "            actions[name] = distribution.sample(distr_params=distr_params, deterministic=deterministic)\n",
    "        return actions, internals\n",
    "    \n",
    "    def tf_discounted_cumulative_reward(self, terminal, reward, discount, final_reward=0.0):\n",
    "        \"\"\"\n",
    "        Creates the TensorFlow operations for calculating the discounted cumulative rewards\n",
    "        for a given sequence of rewards.\n",
    "        Args:\n",
    "            terminal: Terminal boolean tensor.\n",
    "            reward: Reward tensor.\n",
    "            discount: Discount factor.\n",
    "            final_reward: Last reward value in the sequence.\n",
    "        Returns:\n",
    "            Discounted cumulative reward tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: n-step cumulative reward (particularly for envs without terminal)\n",
    "\n",
    "        def cumulate(cumulative, reward_and_terminal):\n",
    "            rew, term = reward_and_terminal\n",
    "            return tf.where(\n",
    "                condition=term,\n",
    "                x=rew,\n",
    "                y=(rew + cumulative * discount)\n",
    "            )\n",
    "\n",
    "        # Reverse since reward cumulation is calculated right-to-left, but tf.scan only works left-to-right\n",
    "        reward = tf.reverse(tensor=reward, axis=(0,))\n",
    "        terminal = tf.reverse(tensor=terminal, axis=(0,))\n",
    "\n",
    "        reward = tf.scan(fn=cumulate, elems=(reward, terminal), initializer=final_reward)\n",
    "\n",
    "        return tf.reverse(tensor=reward, axis=(0,))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        states_spec,\n",
    "        actions_spec,\n",
    "        network_spec,\n",
    "        device,\n",
    "        session_config,\n",
    "        scope,\n",
    "        saver_spec,\n",
    "        summary_spec,\n",
    "        distributed_spec,\n",
    "        optimizer,\n",
    "        discount,\n",
    "        variable_noise,\n",
    "        states_preprocessing_spec,\n",
    "        explorations_spec,\n",
    "        reward_preprocessing_spec,\n",
    "        distributions_spec,\n",
    "        entropy_regularization,\n",
    "        target_sync_frequency,\n",
    "        target_update_weight,\n",
    "        double_q_model,\n",
    "        huber_loss,\n",
    "        # TEMP: Random sampling fix\n",
    "        random_sampling_fix\n",
    "    ):\n",
    "        self.target_sync_frequency = target_sync_frequency\n",
    "        self.target_update_weight = target_update_weight\n",
    "\n",
    "        self.double_q_model = double_q_model\n",
    "\n",
    "        assert huber_loss is None or huber_loss > 0.0\n",
    "        self.huber_loss = huber_loss\n",
    "\n",
    "        # TEMP: Random sampling fix\n",
    "        self.random_sampling_fix = random_sampling_fix\n",
    "\n",
    "        super(QModel, self).__init__(\n",
    "            states_spec=states_spec,\n",
    "            actions_spec=actions_spec,\n",
    "            network_spec=network_spec,\n",
    "            device=device,\n",
    "            session_config=session_config,\n",
    "            scope=scope,\n",
    "            saver_spec=saver_spec,\n",
    "            summary_spec=summary_spec,\n",
    "            distributed_spec=distributed_spec,\n",
    "            optimizer=optimizer,\n",
    "            discount=discount,\n",
    "            variable_noise=variable_noise,\n",
    "            states_preprocessing_spec=states_preprocessing_spec,\n",
    "            explorations_spec=explorations_spec,\n",
    "            reward_preprocessing_spec=reward_preprocessing_spec,\n",
    "            distributions_spec=distributions_spec,\n",
    "            entropy_regularization=entropy_regularization,\n",
    "        )\n",
    "\n",
    "    def initialize(self, custom_getter):\n",
    "        super(QModel, self).initialize(custom_getter)\n",
    "\n",
    "        # TEMP: Random sampling fix\n",
    "        if self.random_sampling_fix:\n",
    "            self.next_states_input = dict()\n",
    "            for name, state in self.states_spec.items():\n",
    "                self.next_states_input[name] = tf.placeholder(\n",
    "                    dtype=util.tf_dtype(state['type']),\n",
    "                    shape=(None,) + tuple(state['shape']),\n",
    "                    name=('next-' + name)\n",
    "                )\n",
    "\n",
    "        # Target network\n",
    "        self.target_network = Network.from_spec(\n",
    "            spec=self.network_spec,\n",
    "            kwargs=dict(scope='target', summary_labels=self.summary_labels)\n",
    "        )\n",
    "\n",
    "        # Target network optimizer\n",
    "        self.target_optimizer = Synchronization(\n",
    "            sync_frequency=self.target_sync_frequency,\n",
    "            update_weight=self.target_update_weight\n",
    "        )\n",
    "\n",
    "        # Target network distributions\n",
    "        self.target_distributions = self.create_distributions()\n",
    "\n",
    "    def tf_q_value(self, embedding, distr_params, action, name):\n",
    "        # Mainly for NAF.\n",
    "        return self.distributions[name].state_action_value(distr_params=distr_params, action=action)\n",
    "\n",
    "    def tf_q_delta(self, q_value, next_q_value, terminal, reward):\n",
    "        \"\"\"\n",
    "        Creates the deltas (or advantage) of the Q values.\n",
    "        :return: A list of deltas per action\n",
    "        \"\"\"\n",
    "        for _ in range(util.rank(q_value) - 1):\n",
    "            terminal = tf.expand_dims(input=terminal, axis=1)\n",
    "            reward = tf.expand_dims(input=reward, axis=1)\n",
    "\n",
    "        multiples = (1,) + util.shape(q_value)[1:]\n",
    "        terminal = tf.tile(input=terminal, multiples=multiples)\n",
    "        reward = tf.tile(input=reward, multiples=multiples)\n",
    "\n",
    "        zeros = tf.zeros_like(tensor=next_q_value)\n",
    "        next_q_value = tf.where(condition=terminal, x=zeros, y=(self.discount * next_q_value))\n",
    "\n",
    "        return reward + next_q_value - q_value  # tf.stop_gradient(q_target)\n",
    "\n",
    "    def tf_loss_per_instance(self, states, internals, actions, terminal, reward, update):\n",
    "        # TEMP: Random sampling fix\n",
    "        if self.random_sampling_fix:\n",
    "            next_states = {name: tf.identity(input=state) for name, state in self.next_states_input.items()}\n",
    "            next_states = self.fn_preprocess_states(states=next_states)\n",
    "            next_states = {name: tf.stop_gradient(input=state) for name, state in next_states.items()}\n",
    "\n",
    "            embedding, next_internals = self.network.apply(\n",
    "                x=states,\n",
    "                internals=internals,\n",
    "                update=update,\n",
    "                return_internals=True\n",
    "            )\n",
    "\n",
    "            # Both networks can use the same internals, could that be a problem?\n",
    "            # Otherwise need to handle internals indices correctly everywhere\n",
    "            target_embedding = self.target_network.apply(\n",
    "                x=next_states,\n",
    "                internals=next_internals,\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            embedding = self.network.apply(\n",
    "                x={name: state[:-1] for name, state in states.items()},\n",
    "                internals=[internal[:-1] for internal in internals],\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "            # Both networks can use the same internals, could that be a problem?\n",
    "            # Otherwise need to handle internals indices correctly everywhere\n",
    "            target_embedding = self.target_network.apply(\n",
    "                x={name: state[1:] for name, state in states.items()},\n",
    "                internals=[internal[1:] for internal in internals],\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "            actions = {name: action[:-1] for name, action in actions.items()}\n",
    "            terminal = terminal[:-1]\n",
    "            reward = reward[:-1]\n",
    "\n",
    "        deltas = list()\n",
    "        for name, distribution in self.distributions.items():\n",
    "            target_distribution = self.target_distributions[name]\n",
    "\n",
    "            distr_params = distribution.parameterize(x=embedding)\n",
    "            target_distr_params = target_distribution.parameterize(x=target_embedding)\n",
    "\n",
    "            q_value = self.tf_q_value(embedding=embedding, distr_params=distr_params, action=actions[name], name=name)\n",
    "\n",
    "            if self.double_q_model:\n",
    "                action_taken = distribution.sample(distr_params=distr_params, deterministic=True)\n",
    "            else:\n",
    "                action_taken = target_distribution.sample(distr_params=target_distr_params, deterministic=True)\n",
    "\n",
    "            next_q_value = target_distribution.state_action_value(distr_params=target_distr_params, action=action_taken)\n",
    "\n",
    "            delta = self.tf_q_delta(q_value=q_value, next_q_value=next_q_value, terminal=terminal, reward=reward)\n",
    "\n",
    "            collapsed_size = util.prod(util.shape(delta)[1:])\n",
    "            delta = tf.reshape(tensor=delta, shape=(-1, collapsed_size))\n",
    "\n",
    "            deltas.append(delta)\n",
    "\n",
    "        # Surrogate loss as the mean squared error between actual observed rewards and expected rewards\n",
    "        loss_per_instance = tf.reduce_mean(input_tensor=tf.concat(values=deltas, axis=1), axis=1)\n",
    "\n",
    "        # Optional Huber loss\n",
    "        if self.huber_loss is not None and self.huber_loss > 0.0:\n",
    "            return tf.where(\n",
    "                condition=(tf.abs(x=loss_per_instance) <= self.huber_loss),\n",
    "                x=(0.5 * tf.square(x=loss_per_instance)),\n",
    "                y=(self.huber_loss * (tf.abs(x=loss_per_instance) - 0.5 * self.huber_loss))\n",
    "            )\n",
    "        else:\n",
    "            return tf.square(x=loss_per_instance)\n",
    "\n",
    "    def tf_optimization(self, states, internals, actions, terminal, reward, update):\n",
    "        optimization = super(QModel, self).tf_optimization(\n",
    "            states=states,\n",
    "            internals=internals,\n",
    "            actions=actions,\n",
    "            terminal=terminal,\n",
    "            reward=reward,\n",
    "            update=update\n",
    "        )\n",
    "\n",
    "        network_distributions_variables = self.get_distributions_variables(self.distributions)\n",
    "        target_distributions_variables = self.get_distributions_variables(self.target_distributions)\n",
    "\n",
    "        target_optimization = self.target_optimizer.minimize(\n",
    "            time=self.timestep,\n",
    "            variables=self.target_network.get_variables() + target_distributions_variables,\n",
    "            source_variables=self.network.get_variables() + network_distributions_variables\n",
    "        )\n",
    "\n",
    "        return tf.group(optimization, target_optimization)\n",
    "\n",
    "    def get_variables(self, include_non_trainable=False):\n",
    "        model_variables = super(QModel, self).get_variables(include_non_trainable=include_non_trainable)\n",
    "\n",
    "        if include_non_trainable:\n",
    "            # Target network and optimizer variables only included if 'include_non_trainable' set\n",
    "            target_variables = self.target_network.get_variables(include_non_trainable=include_non_trainable)\n",
    "            target_distributions_variables = self.get_distributions_variables(self.target_distributions)\n",
    "            target_optimizer_variables = self.target_optimizer.get_variables()\n",
    "\n",
    "            return model_variables + target_variables + target_optimizer_variables + target_distributions_variables\n",
    "\n",
    "        else:\n",
    "            return model_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import TensorForceError\n",
    "from tensorforce.agents import MemoryAgent\n",
    "from tensorforce.models import QModel\n",
    "\n",
    "\n",
    "class DDPGAgent(MemoryAgent):\n",
    "    \"\"\"\n",
    "    Deep-Q-Network agent (DQN). The piece de resistance of deep reinforcement learning as described by\n",
    "    [Minh et al. (2015)](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html). Includes\n",
    "    an option for double-DQN (DDQN; [van Hasselt et al., 2015](https://arxiv.org/abs/1509.06461))\n",
    "    DQN chooses from one of a number of discrete actions by taking the maximum Q-value\n",
    "    from the value function with one output neuron per available action. DQN uses a replay memory for experience\n",
    "    playback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        states_spec,\n",
    "        actions_spec,\n",
    "        network_spec,\n",
    "        device=None,\n",
    "        session_config=None,\n",
    "        scope='dqn',\n",
    "        saver_spec=None,\n",
    "        summary_spec=None,\n",
    "        distributed_spec=None,\n",
    "        optimizer=None,\n",
    "        discount=0.99,\n",
    "        variable_noise=None,\n",
    "        states_preprocessing_spec=None,\n",
    "        explorations_spec=None,\n",
    "        reward_preprocessing_spec=None,\n",
    "        distributions_spec=None,\n",
    "        entropy_regularization=None,\n",
    "        target_sync_frequency=10000,\n",
    "        target_update_weight=1.0,\n",
    "        double_q_model=False,\n",
    "        huber_loss=None,\n",
    "        batched_observe=None,\n",
    "        batch_size=32,\n",
    "        memory=None,\n",
    "        first_update=10000,\n",
    "        update_frequency=4,\n",
    "        repeat_update=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a Deep-Q agent.\n",
    "        Args:\n",
    "            states_spec: Dict containing at least one state definition. In the case of a single state,\n",
    "               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state\n",
    "               is a dict itself with a unique name as its key.\n",
    "            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`\n",
    "                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.\n",
    "            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments\n",
    "                such as activation or regularisation. Full examples are in the examples/configs folder.\n",
    "            device: Device string specifying model device.\n",
    "            session_config: optional tf.ConfigProto with additional desired session configurations\n",
    "            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).\n",
    "            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use\n",
    "                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies\n",
    "                if a model is initially loaded (set to True) from a file `file`.\n",
    "            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`\n",
    "                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values\n",
    "                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.\n",
    "            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`\n",
    "                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow\n",
    "                cluster spec.\n",
    "            optimizer: Dict specifying optimizer type and its optional parameters, typically a `learning_rate`.\n",
    "                Available optimizer types include standard TensorFlow optimizers, `natural_gradient`,\n",
    "                and `evolutionary`. Consult the optimizer test or example configurations for more.\n",
    "            discount: Float specifying reward discount factor.\n",
    "            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).\n",
    "            states_preprocessing_spec: Optional list of states preprocessors to apply to state  \n",
    "                (e.g. `image_resize`, `grayscale`).\n",
    "            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  \n",
    "                or Gaussian noise).\n",
    "            reward_preprocessing_spec: Optional dict specifying reward preprocessing.\n",
    "            distributions_spec: Optional dict specifying action distributions to override default distribution choices.\n",
    "                Must match action names.\n",
    "            entropy_regularization: Optional positive float specifying an entropy regularization value.\n",
    "            target_sync_frequency: Interval between optimization calls synchronizing the target network.\n",
    "            target_update_weight: Update weight, 1.0 meaning a full assignment to target network from training network.\n",
    "            huber_loss: Optional flat specifying Huber-loss clipping.\n",
    "            batched_observe: Optional int specifying how many observe calls are batched into one session run.\n",
    "                Without batching, throughput will be lower because every `observe` triggers a session invocation to\n",
    "                update rewards in the graph.\n",
    "            batch_size: Int specifying batch size used to sample from memory. Should be smaller than memory size.\n",
    "            memory: Dict describing memory via `type` (e.g. `replay`) and `capacity`.\n",
    "            first_update: Int describing at which time step the first update is performed. Should be larger\n",
    "                than batch size.\n",
    "            update_frequency: Int specifying number of observe steps to perform until an update is executed.\n",
    "            repeat_update: Int specifying how many update steps are performed per update, where each update step implies\n",
    "                sampling a batch from the memory and passing it to the model.\n",
    "        \"\"\"\n",
    "\n",
    "        if network_spec is None:\n",
    "            raise TensorForceError(\"No network_spec provided.\")\n",
    "\n",
    "        if optimizer is None:\n",
    "            self.optimizer = dict(\n",
    "                type='adam',\n",
    "                learning_rate=1e-3\n",
    "            )\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "        if memory is None:\n",
    "            memory = dict(\n",
    "                type='replay',\n",
    "                capacity=100000\n",
    "            )\n",
    "        else:\n",
    "            self.memory = memory\n",
    "\n",
    "        self.network_spec = network_spec\n",
    "        self.device = device\n",
    "        self.session_config = session_config\n",
    "        self.scope = scope\n",
    "        self.saver_spec = saver_spec\n",
    "        self.summary_spec = summary_spec\n",
    "        self.distributed_spec = distributed_spec\n",
    "        self.discount = discount\n",
    "        self.variable_noise = variable_noise\n",
    "        self.states_preprocessing_spec = states_preprocessing_spec\n",
    "        self.explorations_spec = explorations_spec\n",
    "        self.reward_preprocessing_spec = reward_preprocessing_spec\n",
    "        self.distributions_spec = distributions_spec\n",
    "        self.entropy_regularization = entropy_regularization\n",
    "        self.target_sync_frequency = target_sync_frequency\n",
    "        self.target_update_weight = target_update_weight\n",
    "        self.double_q_model = double_q_model\n",
    "        self.huber_loss = huber_loss\n",
    "\n",
    "        super(DQNAgent, self).__init__(\n",
    "            states_spec=states_spec,\n",
    "            actions_spec=actions_spec,\n",
    "            batched_observe=batched_observe,\n",
    "            batch_size=batch_size,\n",
    "            memory=memory,\n",
    "            first_update=first_update,\n",
    "            update_frequency=update_frequency,\n",
    "            repeat_update=repeat_update\n",
    "        )\n",
    "\n",
    "    def initialize_model(self):\n",
    "        return QModel(\n",
    "            states_spec=self.states_spec,\n",
    "            actions_spec=self.actions_spec,\n",
    "            network_spec=self.network_spec,\n",
    "            device=self.device,\n",
    "            session_config=self.session_config,\n",
    "            scope=self.scope,\n",
    "            saver_spec=self.saver_spec,\n",
    "            summary_spec=self.summary_spec,\n",
    "            distributed_spec=self.distributed_spec,\n",
    "            optimizer=self.optimizer,\n",
    "            discount=self.discount,\n",
    "            variable_noise=self.variable_noise,\n",
    "            states_preprocessing_spec=self.states_preprocessing_spec,\n",
    "            explorations_spec=self.explorations_spec,\n",
    "            reward_preprocessing_spec=self.reward_preprocessing_spec,\n",
    "            distributions_spec=self.distributions_spec,\n",
    "            entropy_regularization=self.entropy_regularization,\n",
    "            target_sync_frequency=self.target_sync_frequency,\n",
    "            target_update_weight=self.target_update_weight,\n",
    "            double_q_model=self.double_q_model,\n",
    "            huber_loss=self.huber_loss,\n",
    "            # TEMP: Random sampling fix\n",
    "            random_sampling_fix=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestClass(object):\n",
    "    capacity = 100\n",
    "    index = 120\n",
    "    beta = .001\n",
    "    def _sample_index(self, shape, maximum, length, start_index):\n",
    "        assert maximum > length\n",
    "        num_cand = maximum - length + 1\n",
    "        samples = (np.random.geometric(self.beta, shape) - 1) % num_cand\n",
    "        samples = (start_index - samples) % self.capacity\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = TestClass()\n",
    "samples = test._sample_index(1000, 99, 10, 98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.min(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = TestClass()\n",
    "priority = x.get_priority()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\"\\\n",
    "          .format(ep=r.episode, ts=r.episode_timestep, reward=r.episode_rewards[-1]))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.agents import agent\n",
    "\n",
    "\n",
    "config = Configuration(\n",
    "    memory=dict(\n",
    "        type='replay',\n",
    "        capacity=1000\n",
    "    ),\n",
    "    batch_size=8,\n",
    "    first_update=100,\n",
    "    target_sync_frequency=10\n",
    ")\n",
    "\n",
    "T = 10\n",
    "num_stock = 5\n",
    "\n",
    "# Network is an ordered list of layers\n",
    "network_spec = [dict(type='dense', size=32), dict(type='dense', size=32)]\n",
    "\n",
    "# Define a state\n",
    "states = dict(shape=(T, num_stock), type='float')\n",
    "\n",
    "# Define an action\n",
    "actions = dict(type='float', shape=(num_stock,))\n",
    "\n",
    "agent = DQNAgent(\n",
    "    states_spec=states,\n",
    "    actions_spec=actions,\n",
    "    network_spec=network_spec,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "class Agent(agent.Agent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(12).reshape((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.take(1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.agents import TRPOAgent\n",
    "\n",
    "agent = TRPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=[\n",
    "        dict(type='dense', size=64),\n",
    "        dict(type='dense', size=64)\n",
    "    ],\n",
    "    batch_size=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = PPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=network_spec,\n",
    "    batch_size=4096,\n",
    "    # BatchAgent\n",
    "    keep_last_timestep=True,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    # DistributionModel\n",
    "    distributions_spec=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode=None,\n",
    "    baseline=None,\n",
    "    baseline_optimizer=None,\n",
    "    gae_lambda=None,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    summary_spec=None,\n",
    "    distributed_spec=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[\"2017-09-22\"][\"Open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.iloc[-1][\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = pd.read_csv(\"data/Top100Cryptos/100 List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min(time_index[0], time_index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_data = []\n",
    "for name in data.keys():\n",
    "    num_data.append(data[name].shape[0])\n",
    "name_list = np.array(list(data.keys()))[np.argsort(num_data)[::-1]]\n",
    "big_names = name_list[:10]\n",
    "\n",
    "data_ = dict()\n",
    "for name in big_names:\n",
    "    data_[name] = data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ = dict()\n",
    "for name in big_names:\n",
    "    data_[name] = data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_index = set()\n",
    "\n",
    "for key, val in data_.items():\n",
    "    time_index = time_index.union(set(val[\"Date\"].values))\n",
    "time_index = list(time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time(t):\n",
    "    m = {\n",
    "        'Jan' : \"01\",\n",
    "        'Feb' : \"02\",\n",
    "        'Mar' : \"03\",\n",
    "        'Apr' : \"04\",\n",
    "        'May' : \"05\",\n",
    "        'Jun' : \"06\",\n",
    "        'Jul' : \"07\",\n",
    "        'Aug' : \"08\",\n",
    "        'Sep' : \"09\", \n",
    "        'Oct' : \"10\",\n",
    "        'Nov' : \"11\",\n",
    "        'Dec' : \"12\"\n",
    "    }\n",
    "    t_list = t.replace(\",\", \"\").split()\n",
    "    t_list[0] = m[t_list[0]]\n",
    "    return \"-\".join([t_list[2], t_list[0], t_list[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_index = sorted([convert_time(t) for t in time_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = data_[big_names[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import TensorForceError\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "agent_spec = {\n",
    "    \"type\": \"dqn_agent\",\n",
    "    \"batch_size\": 64,\n",
    "    \"memory\": {\n",
    "        \"type\": \"replay\",\n",
    "        \"capacity\": 10000\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"learning_rate\": 1e-3\n",
    "    },\n",
    "\n",
    "    \"discount\": 0.97,\n",
    "\n",
    "    \"exploration\": {\n",
    "        \"type\": \"epsilon_decay\",\n",
    "        \"initial_epsilon\": 1.0,\n",
    "        \"final_epsilon\": 0.1,\n",
    "        \"timesteps\": 1e6\n",
    "    }\n",
    "}\n",
    "\n",
    "gym_id = 'CartPole-v0'\n",
    "max_episodes = 10000\n",
    "max_timesteps = 1000\n",
    "\n",
    "environment = OpenAIGym(gym_id)\n",
    "network_spec = [\n",
    "        dict(type='dense', size=32, activation='tanh'),\n",
    "        dict(type='dense', size=32, activation='tanh')\n",
    "]\n",
    "\n",
    "agent = Agent.from_spec(\n",
    "    spec=agent_spec,\n",
    "    kwargs=dict(\n",
    "        states_spec=environment.states,\n",
    "        actions_spec=environment.actions,\n",
    "        network_spec=network_spec\n",
    "    )\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    repeat_actions=1\n",
    ")\n",
    "\n",
    "report_episodes = 10\n",
    "\n",
    "def episode_finished(r):\n",
    "    if r.episode % report_episodes == 0:\n",
    "        print(\"Finished episode {ep} after {ts} timesteps\".format(ep=r.episode, ts=r.timestep))\n",
    "        print(\"Episode reward: {}\".format(r.episode_rewards[-1]))\n",
    "        print(\"Average of last 100 rewards: {}\".format(sum(r.episode_rewards[-100:]) / 100))\n",
    "    return True\n",
    "\n",
    "runner.run(max_episodes, max_timesteps, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorforce."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
