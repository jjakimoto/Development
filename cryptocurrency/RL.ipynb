{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_ = {}\n",
    "\n",
    "for filename in os.listdir(\"data/Top100Cryptos/\"):\n",
    "    path = os.path.join(\"data/Top100Cryptos/\", filename)\n",
    "    try:\n",
    "        name = filename.split(\".\")[0]\n",
    "        data_[name] = pd.read_csv(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "num_data = []\n",
    "for name in data_.keys():\n",
    "    num_data.append(data_[name].shape[0])\n",
    "name_list = np.array(list(data_.keys()))[np.argsort(num_data)[::-1]]\n",
    "big_names = name_list[:10]\n",
    "\n",
    "data = dict()\n",
    "for name in big_names:\n",
    "    data[name] = data_[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time(t):\n",
    "    m = {\n",
    "        'Jan' : \"01\",\n",
    "        'Feb' : \"02\",\n",
    "        'Mar' : \"03\",\n",
    "        'Apr' : \"04\",\n",
    "        'May' : \"05\",\n",
    "        'Jun' : \"06\",\n",
    "        'June' : \"06\",\n",
    "        'Jul' : \"07\",\n",
    "        'Aug' : \"08\",\n",
    "        'Sep' : \"09\", \n",
    "        'Oct' : \"10\",\n",
    "        'Nov' : \"11\",\n",
    "        'Dec' : \"12\"\n",
    "    }\n",
    "    t_list = t.replace(\",\", \"\").split()\n",
    "    t_list[0] = m[t_list[0]]\n",
    "    return \"-\".join([t_list[2], t_list[0], t_list[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.environments import Environment\n",
    "from copy import deepcopy\n",
    "\n",
    "class TradeEnvironment(Environment):\n",
    "    \"\"\"Environment only for close prices\"\"\"\n",
    "    def __init__(self, data, start=None, end=None):\n",
    "        time_index = set()\n",
    "        impute_data = {}\n",
    "        for key, val in data.items():\n",
    "            dates = val[\"Date\"].values\n",
    "            dates = [convert_time(d) for d in dates]\n",
    "            impute_data[key] = dict(time_range=(dates[-1], dates[0]),\n",
    "                                    impute_val=(val.iloc[-1], val.iloc[0]))\n",
    "            data[key].index = dates\n",
    "            time_index = time_index.union(set(dates))\n",
    "        self.time_index = sorted(list(time_index))\n",
    "        self.impute_data  = impute_data\n",
    "        if start is None:\n",
    "            self.start = self.time_index[0]\n",
    "        else:\n",
    "            self.start = min(start, self.time_index[0])\n",
    "        if end is None:\n",
    "            self.end = self.time_index[-1]\n",
    "        else:\n",
    "            self.end = max(end, self.time_index[-1])\n",
    "        self.data = data\n",
    "        self.symbols = list(data.keys())\n",
    "        self.num_stocks = len(self.symbols)\n",
    "        self.current_time = self.start\n",
    "        self.current_step = 0\n",
    "        # Use for calculate return\n",
    "        self.prev_states = self._get_bar()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset environment and setup for new episode.\n",
    "        Returns:\n",
    "            initial state of resetted environment.\n",
    "        \"\"\"\n",
    "        self.current_time = self.start\n",
    "        self.cirrent_step = 0\n",
    "        current_states = self._get_bar()\n",
    "        states = np.array([current_states[symbol][\"Close\"] for symbol in self.symbols])\n",
    "        return states\n",
    "\n",
    "    def execute(self, actions):\n",
    "        \"\"\"\n",
    "        Executes action, observes next state(s) and reward.\n",
    "        Args:\n",
    "            actions: Actions to execute.\n",
    "        Returns:\n",
    "            (Dict of) next state(s), boolean indicating terminal, and reward signal.\n",
    "        \"\"\"\n",
    "        current_states = self._get_bar()\n",
    "        returns = []\n",
    "        for symbol in self.symbols:\n",
    "            returns.append(current_states[symbol][\"Close\"] / self.prev_states[symbol][\"Close\"] - 1)\n",
    "        # print(\"current_states\", self.current_time)\n",
    "        # Update status\n",
    "        self.prev_states = deepcopy(current_states)\n",
    "        self._update_time()\n",
    "        \n",
    "        states = np.array([current_states[symbol][\"Close\"] for symbol in self.symbols])\n",
    "        terminal = False\n",
    "        reward = np.sum(np.array(returns) * actions)\n",
    "        return states, terminal, reward\n",
    "        \n",
    "            \n",
    "    def _update_time(self):\n",
    "        index = self.time_index.index(self.current_time)\n",
    "        self.current_time = self.time_index[index + 1]\n",
    "        self.current_step += 1\n",
    "        \n",
    "    def _get_bar(self):\n",
    "        bar = {}\n",
    "        for symbol in self.symbols:\n",
    "            min_t = self.impute_data[symbol][\"time_range\"][0]\n",
    "            max_t = self.impute_data[symbol][\"time_range\"][1]\n",
    "            if (min_t <= self.current_time) and (max_t >= self.current_time):\n",
    "                if self.current_time in self.data[symbol].index:\n",
    "                    bar[symbol] = self.data[symbol].loc[self.current_time]\n",
    "                else:\n",
    "                    bar[symbol] = deepcopy(self.impute_bar[symbol])\n",
    "            elif min_t > self.current_time:\n",
    "                bar[symbol] = self.impute_data[symbol][\"impute_val\"][0]\n",
    "            else:\n",
    "                bar[symbol] = self.impute_data[symbol][\"impute_val\"][1]\n",
    "        # Keep value for imputation\n",
    "        self.impute_bar = deepcopy(bar)\n",
    "        return bar\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        \"\"\"\n",
    "        Return the state space. Might include subdicts if multiple states are available simultaneously.\n",
    "        Returns: dict of state properties (shape and type).\n",
    "        \"\"\"\n",
    "        return {'shape': (self.num_stocks,), 'type': 'float'}\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        \"\"\"\n",
    "        Return the action space. Might include subdicts if multiple actions are available simultaneously.\n",
    "        Returns: dict of action properties (continuous, number of actions)\n",
    "        \"\"\"\n",
    "        return {\"shape\": (self.num_stocks,), \"min_value\": 0., \"max_value\": 1., \"type\": \"float\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "from six.moves import xrange\n",
    "\n",
    "\n",
    "class TradeRunner(object):\n",
    "    \"\"\"\n",
    "    Simple runner for non-realtime single-process execution.\n",
    "    \"\"\"\n",
    "    def run(\n",
    "        self,\n",
    "        timesteps=None,\n",
    "        episodes=None,\n",
    "        max_episode_timesteps=None,\n",
    "        deterministic=False,\n",
    "        episode_finished=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Runs the agent on the environment.\n",
    "\n",
    "        Args:\n",
    "            timesteps: Number of timesteps\n",
    "            episodes: Number of episodes\n",
    "            max_episode_timesteps: Max number of timesteps per episode\n",
    "            deterministic: Deterministic flag\n",
    "            episode_finished: Function handler taking a `Runner` argument and returning a boolean indicating\n",
    "                whether to continue execution. For instance, useful for reporting intermediate performance or\n",
    "                integrating termination conditions.\n",
    "        \"\"\"\n",
    "\n",
    "        # Keep track of episode reward and episode length for statistics.\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.agent.reset()\n",
    "\n",
    "        self.episode = self.agent.episode\n",
    "        if episodes is not None:\n",
    "            episodes += self.agent.episode\n",
    "\n",
    "        self.timestep = self.agent.timestep\n",
    "        if timesteps is not None:\n",
    "            timesteps += self.agent.timestep\n",
    "\n",
    "        while True:\n",
    "            episode_start_time = time.time()\n",
    "\n",
    "            self.agent.reset()\n",
    "            state = self.environment.reset()\n",
    "            episode_reward = 0\n",
    "            self.episode_timestep = 0\n",
    "\n",
    "            while True:\n",
    "                action = self.agent.act(states=state, deterministic=deterministic)\n",
    "                # print(\"action\", action, np.sum(action))\n",
    "                if self.repeat_actions > 1:\n",
    "                    reward = 0\n",
    "                    for repeat in xrange(self.repeat_actions):\n",
    "                        state, terminal, step_reward = self.environment.execute(actions=action)\n",
    "                        reward += step_reward\n",
    "                        if terminal:\n",
    "                            break\n",
    "                else:\n",
    "                    state, terminal, reward = self.environment.execute(actions=action)\n",
    "                    # print(\"reward\", reward)\n",
    "\n",
    "                if max_episode_timesteps is not None and self.episode_timestep >= max_episode_timesteps:\n",
    "                    terminal = True\n",
    "\n",
    "                self.agent.observe(terminal=terminal, reward=reward)\n",
    "\n",
    "                self.episode_timestep += 1\n",
    "                self.timestep += 1\n",
    "                episode_reward += reward\n",
    "\n",
    "                if terminal or self.agent.should_stop():  # TODO: should_stop also termina?\n",
    "                    break\n",
    "\n",
    "            time_passed = time.time() - episode_start_time\n",
    "\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_timesteps.append(self.episode_timestep)\n",
    "            self.episode_times.append(time_passed)\n",
    "\n",
    "            self.episode += 1\n",
    "\n",
    "            if episode_finished and not episode_finished(self) or \\\n",
    "                    (episodes is not None and self.agent.episode >= episodes) or \\\n",
    "                    (timesteps is not None and self.agent.timestep >= timesteps) or \\\n",
    "                    self.agent.should_stop():\n",
    "                # agent.episode / agent.timestep are globally updated\n",
    "                break\n",
    "\n",
    "        self.agent.close()\n",
    "        self.environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1 after 201 timesteps (reward: 0.4365537900656106)\n",
      "Finished episode 2 after 201 timesteps (reward: 0.1980432050072013)\n",
      "Finished episode 3 after 201 timesteps (reward: 0.14357231029945416)\n",
      "Finished episode 4 after 201 timesteps (reward: 0.10085387668742578)\n",
      "Finished episode 5 after 201 timesteps (reward: 0.12416170255015677)\n",
      "Finished episode 6 after 201 timesteps (reward: 0.6011622395640375)\n",
      "Finished episode 7 after 201 timesteps (reward: 0.2165106489522159)\n",
      "Finished episode 8 after 201 timesteps (reward: 0.4757951239819698)\n",
      "Finished episode 9 after 201 timesteps (reward: 0.4145342049084961)\n",
      "Finished episode 10 after 201 timesteps (reward: 0.22707979847464327)\n",
      "Finished episode 11 after 201 timesteps (reward: 0.3471384261642542)\n",
      "Finished episode 12 after 201 timesteps (reward: 0.15387689761966789)\n",
      "Finished episode 13 after 201 timesteps (reward: 0.23636941871964393)\n",
      "Finished episode 14 after 201 timesteps (reward: 0.275200894952414)\n",
      "Finished episode 15 after 201 timesteps (reward: 0.2840603603208122)\n",
      "Finished episode 16 after 201 timesteps (reward: 0.13222280016520893)\n",
      "Finished episode 17 after 201 timesteps (reward: 0.49387875079476734)\n",
      "Finished episode 18 after 201 timesteps (reward: 0.18473225097896134)\n",
      "Finished episode 19 after 201 timesteps (reward: 0.45447520967245214)\n",
      "Finished episode 20 after 201 timesteps (reward: 0.1465544810259197)\n",
      "Finished episode 21 after 201 timesteps (reward: 0.796421695735406)\n",
      "Finished episode 22 after 201 timesteps (reward: 0.19694359328032948)\n",
      "Finished episode 23 after 201 timesteps (reward: 0.07225869195977863)\n",
      "Finished episode 24 after 201 timesteps (reward: 0.3554659381905313)\n",
      "Finished episode 25 after 201 timesteps (reward: 0.4809018824352404)\n",
      "Finished episode 26 after 201 timesteps (reward: 0.3331343635831785)\n",
      "Finished episode 27 after 201 timesteps (reward: 0.2658524415529342)\n",
      "Finished episode 28 after 201 timesteps (reward: 0.7133484675826173)\n",
      "Finished episode 29 after 201 timesteps (reward: -0.042953245789543404)\n",
      "Finished episode 30 after 201 timesteps (reward: 0.15592288973196908)\n",
      "Finished episode 31 after 201 timesteps (reward: 0.19852445701448806)\n",
      "Finished episode 32 after 201 timesteps (reward: 0.1737147657074467)\n",
      "Finished episode 33 after 201 timesteps (reward: 0.1866353293510065)\n",
      "Finished episode 34 after 201 timesteps (reward: 0.2778640243170356)\n",
      "Finished episode 35 after 201 timesteps (reward: 0.25710542836944167)\n",
      "Finished episode 36 after 201 timesteps (reward: 0.45367876259457246)\n",
      "Finished episode 37 after 201 timesteps (reward: 0.13015752024851465)\n",
      "Finished episode 38 after 201 timesteps (reward: 0.3208181250940067)\n",
      "Finished episode 39 after 201 timesteps (reward: 0.38727403408862665)\n",
      "Finished episode 40 after 201 timesteps (reward: 0.06460180468055658)\n",
      "Finished episode 41 after 201 timesteps (reward: 0.05685644667041871)\n",
      "Finished episode 42 after 201 timesteps (reward: 0.33018301984223214)\n",
      "Finished episode 43 after 201 timesteps (reward: 0.4639633854927681)\n",
      "Finished episode 44 after 201 timesteps (reward: 0.36026211576508493)\n",
      "Finished episode 45 after 201 timesteps (reward: 0.25108087932104456)\n",
      "Finished episode 46 after 201 timesteps (reward: 0.08708573580783978)\n",
      "Finished episode 47 after 201 timesteps (reward: 0.12576385772577936)\n",
      "Finished episode 48 after 201 timesteps (reward: 0.07614280500798996)\n",
      "Finished episode 49 after 201 timesteps (reward: 0.3945009030978386)\n",
      "Finished episode 50 after 201 timesteps (reward: 0.21363667435991804)\n",
      "Finished episode 51 after 201 timesteps (reward: 0.5758087865308273)\n",
      "Finished episode 52 after 201 timesteps (reward: 0.5811251345472562)\n",
      "Finished episode 53 after 201 timesteps (reward: 0.18530637009029222)\n",
      "Finished episode 54 after 201 timesteps (reward: 0.6625352958269097)\n",
      "Finished episode 55 after 201 timesteps (reward: 0.33444861797423775)\n",
      "Finished episode 56 after 201 timesteps (reward: 0.2374619498503877)\n",
      "Finished episode 57 after 201 timesteps (reward: 0.4123755781786075)\n",
      "Finished episode 58 after 201 timesteps (reward: 0.4254351619758111)\n",
      "Finished episode 59 after 201 timesteps (reward: 0.7016312826901365)\n",
      "Finished episode 60 after 201 timesteps (reward: 0.479651993722217)\n",
      "Finished episode 61 after 201 timesteps (reward: -0.033205609488128354)\n",
      "Finished episode 62 after 201 timesteps (reward: 0.3942827006694759)\n",
      "Finished episode 63 after 201 timesteps (reward: 0.337926220400528)\n",
      "Finished episode 64 after 201 timesteps (reward: 0.15622565070071973)\n",
      "Finished episode 65 after 201 timesteps (reward: 0.35827623680391196)\n",
      "Finished episode 66 after 201 timesteps (reward: 0.08830151379255317)\n",
      "Finished episode 67 after 201 timesteps (reward: 0.20152307354486673)\n",
      "Finished episode 68 after 201 timesteps (reward: 0.3527365437329777)\n",
      "Finished episode 69 after 201 timesteps (reward: 0.4641415102993531)\n",
      "Finished episode 70 after 201 timesteps (reward: 0.2027972618374799)\n",
      "Finished episode 71 after 201 timesteps (reward: 0.40900503750087447)\n",
      "Finished episode 72 after 201 timesteps (reward: 0.5123506884307795)\n",
      "Finished episode 73 after 201 timesteps (reward: 0.18680383725621744)\n",
      "Finished episode 74 after 201 timesteps (reward: 0.5737438670960325)\n",
      "Finished episode 75 after 201 timesteps (reward: 0.1923603427682827)\n",
      "Finished episode 76 after 201 timesteps (reward: 0.34832299054261656)\n",
      "Finished episode 77 after 201 timesteps (reward: 0.2949855044075977)\n",
      "Finished episode 78 after 201 timesteps (reward: 0.21732990383130407)\n",
      "Finished episode 79 after 201 timesteps (reward: 0.6003429451629652)\n",
      "Finished episode 80 after 201 timesteps (reward: 0.39512108419034264)\n",
      "Finished episode 81 after 201 timesteps (reward: 0.06536762228209082)\n",
      "Finished episode 82 after 201 timesteps (reward: 0.2446920369580889)\n",
      "Finished episode 83 after 201 timesteps (reward: 0.2584753837824102)\n",
      "Finished episode 84 after 201 timesteps (reward: 0.09616675140929394)\n",
      "Finished episode 85 after 201 timesteps (reward: 0.3372617063448503)\n",
      "Finished episode 86 after 201 timesteps (reward: 0.44150147583249116)\n",
      "Finished episode 87 after 201 timesteps (reward: 0.33581493724964956)\n",
      "Finished episode 88 after 201 timesteps (reward: 0.1502279172850611)\n",
      "Finished episode 89 after 201 timesteps (reward: 0.4678687872128265)\n",
      "Finished episode 90 after 201 timesteps (reward: -0.052031943764195815)\n",
      "Finished episode 91 after 201 timesteps (reward: 0.09838740180781437)\n",
      "Finished episode 92 after 201 timesteps (reward: 0.4512592110363955)\n",
      "Finished episode 93 after 201 timesteps (reward: 0.27507949513989277)\n",
      "Finished episode 94 after 201 timesteps (reward: 0.46119900996678703)\n",
      "Finished episode 95 after 201 timesteps (reward: 0.6220667762549351)\n",
      "Finished episode 96 after 201 timesteps (reward: 0.2713961183671435)\n",
      "Finished episode 97 after 201 timesteps (reward: 0.23184481997506137)\n",
      "Finished episode 98 after 201 timesteps (reward: 0.5418824755830338)\n",
      "Finished episode 99 after 201 timesteps (reward: 0.33258796875230934)\n",
      "Finished episode 100 after 201 timesteps (reward: 0.39067310344305145)\n",
      "Finished episode 101 after 201 timesteps (reward: 0.21960892510489602)\n",
      "Finished episode 102 after 201 timesteps (reward: 0.12315708383443871)\n",
      "Finished episode 103 after 201 timesteps (reward: 0.11613217522829931)\n",
      "Finished episode 104 after 201 timesteps (reward: 0.2977099059711923)\n",
      "Finished episode 105 after 201 timesteps (reward: 0.23353511788934503)\n",
      "Finished episode 106 after 201 timesteps (reward: 0.24432109541385322)\n",
      "Finished episode 107 after 201 timesteps (reward: 0.30787428601407896)\n",
      "Finished episode 108 after 201 timesteps (reward: 0.24840875787785283)\n",
      "Finished episode 109 after 201 timesteps (reward: 0.28065390809488333)\n",
      "Finished episode 110 after 201 timesteps (reward: 0.2345276911851415)\n",
      "Finished episode 111 after 201 timesteps (reward: 0.20378768394361188)\n",
      "Finished episode 112 after 201 timesteps (reward: 0.3497884776603199)\n",
      "Finished episode 113 after 201 timesteps (reward: 0.3299116792340054)\n",
      "Finished episode 114 after 201 timesteps (reward: 0.04930167791516783)\n",
      "Finished episode 115 after 201 timesteps (reward: 0.30931641992794506)\n",
      "Finished episode 116 after 201 timesteps (reward: 0.20777777754211757)\n",
      "Finished episode 117 after 201 timesteps (reward: 0.23379996423291902)\n",
      "Finished episode 118 after 201 timesteps (reward: 0.226316187786251)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 119 after 201 timesteps (reward: 0.3227528602848874)\n",
      "Finished episode 120 after 201 timesteps (reward: 0.3257591138492612)\n",
      "Finished episode 121 after 201 timesteps (reward: 0.34419066320022956)\n",
      "Finished episode 122 after 201 timesteps (reward: 0.644819982952338)\n",
      "Finished episode 123 after 201 timesteps (reward: 0.7181440970092735)\n",
      "Finished episode 124 after 201 timesteps (reward: 0.26704314021211767)\n",
      "Finished episode 125 after 201 timesteps (reward: 0.5590642123469617)\n",
      "Finished episode 126 after 201 timesteps (reward: 0.22034467345158967)\n",
      "Finished episode 127 after 201 timesteps (reward: 0.3492757893876897)\n",
      "Finished episode 128 after 201 timesteps (reward: 0.16884583307093956)\n",
      "Finished episode 129 after 201 timesteps (reward: 0.5962164088515772)\n",
      "Finished episode 130 after 201 timesteps (reward: 0.4693997892332481)\n",
      "Finished episode 131 after 201 timesteps (reward: 0.27758639859247747)\n",
      "Finished episode 132 after 201 timesteps (reward: 0.33186081105677934)\n",
      "Finished episode 133 after 201 timesteps (reward: 0.4830414626720681)\n",
      "Finished episode 134 after 201 timesteps (reward: 0.09734531512721914)\n",
      "Finished episode 135 after 201 timesteps (reward: 0.5491891672506778)\n",
      "Finished episode 136 after 201 timesteps (reward: 0.3399471118512141)\n",
      "Finished episode 137 after 201 timesteps (reward: 0.3592889976905584)\n",
      "Finished episode 138 after 201 timesteps (reward: 0.38875590673195587)\n",
      "Finished episode 139 after 201 timesteps (reward: 0.3219124286116387)\n",
      "Finished episode 140 after 201 timesteps (reward: 0.13713149099935706)\n",
      "Finished episode 141 after 201 timesteps (reward: 0.6209263873382517)\n",
      "Finished episode 142 after 201 timesteps (reward: 0.2728973324836365)\n",
      "Finished episode 143 after 201 timesteps (reward: 0.31548438900071196)\n",
      "Finished episode 144 after 201 timesteps (reward: 0.21658140710577625)\n",
      "Finished episode 145 after 201 timesteps (reward: 0.4372525262459257)\n",
      "Finished episode 146 after 201 timesteps (reward: 0.4546242199158477)\n",
      "Finished episode 147 after 201 timesteps (reward: 0.2991711417671013)\n",
      "Finished episode 148 after 201 timesteps (reward: 0.31327381926264825)\n",
      "Finished episode 149 after 201 timesteps (reward: 0.5902630308427039)\n",
      "Finished episode 150 after 201 timesteps (reward: -0.019892832668236802)\n",
      "Finished episode 151 after 201 timesteps (reward: 0.14293613274306186)\n",
      "Finished episode 152 after 201 timesteps (reward: 0.3135244092101165)\n",
      "Finished episode 153 after 201 timesteps (reward: 0.3823472357658138)\n",
      "Finished episode 154 after 201 timesteps (reward: 0.44762559642616284)\n",
      "Finished episode 155 after 201 timesteps (reward: 0.008518959450708439)\n",
      "Finished episode 156 after 201 timesteps (reward: 0.04452697190709461)\n",
      "Finished episode 157 after 201 timesteps (reward: 0.008485059318960797)\n",
      "Finished episode 158 after 201 timesteps (reward: 0.4279630599346294)\n",
      "Finished episode 159 after 201 timesteps (reward: 0.3320098846115277)\n",
      "Finished episode 160 after 201 timesteps (reward: 0.3776507008677503)\n",
      "Finished episode 161 after 201 timesteps (reward: 0.23757474227028408)\n",
      "Finished episode 162 after 201 timesteps (reward: 0.36666131287290066)\n",
      "Finished episode 163 after 201 timesteps (reward: 0.2671994726194664)\n",
      "Finished episode 164 after 201 timesteps (reward: 0.6195227964636442)\n",
      "Finished episode 165 after 201 timesteps (reward: 0.3269937917623162)\n",
      "Finished episode 166 after 201 timesteps (reward: 0.7100075621220318)\n",
      "Finished episode 167 after 201 timesteps (reward: 0.009437942254266704)\n",
      "Finished episode 168 after 201 timesteps (reward: 0.37607900648664533)\n",
      "Finished episode 169 after 201 timesteps (reward: 0.5078242039220628)\n",
      "Finished episode 170 after 201 timesteps (reward: 0.11129729979304205)\n",
      "Finished episode 171 after 201 timesteps (reward: 0.31414618091788044)\n",
      "Finished episode 172 after 201 timesteps (reward: 0.18932364766317403)\n",
      "Finished episode 173 after 201 timesteps (reward: 0.3528985765264678)\n",
      "Finished episode 174 after 201 timesteps (reward: 0.5573748854323857)\n",
      "Finished episode 175 after 201 timesteps (reward: 0.27816698147055885)\n",
      "Finished episode 176 after 201 timesteps (reward: 0.12803779390466566)\n",
      "Finished episode 177 after 201 timesteps (reward: 0.047422201839093014)\n",
      "Finished episode 178 after 201 timesteps (reward: 0.31990847461316996)\n",
      "Finished episode 179 after 201 timesteps (reward: 0.6203118986768951)\n",
      "Finished episode 180 after 201 timesteps (reward: 0.4130285285967368)\n",
      "Finished episode 181 after 201 timesteps (reward: 0.3639551371759151)\n",
      "Finished episode 182 after 201 timesteps (reward: 0.12838153449434186)\n",
      "Finished episode 183 after 201 timesteps (reward: 0.25906357165135135)\n",
      "Finished episode 184 after 201 timesteps (reward: 0.3047773068459062)\n",
      "Finished episode 185 after 201 timesteps (reward: 0.19592234503067876)\n",
      "Finished episode 186 after 201 timesteps (reward: 0.1770183135874273)\n",
      "Finished episode 187 after 201 timesteps (reward: 0.3484130759763347)\n",
      "Finished episode 188 after 201 timesteps (reward: 0.5628685777096539)\n",
      "Finished episode 189 after 201 timesteps (reward: 0.22776055773243414)\n",
      "Finished episode 190 after 201 timesteps (reward: 0.055563514505896264)\n",
      "Finished episode 191 after 201 timesteps (reward: 0.44054067524769636)\n",
      "Finished episode 192 after 201 timesteps (reward: 0.12336803019473103)\n",
      "Finished episode 193 after 201 timesteps (reward: 0.24706912545331647)\n",
      "Finished episode 194 after 201 timesteps (reward: 0.10553902661235096)\n",
      "Finished episode 195 after 201 timesteps (reward: 0.6256647892381901)\n",
      "Finished episode 196 after 201 timesteps (reward: 0.08373495166719004)\n",
      "Finished episode 197 after 201 timesteps (reward: 0.4481753584439241)\n",
      "Finished episode 198 after 201 timesteps (reward: 0.18423020628802425)\n",
      "Finished episode 199 after 201 timesteps (reward: 0.3527790954387803)\n",
      "Finished episode 200 after 201 timesteps (reward: 0.4059504953737304)\n",
      "Finished episode 201 after 201 timesteps (reward: 0.4249575924826082)\n",
      "Finished episode 202 after 201 timesteps (reward: -0.02458179178411862)\n",
      "Finished episode 203 after 201 timesteps (reward: 0.6915775876678258)\n",
      "Finished episode 204 after 201 timesteps (reward: -0.14764032140534483)\n",
      "Finished episode 205 after 201 timesteps (reward: 0.13135295529419738)\n",
      "Finished episode 206 after 201 timesteps (reward: 0.40008045624627936)\n",
      "Finished episode 207 after 201 timesteps (reward: 0.23938923246950689)\n",
      "Finished episode 208 after 201 timesteps (reward: 0.1657024290745308)\n",
      "Finished episode 209 after 201 timesteps (reward: 0.45117635566407516)\n",
      "Finished episode 210 after 201 timesteps (reward: 0.2236814639450278)\n",
      "Finished episode 211 after 201 timesteps (reward: -0.0007832926241789617)\n",
      "Finished episode 212 after 201 timesteps (reward: 0.10014582366992687)\n",
      "Finished episode 213 after 201 timesteps (reward: 0.38376228480685115)\n",
      "Finished episode 214 after 201 timesteps (reward: 0.2853603047203167)\n",
      "Finished episode 215 after 201 timesteps (reward: 0.23989219649327356)\n",
      "Finished episode 216 after 201 timesteps (reward: 0.36685506145419833)\n",
      "Finished episode 217 after 201 timesteps (reward: 0.3194089089371777)\n",
      "Finished episode 218 after 201 timesteps (reward: 0.3557237973461719)\n",
      "Finished episode 219 after 201 timesteps (reward: 0.09862921858648566)\n",
      "Finished episode 220 after 201 timesteps (reward: 0.19728679624378634)\n",
      "Finished episode 221 after 201 timesteps (reward: 0.10187982754398228)\n",
      "Finished episode 222 after 201 timesteps (reward: 0.6406563060016718)\n",
      "Finished episode 223 after 201 timesteps (reward: 0.33352441829672336)\n",
      "Finished episode 224 after 201 timesteps (reward: 0.20184757907416073)\n",
      "Finished episode 225 after 201 timesteps (reward: 0.03799480109557087)\n",
      "Finished episode 226 after 201 timesteps (reward: 0.25903041843880514)\n",
      "Finished episode 227 after 201 timesteps (reward: 0.1717146608700141)\n",
      "Finished episode 228 after 201 timesteps (reward: 0.415968617818276)\n",
      "Finished episode 229 after 201 timesteps (reward: 0.31925597082123375)\n",
      "Finished episode 230 after 201 timesteps (reward: 0.24962497535272782)\n",
      "Finished episode 231 after 201 timesteps (reward: 0.44009573993358336)\n",
      "Finished episode 232 after 201 timesteps (reward: 0.37641413167196297)\n",
      "Finished episode 233 after 201 timesteps (reward: 0.39664915193935874)\n",
      "Finished episode 234 after 201 timesteps (reward: 0.2613066145617863)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 235 after 201 timesteps (reward: 0.28692524028660005)\n",
      "Finished episode 236 after 201 timesteps (reward: -0.06513052265387305)\n",
      "Finished episode 237 after 201 timesteps (reward: 0.2866458902841674)\n",
      "Finished episode 238 after 201 timesteps (reward: 0.12032037516830367)\n",
      "Finished episode 239 after 201 timesteps (reward: 0.3445137575266846)\n",
      "Finished episode 240 after 201 timesteps (reward: 0.6994294147615686)\n",
      "Finished episode 241 after 201 timesteps (reward: 0.3970396209381291)\n",
      "Finished episode 242 after 201 timesteps (reward: 0.04480319754138636)\n",
      "Finished episode 243 after 201 timesteps (reward: 0.2684682505620302)\n",
      "Finished episode 244 after 201 timesteps (reward: 0.36938389637930097)\n",
      "Finished episode 245 after 201 timesteps (reward: 0.36259257313542115)\n",
      "Finished episode 246 after 201 timesteps (reward: 0.3457220334489663)\n",
      "Finished episode 247 after 201 timesteps (reward: 0.5119743077768025)\n",
      "Finished episode 248 after 201 timesteps (reward: -0.0932361688742099)\n",
      "Finished episode 249 after 201 timesteps (reward: 0.4010334763566492)\n",
      "Finished episode 250 after 201 timesteps (reward: 0.39848556538119645)\n",
      "Finished episode 251 after 201 timesteps (reward: 0.17224473766140114)\n",
      "Finished episode 252 after 201 timesteps (reward: 0.18636547459091074)\n",
      "Finished episode 253 after 201 timesteps (reward: 0.11002266261166449)\n",
      "Finished episode 254 after 201 timesteps (reward: 0.5627616829080961)\n",
      "Finished episode 255 after 201 timesteps (reward: 0.15835685121033355)\n",
      "Finished episode 256 after 201 timesteps (reward: 0.38026642011211237)\n",
      "Finished episode 257 after 201 timesteps (reward: -0.0600871980285403)\n",
      "Finished episode 258 after 201 timesteps (reward: 0.7260547439703626)\n",
      "Finished episode 259 after 201 timesteps (reward: 0.22428754452853375)\n",
      "Finished episode 260 after 201 timesteps (reward: 0.31931727767458634)\n",
      "Finished episode 261 after 201 timesteps (reward: 0.23603874472836409)\n",
      "Finished episode 262 after 201 timesteps (reward: 0.2817330806156653)\n",
      "Finished episode 263 after 201 timesteps (reward: 0.3764610764362378)\n",
      "Finished episode 264 after 201 timesteps (reward: 0.3963025162073355)\n",
      "Finished episode 265 after 201 timesteps (reward: -0.10630674886804686)\n",
      "Finished episode 266 after 201 timesteps (reward: 0.5284941095839903)\n",
      "Finished episode 267 after 201 timesteps (reward: 0.41312370119148994)\n",
      "Finished episode 268 after 201 timesteps (reward: 0.3017471538445194)\n",
      "Finished episode 269 after 201 timesteps (reward: 0.5274257081532358)\n",
      "Finished episode 270 after 201 timesteps (reward: 0.10235648462149477)\n",
      "Finished episode 271 after 201 timesteps (reward: -0.010783923189594386)\n",
      "Finished episode 272 after 201 timesteps (reward: 0.485132054087651)\n",
      "Finished episode 273 after 201 timesteps (reward: 0.32609984308082063)\n",
      "Finished episode 274 after 201 timesteps (reward: 0.3446480970881422)\n",
      "Finished episode 275 after 201 timesteps (reward: 0.43573960882120427)\n",
      "Finished episode 276 after 201 timesteps (reward: 0.35883749691573946)\n",
      "Finished episode 277 after 201 timesteps (reward: 0.42204420892897127)\n",
      "Finished episode 278 after 201 timesteps (reward: 0.26426101769609583)\n",
      "Finished episode 279 after 201 timesteps (reward: 0.35034623888748617)\n",
      "Finished episode 280 after 201 timesteps (reward: 0.02803478612150478)\n",
      "Finished episode 281 after 201 timesteps (reward: 0.4413779368153076)\n",
      "Finished episode 282 after 201 timesteps (reward: 0.16530400950633883)\n",
      "Finished episode 283 after 201 timesteps (reward: 0.4657481704375987)\n",
      "Finished episode 284 after 201 timesteps (reward: 0.2590004145588763)\n",
      "Finished episode 285 after 201 timesteps (reward: -0.04287587171196856)\n",
      "Finished episode 286 after 201 timesteps (reward: 0.11443394371224633)\n",
      "Finished episode 287 after 201 timesteps (reward: 0.2712296380062137)\n",
      "Finished episode 288 after 201 timesteps (reward: 0.4090284434798944)\n",
      "Finished episode 289 after 201 timesteps (reward: 0.6015845150586727)\n",
      "Finished episode 290 after 201 timesteps (reward: 0.24290423953046208)\n",
      "Finished episode 291 after 201 timesteps (reward: 0.14135451732243573)\n",
      "Finished episode 292 after 201 timesteps (reward: 0.4621369930344893)\n",
      "Finished episode 293 after 201 timesteps (reward: 0.16668228926315518)\n",
      "Finished episode 294 after 201 timesteps (reward: 0.31522428978987055)\n",
      "Finished episode 295 after 201 timesteps (reward: 0.5574862076300982)\n",
      "Finished episode 296 after 201 timesteps (reward: 0.300749499183543)\n",
      "Finished episode 297 after 201 timesteps (reward: 0.14931031794733746)\n",
      "Finished episode 298 after 201 timesteps (reward: 0.8306221346185958)\n",
      "Finished episode 299 after 201 timesteps (reward: 0.3705461394270717)\n",
      "Finished episode 300 after 201 timesteps (reward: 0.21118525962229667)\n",
      "Finished episode 301 after 201 timesteps (reward: 0.09893910703419716)\n",
      "Finished episode 302 after 201 timesteps (reward: 0.42185559969717873)\n",
      "Finished episode 303 after 201 timesteps (reward: 0.24818766731301894)\n",
      "Finished episode 304 after 201 timesteps (reward: 0.6236079935256993)\n",
      "Finished episode 305 after 201 timesteps (reward: 0.16433848536700332)\n",
      "Finished episode 306 after 201 timesteps (reward: 0.3421790825336411)\n",
      "Finished episode 307 after 201 timesteps (reward: 0.12708011642986733)\n",
      "Finished episode 308 after 201 timesteps (reward: 0.1295419170659525)\n",
      "Finished episode 309 after 201 timesteps (reward: 0.3598153307711233)\n",
      "Finished episode 310 after 201 timesteps (reward: 0.25055123709099836)\n",
      "Finished episode 311 after 201 timesteps (reward: 0.09400036347373765)\n",
      "Finished episode 312 after 201 timesteps (reward: 0.5006092625661509)\n",
      "Finished episode 313 after 201 timesteps (reward: 0.42840299294936424)\n",
      "Finished episode 314 after 201 timesteps (reward: 0.21112714471657157)\n",
      "Finished episode 315 after 201 timesteps (reward: 0.28346823889387385)\n",
      "Finished episode 316 after 201 timesteps (reward: 0.06324652798282074)\n",
      "Finished episode 317 after 201 timesteps (reward: 0.4143770758812671)\n",
      "Finished episode 318 after 201 timesteps (reward: 0.35623381591099107)\n",
      "Finished episode 319 after 201 timesteps (reward: 0.4625981767759143)\n",
      "Finished episode 320 after 201 timesteps (reward: 0.025648669342556173)\n",
      "Finished episode 321 after 201 timesteps (reward: 0.002722855767678639)\n",
      "Finished episode 322 after 201 timesteps (reward: 0.3793204609550834)\n",
      "Finished episode 323 after 201 timesteps (reward: -0.09607595594707127)\n",
      "Finished episode 324 after 201 timesteps (reward: 0.160646479413359)\n",
      "Finished episode 325 after 201 timesteps (reward: 0.1668172562672617)\n",
      "Finished episode 326 after 201 timesteps (reward: 0.1278563792054375)\n",
      "Finished episode 327 after 201 timesteps (reward: 0.13024472303337126)\n",
      "Finished episode 328 after 201 timesteps (reward: 0.10879982919348552)\n",
      "Finished episode 329 after 201 timesteps (reward: 0.4277165170081072)\n",
      "Finished episode 330 after 201 timesteps (reward: 0.20252053534413672)\n",
      "Finished episode 331 after 201 timesteps (reward: 0.4778742977934491)\n",
      "Finished episode 332 after 201 timesteps (reward: 0.25067253843997156)\n",
      "Finished episode 333 after 201 timesteps (reward: 0.27660521887484246)\n",
      "Finished episode 334 after 201 timesteps (reward: 0.7842888880749139)\n",
      "Finished episode 335 after 201 timesteps (reward: 0.3371850041667176)\n",
      "Finished episode 336 after 201 timesteps (reward: 0.36518475695028346)\n",
      "Finished episode 337 after 201 timesteps (reward: -0.07565597467093667)\n",
      "Finished episode 338 after 201 timesteps (reward: 0.6457712601132868)\n",
      "Finished episode 339 after 201 timesteps (reward: 0.14062829420589457)\n",
      "Finished episode 340 after 201 timesteps (reward: 0.45213589548250277)\n",
      "Finished episode 341 after 201 timesteps (reward: 0.010499007244898245)\n",
      "Finished episode 342 after 201 timesteps (reward: 0.44737609903853737)\n",
      "Finished episode 343 after 201 timesteps (reward: 0.029549854005350158)\n",
      "Finished episode 344 after 201 timesteps (reward: 0.16019712334581443)\n",
      "Finished episode 345 after 201 timesteps (reward: 0.11107535498072564)\n",
      "Finished episode 346 after 201 timesteps (reward: 0.7125627588502147)\n",
      "Finished episode 347 after 201 timesteps (reward: 0.15919621971022638)\n",
      "Finished episode 348 after 201 timesteps (reward: 0.31617037558002287)\n",
      "Finished episode 349 after 201 timesteps (reward: 0.5028987202972695)\n",
      "Finished episode 350 after 201 timesteps (reward: -0.0970806684277209)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 351 after 201 timesteps (reward: 0.30045067349789867)\n",
      "Finished episode 352 after 201 timesteps (reward: 0.3277566726349783)\n",
      "Finished episode 353 after 201 timesteps (reward: 0.2442240477535162)\n",
      "Finished episode 354 after 201 timesteps (reward: 0.26287229387683175)\n",
      "Finished episode 355 after 201 timesteps (reward: 0.3444936990312285)\n",
      "Finished episode 356 after 201 timesteps (reward: 0.44805862007319763)\n",
      "Finished episode 357 after 201 timesteps (reward: 0.4173930646945503)\n",
      "Finished episode 358 after 201 timesteps (reward: 0.6954883356201589)\n",
      "Finished episode 359 after 201 timesteps (reward: 0.0652976534065094)\n",
      "Finished episode 360 after 201 timesteps (reward: -0.10715976956156605)\n",
      "Finished episode 361 after 201 timesteps (reward: 0.3908661708675782)\n",
      "Finished episode 362 after 201 timesteps (reward: 0.18809274663771003)\n",
      "Finished episode 363 after 201 timesteps (reward: 0.2801781204438374)\n",
      "Finished episode 364 after 201 timesteps (reward: 0.651359268305824)\n",
      "Finished episode 365 after 201 timesteps (reward: 0.3071813128899521)\n",
      "Finished episode 366 after 201 timesteps (reward: 0.4405445341266014)\n",
      "Finished episode 367 after 201 timesteps (reward: 0.41958081473756376)\n",
      "Finished episode 368 after 201 timesteps (reward: 0.12738806668966554)\n",
      "Finished episode 369 after 201 timesteps (reward: 0.2631746949450464)\n",
      "Finished episode 370 after 201 timesteps (reward: 0.21432478594468332)\n",
      "Finished episode 371 after 201 timesteps (reward: 0.4283541302026949)\n",
      "Finished episode 372 after 201 timesteps (reward: 0.41750039738746275)\n",
      "Finished episode 373 after 201 timesteps (reward: 0.6760788211916773)\n",
      "Finished episode 374 after 201 timesteps (reward: 0.1954938791612332)\n",
      "Finished episode 375 after 201 timesteps (reward: 0.3220378533663116)\n",
      "Finished episode 376 after 201 timesteps (reward: 0.098568271549831)\n",
      "Finished episode 377 after 201 timesteps (reward: 0.4650116443781328)\n",
      "Finished episode 378 after 201 timesteps (reward: 0.4131437714065277)\n",
      "Finished episode 379 after 201 timesteps (reward: 0.29117030136987354)\n",
      "Finished episode 380 after 201 timesteps (reward: 0.2468296244572583)\n",
      "Finished episode 381 after 201 timesteps (reward: 0.9450839703490759)\n",
      "Finished episode 382 after 201 timesteps (reward: 0.5385983494684017)\n",
      "Finished episode 383 after 201 timesteps (reward: 0.2788236519682782)\n",
      "Finished episode 384 after 201 timesteps (reward: 0.20136328627901465)\n",
      "Finished episode 385 after 201 timesteps (reward: 0.6781065740150177)\n",
      "Finished episode 386 after 201 timesteps (reward: 0.36537506661252633)\n",
      "Finished episode 387 after 201 timesteps (reward: 0.5336436141166566)\n",
      "Finished episode 388 after 201 timesteps (reward: 0.20986049130100104)\n",
      "Finished episode 389 after 201 timesteps (reward: 0.12542812117947377)\n",
      "Finished episode 390 after 201 timesteps (reward: 0.07720446008820564)\n",
      "Finished episode 391 after 201 timesteps (reward: 0.3541189028658532)\n",
      "Finished episode 392 after 201 timesteps (reward: 0.3840792274954353)\n",
      "Finished episode 393 after 201 timesteps (reward: 0.26797470109952576)\n",
      "Finished episode 394 after 201 timesteps (reward: -0.10837545121941732)\n",
      "Finished episode 395 after 201 timesteps (reward: 0.3358145865034621)\n",
      "Finished episode 396 after 201 timesteps (reward: 0.11425282569101862)\n",
      "Finished episode 397 after 201 timesteps (reward: 0.4794240769157074)\n",
      "Finished episode 398 after 201 timesteps (reward: 0.30385968874644886)\n",
      "Finished episode 399 after 201 timesteps (reward: 0.382517904871497)\n",
      "Finished episode 400 after 201 timesteps (reward: 0.38490848083852863)\n",
      "Finished episode 401 after 201 timesteps (reward: 0.5085846063001832)\n",
      "Finished episode 402 after 201 timesteps (reward: 0.5544218009847391)\n",
      "Finished episode 403 after 201 timesteps (reward: 0.27951045241400013)\n",
      "Finished episode 404 after 201 timesteps (reward: 0.48394079507122634)\n",
      "Finished episode 405 after 201 timesteps (reward: 0.3084079596086586)\n",
      "Finished episode 406 after 201 timesteps (reward: 0.3753601672694086)\n",
      "Finished episode 407 after 201 timesteps (reward: 0.461697467300169)\n",
      "Finished episode 408 after 201 timesteps (reward: -0.037879963877399336)\n",
      "Finished episode 409 after 201 timesteps (reward: 0.41385097625433404)\n",
      "Finished episode 410 after 201 timesteps (reward: 0.5555274725878616)\n",
      "Finished episode 411 after 201 timesteps (reward: -0.02387874501008305)\n",
      "Finished episode 412 after 201 timesteps (reward: 0.4207148735339903)\n",
      "Finished episode 413 after 201 timesteps (reward: 0.1848474508143469)\n",
      "Finished episode 414 after 201 timesteps (reward: 0.22180141725185118)\n",
      "Finished episode 415 after 201 timesteps (reward: 0.4173474014566854)\n",
      "Finished episode 416 after 201 timesteps (reward: 0.276274449085491)\n",
      "Finished episode 417 after 201 timesteps (reward: 0.08987171953728093)\n",
      "Finished episode 418 after 201 timesteps (reward: -0.09698486664404826)\n",
      "Finished episode 419 after 201 timesteps (reward: 0.2822566220244956)\n",
      "Finished episode 420 after 201 timesteps (reward: 0.3660125314731325)\n",
      "Finished episode 421 after 201 timesteps (reward: 0.11004544394601534)\n",
      "Finished episode 422 after 201 timesteps (reward: 0.23162790028440824)\n",
      "Finished episode 423 after 201 timesteps (reward: 0.13120500817049963)\n",
      "Finished episode 424 after 201 timesteps (reward: 0.38642654152693245)\n",
      "Finished episode 425 after 201 timesteps (reward: 0.303220644602225)\n",
      "Finished episode 426 after 201 timesteps (reward: 0.17912099031248727)\n",
      "Finished episode 427 after 201 timesteps (reward: 0.4812072464000869)\n",
      "Finished episode 428 after 201 timesteps (reward: 0.2963012533569195)\n",
      "Finished episode 429 after 201 timesteps (reward: 0.07783008088961475)\n",
      "Finished episode 430 after 201 timesteps (reward: 0.33807001274991394)\n",
      "Finished episode 431 after 201 timesteps (reward: 0.21654431289052856)\n",
      "Finished episode 432 after 201 timesteps (reward: 0.3374370681883885)\n",
      "Finished episode 433 after 201 timesteps (reward: 0.6616742214694531)\n",
      "Finished episode 434 after 201 timesteps (reward: 0.21981680285534194)\n",
      "Finished episode 435 after 201 timesteps (reward: 0.12226145290498627)\n",
      "Finished episode 436 after 201 timesteps (reward: 0.3564336480067911)\n",
      "Finished episode 437 after 201 timesteps (reward: 0.3144997827655485)\n",
      "Finished episode 438 after 201 timesteps (reward: 0.28959491519020697)\n",
      "Finished episode 439 after 201 timesteps (reward: 0.5161068932445634)\n",
      "Finished episode 440 after 201 timesteps (reward: 0.3925155421508566)\n",
      "Finished episode 441 after 201 timesteps (reward: 0.27370079944972486)\n",
      "Finished episode 442 after 201 timesteps (reward: 0.16710557454990022)\n",
      "Finished episode 443 after 201 timesteps (reward: 0.27026275433326064)\n",
      "Finished episode 444 after 201 timesteps (reward: 0.43318249806788434)\n",
      "Finished episode 445 after 201 timesteps (reward: 0.23170982870490853)\n",
      "Finished episode 446 after 201 timesteps (reward: 0.10779507158621998)\n",
      "Finished episode 447 after 201 timesteps (reward: 0.23840998703526595)\n",
      "Finished episode 448 after 201 timesteps (reward: 0.06778581539635813)\n",
      "Finished episode 449 after 201 timesteps (reward: 0.527206511347781)\n",
      "Finished episode 450 after 201 timesteps (reward: 0.18610736938119588)\n",
      "Finished episode 451 after 201 timesteps (reward: 0.4366457105109992)\n",
      "Finished episode 452 after 201 timesteps (reward: -0.14266456598917845)\n",
      "Finished episode 453 after 201 timesteps (reward: 0.6320780306184075)\n",
      "Finished episode 454 after 201 timesteps (reward: 0.3632347715784781)\n",
      "Finished episode 455 after 201 timesteps (reward: 0.30441546019958804)\n",
      "Finished episode 456 after 201 timesteps (reward: 0.3152178740769548)\n",
      "Finished episode 457 after 201 timesteps (reward: 0.20446630583153885)\n",
      "Finished episode 458 after 201 timesteps (reward: 0.3074948394606411)\n",
      "Finished episode 459 after 201 timesteps (reward: 0.5636496198809923)\n",
      "Finished episode 460 after 201 timesteps (reward: 0.08581572760457859)\n",
      "Finished episode 461 after 201 timesteps (reward: 0.39615353084030647)\n",
      "Finished episode 462 after 201 timesteps (reward: 0.6573465919671555)\n",
      "Finished episode 463 after 201 timesteps (reward: 0.39398656630270984)\n",
      "Finished episode 464 after 201 timesteps (reward: 0.31240394120806714)\n",
      "Finished episode 465 after 201 timesteps (reward: 0.16191508369917076)\n",
      "Finished episode 466 after 201 timesteps (reward: 0.5363878580720105)\n",
      "Finished episode 467 after 201 timesteps (reward: 0.41250898527930663)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 468 after 201 timesteps (reward: 0.5349131667452705)\n",
      "Finished episode 469 after 201 timesteps (reward: 0.30698677219564136)\n",
      "Finished episode 470 after 201 timesteps (reward: 0.36688949497747736)\n",
      "Finished episode 471 after 201 timesteps (reward: 0.42915514234411445)\n",
      "Finished episode 472 after 201 timesteps (reward: 0.38708692364527475)\n",
      "Finished episode 473 after 201 timesteps (reward: 0.3825040588317931)\n",
      "Finished episode 474 after 201 timesteps (reward: 0.27729700473067026)\n",
      "Finished episode 475 after 201 timesteps (reward: 0.5852140082692431)\n",
      "Finished episode 476 after 201 timesteps (reward: 0.26408924406384243)\n",
      "Finished episode 477 after 201 timesteps (reward: 0.065277719284413)\n",
      "Finished episode 478 after 201 timesteps (reward: 0.3734156620877674)\n",
      "Finished episode 479 after 201 timesteps (reward: 0.07449703437629494)\n",
      "Finished episode 480 after 201 timesteps (reward: 0.5876472071822271)\n",
      "Finished episode 481 after 201 timesteps (reward: 0.03713052166679924)\n",
      "Finished episode 482 after 201 timesteps (reward: 0.30355905449216125)\n",
      "Finished episode 483 after 201 timesteps (reward: 0.2425918283321347)\n",
      "Finished episode 484 after 201 timesteps (reward: 0.04353549512683781)\n",
      "Finished episode 485 after 201 timesteps (reward: 0.23686818206593344)\n",
      "Finished episode 486 after 201 timesteps (reward: 0.1787940801862539)\n",
      "Finished episode 487 after 201 timesteps (reward: 0.3400041605512554)\n",
      "Finished episode 488 after 201 timesteps (reward: 0.29111859166488185)\n",
      "Finished episode 489 after 201 timesteps (reward: 0.40070673604743395)\n",
      "Finished episode 490 after 201 timesteps (reward: 0.45271626096858714)\n",
      "Finished episode 491 after 201 timesteps (reward: 0.12519101465300986)\n",
      "Finished episode 492 after 201 timesteps (reward: 0.4681536822751646)\n",
      "Finished episode 493 after 201 timesteps (reward: 0.19772237114047753)\n",
      "Finished episode 494 after 201 timesteps (reward: 0.2599905144164847)\n",
      "Finished episode 495 after 201 timesteps (reward: 0.37252326197741764)\n",
      "Finished episode 496 after 201 timesteps (reward: 0.2251025808339041)\n",
      "Finished episode 497 after 201 timesteps (reward: 0.2850818519158433)\n",
      "Finished episode 498 after 201 timesteps (reward: -0.16117370795558925)\n",
      "Finished episode 499 after 201 timesteps (reward: 0.10043836104495625)\n",
      "Finished episode 500 after 201 timesteps (reward: 0.47025977548160386)\n",
      "Finished episode 501 after 201 timesteps (reward: -0.07589295939036157)\n",
      "Finished episode 502 after 201 timesteps (reward: -0.01752505831212618)\n",
      "Finished episode 503 after 201 timesteps (reward: 0.23610806812512436)\n",
      "Finished episode 504 after 201 timesteps (reward: 0.18391366328977454)\n",
      "Finished episode 505 after 201 timesteps (reward: 0.33072908940518136)\n",
      "Finished episode 506 after 201 timesteps (reward: 0.3187003542319326)\n",
      "Finished episode 507 after 201 timesteps (reward: 0.20783644892333347)\n",
      "Finished episode 508 after 201 timesteps (reward: 0.14102071570803398)\n",
      "Finished episode 509 after 201 timesteps (reward: 0.6879239904072016)\n",
      "Finished episode 510 after 201 timesteps (reward: 0.6538902137743667)\n",
      "Finished episode 511 after 201 timesteps (reward: 0.43671311167933796)\n",
      "Finished episode 512 after 201 timesteps (reward: 0.16771671585015427)\n",
      "Finished episode 513 after 201 timesteps (reward: 0.8605456493001812)\n",
      "Finished episode 514 after 201 timesteps (reward: 0.23727830143376685)\n",
      "Finished episode 515 after 201 timesteps (reward: -0.047965348641723915)\n",
      "Finished episode 516 after 201 timesteps (reward: 0.14841689275109565)\n",
      "Finished episode 517 after 201 timesteps (reward: 0.0879373716373346)\n",
      "Finished episode 518 after 201 timesteps (reward: 0.3396236104522788)\n",
      "Finished episode 519 after 201 timesteps (reward: 0.0030557209658832523)\n",
      "Finished episode 520 after 201 timesteps (reward: 0.3017654346444558)\n",
      "Finished episode 521 after 201 timesteps (reward: 0.36104248945022127)\n",
      "Finished episode 522 after 201 timesteps (reward: 0.22190836912358228)\n",
      "Finished episode 523 after 201 timesteps (reward: 0.2531416840864946)\n",
      "Finished episode 524 after 201 timesteps (reward: 0.478371978264203)\n",
      "Finished episode 525 after 201 timesteps (reward: 0.6467351442451557)\n",
      "Finished episode 526 after 201 timesteps (reward: 0.4185428719307042)\n",
      "Finished episode 527 after 201 timesteps (reward: 0.318132522576285)\n",
      "Finished episode 528 after 201 timesteps (reward: 0.10369789286489026)\n",
      "Finished episode 529 after 201 timesteps (reward: 0.20046298366679297)\n",
      "Finished episode 530 after 201 timesteps (reward: 0.5891661121310967)\n",
      "Finished episode 531 after 201 timesteps (reward: 0.275570211475813)\n",
      "Finished episode 532 after 201 timesteps (reward: 0.17329025545061813)\n",
      "Finished episode 533 after 201 timesteps (reward: 0.20061058796616538)\n",
      "Finished episode 534 after 201 timesteps (reward: 0.23891916430829593)\n",
      "Finished episode 535 after 201 timesteps (reward: 0.3866742512897114)\n",
      "Finished episode 536 after 201 timesteps (reward: 0.5028215348097591)\n",
      "Finished episode 537 after 201 timesteps (reward: 0.4165573372226362)\n",
      "Finished episode 538 after 201 timesteps (reward: 0.191549828298106)\n",
      "Finished episode 539 after 201 timesteps (reward: 0.4106337252660778)\n",
      "Finished episode 540 after 201 timesteps (reward: 0.4305363614712797)\n",
      "Finished episode 541 after 201 timesteps (reward: 0.3940230848843155)\n",
      "Finished episode 542 after 201 timesteps (reward: 0.43648212289504473)\n",
      "Finished episode 543 after 201 timesteps (reward: 0.34275319110666)\n",
      "Finished episode 544 after 201 timesteps (reward: 0.10708450895031599)\n",
      "Finished episode 545 after 201 timesteps (reward: 0.2883663684291084)\n",
      "Finished episode 546 after 201 timesteps (reward: 0.5815873408593274)\n",
      "Finished episode 547 after 201 timesteps (reward: 0.2062097471148885)\n",
      "Finished episode 548 after 201 timesteps (reward: 0.0685303462723083)\n",
      "Finished episode 549 after 201 timesteps (reward: 0.24657292994512714)\n",
      "Finished episode 550 after 201 timesteps (reward: 0.19352646219293582)\n",
      "Finished episode 551 after 201 timesteps (reward: 0.4656880010161328)\n",
      "Finished episode 552 after 201 timesteps (reward: 0.5540156779876317)\n",
      "Finished episode 553 after 201 timesteps (reward: 0.3244753824525816)\n",
      "Finished episode 554 after 201 timesteps (reward: 0.3277861789361695)\n",
      "Finished episode 555 after 201 timesteps (reward: 0.4874175712801983)\n",
      "Finished episode 556 after 201 timesteps (reward: 0.3220216455801426)\n",
      "Finished episode 557 after 201 timesteps (reward: 0.2922901822258549)\n",
      "Finished episode 558 after 201 timesteps (reward: 0.398517863555515)\n",
      "Finished episode 559 after 201 timesteps (reward: 0.0307852540519327)\n",
      "Finished episode 560 after 201 timesteps (reward: 0.5628599934058188)\n",
      "Finished episode 561 after 201 timesteps (reward: 0.2861406592954937)\n",
      "Finished episode 562 after 201 timesteps (reward: 0.42424087390163917)\n",
      "Finished episode 563 after 201 timesteps (reward: -0.07767445061701767)\n",
      "Finished episode 564 after 201 timesteps (reward: 0.29744853634072593)\n",
      "Finished episode 565 after 201 timesteps (reward: 0.5183994800331887)\n",
      "Finished episode 566 after 201 timesteps (reward: 0.2877401852896529)\n",
      "Finished episode 567 after 201 timesteps (reward: 0.1966222314122643)\n",
      "Finished episode 568 after 201 timesteps (reward: 0.423839244312619)\n",
      "Finished episode 569 after 201 timesteps (reward: 0.6798913739966385)\n",
      "Finished episode 570 after 201 timesteps (reward: 0.24677480684758726)\n",
      "Finished episode 571 after 201 timesteps (reward: 0.5370305903402843)\n",
      "Finished episode 572 after 201 timesteps (reward: -0.40274487654120616)\n",
      "Finished episode 573 after 201 timesteps (reward: 0.12323193991743348)\n",
      "Finished episode 574 after 201 timesteps (reward: 0.6215451261147773)\n",
      "Finished episode 575 after 201 timesteps (reward: 0.4034500757005425)\n",
      "Finished episode 576 after 201 timesteps (reward: 0.31477841305288323)\n",
      "Finished episode 577 after 201 timesteps (reward: 0.45805207035711737)\n",
      "Finished episode 578 after 201 timesteps (reward: 0.812192238097406)\n",
      "Finished episode 579 after 201 timesteps (reward: 0.2660187394114425)\n",
      "Finished episode 580 after 201 timesteps (reward: 0.3134946683497997)\n",
      "Finished episode 581 after 201 timesteps (reward: 0.6249304983798423)\n",
      "Finished episode 582 after 201 timesteps (reward: 0.3615086832822372)\n",
      "Finished episode 583 after 201 timesteps (reward: 0.019182617186949927)\n",
      "Finished episode 584 after 201 timesteps (reward: 0.5626674079959662)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 585 after 201 timesteps (reward: 0.32150141850723635)\n",
      "Finished episode 586 after 201 timesteps (reward: 0.23191537970857362)\n",
      "Finished episode 587 after 201 timesteps (reward: 0.6530766216568417)\n",
      "Finished episode 588 after 201 timesteps (reward: 0.15603210811182877)\n",
      "Finished episode 589 after 201 timesteps (reward: 0.24004291794405572)\n",
      "Finished episode 590 after 201 timesteps (reward: 0.30137686834788735)\n",
      "Finished episode 591 after 201 timesteps (reward: 0.25656233845496074)\n",
      "Finished episode 592 after 201 timesteps (reward: 0.18464256126695214)\n",
      "Finished episode 593 after 201 timesteps (reward: 0.2058304294035992)\n",
      "Finished episode 594 after 201 timesteps (reward: 0.279251974521997)\n",
      "Finished episode 595 after 201 timesteps (reward: 0.23632239269245459)\n",
      "Finished episode 596 after 201 timesteps (reward: -0.03935723295206581)\n",
      "Finished episode 597 after 201 timesteps (reward: 0.4674673219569036)\n",
      "Finished episode 598 after 201 timesteps (reward: 0.20192765705419433)\n",
      "Finished episode 599 after 201 timesteps (reward: 0.11461725581704685)\n",
      "Finished episode 600 after 201 timesteps (reward: 0.25258645933790874)\n",
      "Finished episode 601 after 201 timesteps (reward: 0.5521216013913406)\n",
      "Finished episode 602 after 201 timesteps (reward: 0.5684056691419677)\n",
      "Finished episode 603 after 201 timesteps (reward: 0.03771503175189253)\n",
      "Finished episode 604 after 201 timesteps (reward: 0.11754679271223414)\n",
      "Finished episode 605 after 201 timesteps (reward: 0.48588600509568197)\n",
      "Finished episode 606 after 201 timesteps (reward: 0.20949502848151524)\n",
      "Finished episode 607 after 201 timesteps (reward: -0.005665668875247292)\n",
      "Finished episode 608 after 201 timesteps (reward: 0.7703401194200851)\n",
      "Finished episode 609 after 201 timesteps (reward: 0.044100077921810046)\n",
      "Finished episode 610 after 201 timesteps (reward: 0.30232350620843906)\n",
      "Finished episode 611 after 201 timesteps (reward: 0.36436403711398463)\n",
      "Finished episode 612 after 201 timesteps (reward: 0.33726945467930675)\n",
      "Finished episode 613 after 201 timesteps (reward: 0.13841523062427896)\n",
      "Finished episode 614 after 201 timesteps (reward: 0.6431272714850752)\n",
      "Finished episode 615 after 201 timesteps (reward: 0.3423299972853308)\n",
      "Finished episode 616 after 201 timesteps (reward: 0.5067662804549171)\n",
      "Finished episode 617 after 201 timesteps (reward: 0.09361848032315354)\n",
      "Finished episode 618 after 201 timesteps (reward: 0.3736351145634754)\n",
      "Finished episode 619 after 201 timesteps (reward: 0.41058401725594446)\n",
      "Finished episode 620 after 201 timesteps (reward: 0.5618633275535366)\n",
      "Finished episode 621 after 201 timesteps (reward: 0.2781976493377835)\n",
      "Finished episode 622 after 201 timesteps (reward: 0.24272822847450964)\n",
      "Finished episode 623 after 201 timesteps (reward: 0.32938343891313204)\n",
      "Finished episode 624 after 201 timesteps (reward: 0.19051110564365104)\n",
      "Finished episode 625 after 201 timesteps (reward: 0.06845229528463925)\n",
      "Finished episode 626 after 201 timesteps (reward: 0.7152704027767622)\n",
      "Finished episode 627 after 201 timesteps (reward: -0.0745404241887536)\n",
      "Finished episode 628 after 201 timesteps (reward: 0.19286773178620198)\n",
      "Finished episode 629 after 201 timesteps (reward: 0.6512670102675099)\n",
      "Finished episode 630 after 201 timesteps (reward: -0.0685932305442097)\n",
      "Finished episode 631 after 201 timesteps (reward: 0.18865741906519928)\n",
      "Finished episode 632 after 201 timesteps (reward: 0.21138490215079225)\n",
      "Finished episode 633 after 201 timesteps (reward: 0.025547981168725722)\n",
      "Finished episode 634 after 201 timesteps (reward: 0.2092878995467618)\n",
      "Finished episode 635 after 201 timesteps (reward: 0.12221039685806992)\n",
      "Finished episode 636 after 201 timesteps (reward: 0.30419410523667784)\n",
      "Finished episode 637 after 201 timesteps (reward: 0.25806125449836326)\n",
      "Finished episode 638 after 201 timesteps (reward: 0.2783592758449539)\n",
      "Finished episode 639 after 201 timesteps (reward: 0.3066727752679243)\n",
      "Finished episode 640 after 201 timesteps (reward: 0.30769568321046786)\n",
      "Finished episode 641 after 201 timesteps (reward: 0.28208012809467375)\n",
      "Finished episode 642 after 201 timesteps (reward: 0.6188324592361844)\n",
      "Finished episode 643 after 201 timesteps (reward: 0.19258883459721662)\n",
      "Finished episode 644 after 201 timesteps (reward: 0.006783279185306322)\n",
      "Finished episode 645 after 201 timesteps (reward: 0.3174699040500018)\n",
      "Finished episode 646 after 201 timesteps (reward: 0.5412453783238491)\n",
      "Finished episode 647 after 201 timesteps (reward: -0.04494583292990492)\n",
      "Finished episode 648 after 201 timesteps (reward: 0.3252529979519448)\n",
      "Finished episode 649 after 201 timesteps (reward: 0.25491370572672234)\n",
      "Finished episode 650 after 201 timesteps (reward: 0.306112336480833)\n",
      "Finished episode 651 after 201 timesteps (reward: 0.01500526946931142)\n",
      "Finished episode 652 after 201 timesteps (reward: 0.43462246879946487)\n",
      "Finished episode 653 after 201 timesteps (reward: 0.28989656043627565)\n",
      "Finished episode 654 after 201 timesteps (reward: 0.27462274471825604)\n",
      "Finished episode 655 after 201 timesteps (reward: 0.09326251937789869)\n",
      "Finished episode 656 after 201 timesteps (reward: 0.4749276934316488)\n",
      "Finished episode 657 after 201 timesteps (reward: 0.35532712769812436)\n",
      "Finished episode 658 after 201 timesteps (reward: 0.35066819896860735)\n",
      "Finished episode 659 after 201 timesteps (reward: -0.010711265586444467)\n",
      "Finished episode 660 after 201 timesteps (reward: 0.29199933422885405)\n",
      "Finished episode 661 after 201 timesteps (reward: 0.18452445157763836)\n",
      "Finished episode 662 after 201 timesteps (reward: 0.8317718391837375)\n",
      "Finished episode 663 after 201 timesteps (reward: 0.28781921498721885)\n",
      "Finished episode 664 after 201 timesteps (reward: 0.14436194294042481)\n",
      "Finished episode 665 after 201 timesteps (reward: 0.308506867161212)\n",
      "Finished episode 666 after 201 timesteps (reward: 0.09806488392006087)\n",
      "Finished episode 667 after 201 timesteps (reward: 0.09285121032699867)\n",
      "Finished episode 668 after 201 timesteps (reward: 0.08244843692965044)\n",
      "Finished episode 669 after 201 timesteps (reward: 0.2480707908609896)\n",
      "Finished episode 670 after 201 timesteps (reward: -0.0721979540425717)\n",
      "Finished episode 671 after 201 timesteps (reward: 0.6438174303164984)\n",
      "Finished episode 672 after 201 timesteps (reward: 0.4370786793822166)\n",
      "Finished episode 673 after 201 timesteps (reward: 0.42080923802581127)\n",
      "Finished episode 674 after 201 timesteps (reward: 0.47943866722532363)\n",
      "Finished episode 675 after 201 timesteps (reward: 0.23067548813368385)\n",
      "Finished episode 676 after 201 timesteps (reward: 0.24300908168290147)\n",
      "Finished episode 677 after 201 timesteps (reward: -0.036574862086569244)\n",
      "Finished episode 678 after 201 timesteps (reward: 0.4071542876223433)\n",
      "Finished episode 679 after 201 timesteps (reward: 0.08768983035499794)\n",
      "Finished episode 680 after 201 timesteps (reward: 0.13942251236927292)\n",
      "Finished episode 681 after 201 timesteps (reward: 0.21197434597355697)\n",
      "Finished episode 682 after 201 timesteps (reward: 0.37275587000633786)\n",
      "Finished episode 683 after 201 timesteps (reward: 0.5716490561222604)\n",
      "Finished episode 684 after 201 timesteps (reward: 0.2812175897955159)\n",
      "Finished episode 685 after 201 timesteps (reward: 0.3842633969602963)\n",
      "Finished episode 686 after 201 timesteps (reward: 0.2533232030676748)\n",
      "Finished episode 687 after 201 timesteps (reward: 0.23578872306196444)\n",
      "Finished episode 688 after 201 timesteps (reward: 0.2339589848559448)\n",
      "Finished episode 689 after 201 timesteps (reward: 0.47988233947562475)\n",
      "Finished episode 690 after 201 timesteps (reward: 0.2461135652611922)\n",
      "Finished episode 691 after 201 timesteps (reward: 0.14916157185421688)\n",
      "Finished episode 692 after 201 timesteps (reward: 0.4576196163822324)\n",
      "Finished episode 693 after 201 timesteps (reward: 0.35009113319938284)\n",
      "Finished episode 694 after 201 timesteps (reward: 0.39292170315626207)\n",
      "Finished episode 695 after 201 timesteps (reward: 0.41745639740109924)\n",
      "Finished episode 696 after 201 timesteps (reward: 0.15522115297259143)\n",
      "Finished episode 697 after 201 timesteps (reward: 0.5236700551359856)\n",
      "Finished episode 698 after 201 timesteps (reward: 0.32619418446137544)\n",
      "Finished episode 699 after 201 timesteps (reward: 0.45127946501329474)\n",
      "Finished episode 700 after 201 timesteps (reward: 0.43637304330228566)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 701 after 201 timesteps (reward: 0.4524068766759857)\n",
      "Finished episode 702 after 201 timesteps (reward: 0.21298975524066557)\n",
      "Finished episode 703 after 201 timesteps (reward: 0.2568843501805671)\n",
      "Finished episode 704 after 201 timesteps (reward: 0.14362626898300074)\n",
      "Finished episode 705 after 201 timesteps (reward: 0.43584900666636744)\n",
      "Finished episode 706 after 201 timesteps (reward: 0.09126021583937158)\n",
      "Finished episode 707 after 201 timesteps (reward: 0.3462601837586952)\n",
      "Finished episode 708 after 201 timesteps (reward: 0.32077522762251076)\n",
      "Finished episode 709 after 201 timesteps (reward: 0.7959382374594205)\n",
      "Finished episode 710 after 201 timesteps (reward: 0.4190414683334173)\n",
      "Finished episode 711 after 201 timesteps (reward: 0.3569721081755196)\n",
      "Finished episode 712 after 201 timesteps (reward: 0.2610738901571045)\n",
      "Finished episode 713 after 201 timesteps (reward: 0.47902624071153177)\n",
      "Finished episode 714 after 201 timesteps (reward: 0.5644565958679846)\n",
      "Finished episode 715 after 201 timesteps (reward: 0.11405861296236833)\n",
      "Finished episode 716 after 201 timesteps (reward: 0.19506911275015398)\n",
      "Finished episode 717 after 201 timesteps (reward: 0.0912537728039488)\n",
      "Finished episode 718 after 201 timesteps (reward: 0.27543985012586136)\n",
      "Finished episode 719 after 201 timesteps (reward: 0.2852511587147497)\n",
      "Finished episode 720 after 201 timesteps (reward: 0.3342627195074583)\n",
      "Finished episode 721 after 201 timesteps (reward: 0.1743961552382552)\n",
      "Finished episode 722 after 201 timesteps (reward: 0.2725394178248405)\n",
      "Finished episode 723 after 201 timesteps (reward: 0.430814532118724)\n",
      "Finished episode 724 after 201 timesteps (reward: 0.5436979534435538)\n",
      "Finished episode 725 after 201 timesteps (reward: 0.4262679005961732)\n",
      "Finished episode 726 after 201 timesteps (reward: 0.08378484861326699)\n",
      "Finished episode 727 after 201 timesteps (reward: 0.7448239845545956)\n",
      "Finished episode 728 after 201 timesteps (reward: 0.21619532777390996)\n",
      "Finished episode 729 after 201 timesteps (reward: 0.30068397820002835)\n",
      "Finished episode 730 after 201 timesteps (reward: 0.44487662104951886)\n",
      "Finished episode 731 after 201 timesteps (reward: 0.21630191540380397)\n",
      "Finished episode 732 after 201 timesteps (reward: 0.08283039260780896)\n",
      "Finished episode 733 after 201 timesteps (reward: 0.0857656345594425)\n",
      "Finished episode 734 after 201 timesteps (reward: 0.20394554008898264)\n",
      "Finished episode 735 after 201 timesteps (reward: 0.18862574140620508)\n",
      "Finished episode 736 after 201 timesteps (reward: 0.25601434930335476)\n",
      "Finished episode 737 after 201 timesteps (reward: 0.4273188298171956)\n",
      "Finished episode 738 after 201 timesteps (reward: 0.43660993229725775)\n",
      "Finished episode 739 after 201 timesteps (reward: 0.35887943168321335)\n",
      "Finished episode 740 after 201 timesteps (reward: 0.09854862879633049)\n",
      "Finished episode 741 after 201 timesteps (reward: 0.299079058420065)\n",
      "Finished episode 742 after 201 timesteps (reward: 0.5752193119856808)\n",
      "Finished episode 743 after 201 timesteps (reward: 0.30767087540040217)\n",
      "Finished episode 744 after 201 timesteps (reward: 0.12899477396646777)\n",
      "Finished episode 745 after 201 timesteps (reward: 0.38027648374711154)\n",
      "Finished episode 746 after 201 timesteps (reward: 0.5059949606662287)\n",
      "Finished episode 747 after 201 timesteps (reward: 0.07858284652340392)\n",
      "Finished episode 748 after 201 timesteps (reward: 0.422475415845384)\n",
      "Finished episode 749 after 201 timesteps (reward: 0.08496868605399951)\n",
      "Finished episode 750 after 201 timesteps (reward: 0.26324052857482383)\n",
      "Finished episode 751 after 201 timesteps (reward: 0.44123871810952137)\n",
      "Finished episode 752 after 201 timesteps (reward: 0.14869613804187137)\n",
      "Finished episode 753 after 201 timesteps (reward: 0.3207770606526234)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1e92672a033c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Start learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_finished\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-f4325df00330>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, timesteps, episodes, max_episode_timesteps, deterministic, episode_finished)\u001b[0m\n\u001b[1;32m     89\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                     \u001b[0;31m# print(\"reward\", reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7c5ba1afe6d4>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mDict\u001b[0m \u001b[0mof\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboolean\u001b[0m \u001b[0mindicating\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreward\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mcurrent_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msymbol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7c5ba1afe6d4>\u001b[0m in \u001b[0;36m_get_bar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin_t\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_time\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mbar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mbar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute_bar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no slices here, handle elsewhere'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;31m# may need to box a datelike-scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mfast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m   3547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3548\u001b[0m         \u001b[0;31m# unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3549\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_interleaved_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3550\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3551\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_interleaved_dtype\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   4502\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4504\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_common_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4506\u001b[0m     \u001b[0;31m# only numpy compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mfind_common_type\u001b[0;34m(types)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_common_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36mfind_common_type\u001b[0;34m(array_types, scalar_types)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \"\"\"\n\u001b[0;32m-> 1015\u001b[0;31m     \u001b[0marray_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0mscalar_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscalar_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorforce.agents import PPOAgent\n",
    "# from tensorforce.execution import Runner\n",
    "\n",
    "env = TradeEnvironment(data)\n",
    "num_stocks = env.num_stocks\n",
    "# Create a Proximal Policy Optimization agent\n",
    "agent = PPOAgent(\n",
    "    states_spec=dict(type='float', shape=(num_stocks,)),\n",
    "    actions_spec=dict(type='float', shape=(num_stocks,)),\n",
    "    network_spec=[\n",
    "        dict(type=\"flatten\"),\n",
    "        dict(type='dense', size=64),\n",
    "        dict(type='dense', size=64)\n",
    "    ],\n",
    "    distributions_spec=dict(action=dict(type=\"dirichlet\",\n",
    "                                        alphas=np.zeros(num_stocks),\n",
    "                                        min_value=0.,\n",
    "                                        max_value=1.)),\n",
    "    batch_size=1000,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4\n",
    "    )\n",
    ")\n",
    "\n",
    "# Poll new state from client\n",
    "state = env.reset()\n",
    "\n",
    "runner = Runner(\n",
    "        agent=agent,\n",
    "        environment=env,\n",
    "        repeat_actions=1\n",
    "    )\n",
    "\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=3000, max_episode_timesteps=200, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.core.memories import Replay\n",
    "\n",
    "\n",
    "class SeqReplay(Replay):\n",
    "    \"\"\"\n",
    "    Replay memory to store observations and sample mini batches for training from.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, states_spec, actions_spec, capacity, random_sampling=True, beta=0.1):\n",
    "        self.beta = beta\n",
    "        super(SeqReplay, self).__init__(states_spec, actions_spec, capacity, random_sampling)\n",
    "        self.capacity = capacity\n",
    "        state_shape = list(state[\"shape\"])[1:]\n",
    "        self.length = state[\"shape\"][0]\n",
    "        self.states = {name: np.zeros((capacity,) + tuple(state_shape), dtype=util.np_dtype(state['type'])) for name, state in states_spec.items()}\n",
    "        self.internals = None\n",
    "        self.actions = {name: np.zeros((capacity,) + tuple(action['shape']), dtype=util.np_dtype(action['type'])) for name, action in actions_spec.items()}\n",
    "        self.terminal = np.zeros((capacity,), dtype=util.np_dtype('bool'))\n",
    "        self.reward = np.zeros((capacity,), dtype=util.np_dtype('float'))\n",
    "\n",
    "        self.size = 0\n",
    "        self.index = 0\n",
    "        self.random_sampling = random_sampling\n",
    "        \n",
    "    def _get_seq_index(self, X, index):\n",
    "        X_seq = []\n",
    "        for i in index:\n",
    "            idx = np.arange(i - self.length + 1, i + 1) % self.size\n",
    "            X_seq.append(X.take(idx, axis=0))\n",
    "        return np.array(X_seq)\n",
    "    \n",
    "    def _sample_index(self, shape, maximum, length, start_index):\n",
    "        assert maximum > length\n",
    "        num_cand = maximum - length + 1\n",
    "        samples = (np.random.geometric(self.beta, shape) - 1) % num_cand\n",
    "        samples = (start_index - samples) % self.capacity\n",
    "        return samples\n",
    "\n",
    "    def get_batch(self, batch_size, next_states=False, keep_terminal_states=True):\n",
    "        \"\"\"\n",
    "        Samples a batch of the specified size by selecting a random start/end point and returning\n",
    "        the contained sequence or random indices depending on the field 'random_sampling'.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: The batch size\n",
    "            next_states: A boolean flag indicating whether 'next_states' values should be included\n",
    "            keep_terminal_states: A boolean flag indicating whether to keep terminal states when\n",
    "                `next_states` are requested. In this case, the next state is not from the same episode\n",
    "                and should probably not be used to learn a model of the environment. However, if the\n",
    "                environment produces sparse rewards (i.e. only one reward at the end of the episode) we\n",
    "                cannot exclude terminal states, as otherwise there would never be a reward to learn from.\n",
    "        Returns: A dict containing states, actions, rewards, terminals, internal states (and next states)\n",
    "        \"\"\"\n",
    "        if self.random_sampling:\n",
    "            if next_states:\n",
    "                indices = self._sample_index(batch_size, self.size - 1, self.length, self.index - 1)\n",
    "                terminal = self.terminal.take(indices)\n",
    "                if not keep_terminal_states:\n",
    "                    while np.any(terminal):\n",
    "                        alternative = np.random.randint(self.size - 1, size=batch_size)\n",
    "                        indices = np.where(terminal, alternative, indices)\n",
    "                        terminal = self.terminal.take(indices)\n",
    "            else:\n",
    "                indices = self._sample_index(batch_size, self.size - 1, self.length, self.index - 1)\n",
    "\n",
    "            states = {name: self._get_seq_index(state, indices) for name, state in self.states.items()}\n",
    "            internals = [self._get_seq_index(internal, indices) for internal in self.internals]\n",
    "            actions = {name: self._get_seq_index(action, indices) for name, action in self.actions.items()}\n",
    "            terminal = self._get_seq_index(self.terminal, indices)\n",
    "            reward = self._get_seq_index(self.reward, indices)\n",
    "            if next_states:\n",
    "                indices = (indices + 1) % self.capacity\n",
    "                next_states = {name: self._get_seq_index(state, indices) for name, state in self.states.items()}\n",
    "                next_internals = [self._get_seq_index(internal, indices) for internal in self.internals]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        batch = dict(states=states, internals=internals, actions=actions, terminal=terminal, reward=reward)\n",
    "        if next_states:\n",
    "            batch['next_states'] = next_states\n",
    "            batch['next_internals'] = next_internals\n",
    "        return batch\n",
    "\n",
    "    def update_batch(self, loss_per_instance):\n",
    "        pass\n",
    "\n",
    "    def set_memory(self, states, internals, actions, terminal, reward):\n",
    "        \"\"\"\n",
    "        Convenience function to set whole batches as memory content to bypass\n",
    "        calling the insert function for every single experience.\n",
    "        Args:\n",
    "            states:\n",
    "            internals:\n",
    "            actions:\n",
    "            terminal:\n",
    "            reward:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.size = len(terminal)\n",
    "\n",
    "        if len(terminal) == self.capacity:\n",
    "            # Assign directly if capacity matches size.\n",
    "            for name, state in states.items():\n",
    "                self.states[name] = np.asarray(state)\n",
    "            self.internals = [np.asarray(internal) for internal in internals]\n",
    "            for name, action in actions.items():\n",
    "                self.actions[name] = np.asarray(action)\n",
    "            self.terminal = np.asarray(terminal)\n",
    "            self.reward = np.asarray(reward)\n",
    "            # Filled capacity to point of index wrap\n",
    "            self.index = 0\n",
    "\n",
    "        else:\n",
    "            # Otherwise partial assignment.\n",
    "            if self.internals is None and internals is not None:\n",
    "                self.internals = [np.zeros((self.capacity,) + internal.shape, internal.dtype) for internal\n",
    "                                  in internals]\n",
    "\n",
    "            for name, state in states.items():\n",
    "                self.states[name][:len(state)] = state\n",
    "            for n, internal in enumerate(internals):\n",
    "                self.internals[n][:len(internal)] = internal\n",
    "            for name, action in actions.items():\n",
    "                self.actions[name][:len(action)] = action\n",
    "            self.terminal[:len(terminal)] = terminal\n",
    "            self.reward[:len(reward)] = reward\n",
    "            self.index = len(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorforce import util, TensorForceError\n",
    "from tensorforce.models import DistributionModel\n",
    "from tensorforce.core.networks import Network\n",
    "from tensorforce.core.optimizers import Synchronization\n",
    "\n",
    "\n",
    "class DDPGModel(DistributionModel):\n",
    "    \"\"\"\n",
    "    Q-value model.\n",
    "    \"\"\"\n",
    "    def tf_actions_and_internals(self, states, internals, update, deterministic):\n",
    "        \"\"\"You have to implement to have connections between actions and input\"\"\"\n",
    "        embedding, internals = self.network.apply(x=states,\n",
    "                                                  internals=internals,\n",
    "                                                  update=update,\n",
    "                                                  return_internals=True)\n",
    "        actions = dict()\n",
    "        for name, distribution in self.distributions.items():\n",
    "            distr_params = distribution.parameterize(x=embedding)\n",
    "            actions[name] = distribution.sample(distr_params=distr_params, deterministic=deterministic)\n",
    "        return actions, internals\n",
    "    \n",
    "    def tf_discounted_cumulative_reward(self, terminal, reward, discount, final_reward=0.0):\n",
    "        \"\"\"\n",
    "        Creates the TensorFlow operations for calculating the discounted cumulative rewards\n",
    "        for a given sequence of rewards.\n",
    "        Args:\n",
    "            terminal: Terminal boolean tensor.\n",
    "            reward: Reward tensor.\n",
    "            discount: Discount factor.\n",
    "            final_reward: Last reward value in the sequence.\n",
    "        Returns:\n",
    "            Discounted cumulative reward tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: n-step cumulative reward (particularly for envs without terminal)\n",
    "\n",
    "        def cumulate(cumulative, reward_and_terminal):\n",
    "            rew, term = reward_and_terminal\n",
    "            return tf.where(\n",
    "                condition=term,\n",
    "                x=rew,\n",
    "                y=(rew + cumulative * discount)\n",
    "            )\n",
    "\n",
    "        # Reverse since reward cumulation is calculated right-to-left, but tf.scan only works left-to-right\n",
    "        reward = tf.reverse(tensor=reward, axis=(0,))\n",
    "        terminal = tf.reverse(tensor=terminal, axis=(0,))\n",
    "\n",
    "        reward = tf.scan(fn=cumulate, elems=(reward, terminal), initializer=final_reward)\n",
    "\n",
    "        return tf.reverse(tensor=reward, axis=(0,))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        states_spec,\n",
    "        actions_spec,\n",
    "        network_spec,\n",
    "        device,\n",
    "        session_config,\n",
    "        scope,\n",
    "        saver_spec,\n",
    "        summary_spec,\n",
    "        distributed_spec,\n",
    "        optimizer,\n",
    "        discount,\n",
    "        variable_noise,\n",
    "        states_preprocessing_spec,\n",
    "        explorations_spec,\n",
    "        reward_preprocessing_spec,\n",
    "        distributions_spec,\n",
    "        entropy_regularization,\n",
    "        target_sync_frequency,\n",
    "        target_update_weight,\n",
    "        double_q_model,\n",
    "        huber_loss,\n",
    "        # TEMP: Random sampling fix\n",
    "        random_sampling_fix\n",
    "    ):\n",
    "        self.target_sync_frequency = target_sync_frequency\n",
    "        self.target_update_weight = target_update_weight\n",
    "\n",
    "        self.double_q_model = double_q_model\n",
    "\n",
    "        assert huber_loss is None or huber_loss > 0.0\n",
    "        self.huber_loss = huber_loss\n",
    "\n",
    "        # TEMP: Random sampling fix\n",
    "        self.random_sampling_fix = random_sampling_fix\n",
    "\n",
    "        super(QModel, self).__init__(\n",
    "            states_spec=states_spec,\n",
    "            actions_spec=actions_spec,\n",
    "            network_spec=network_spec,\n",
    "            device=device,\n",
    "            session_config=session_config,\n",
    "            scope=scope,\n",
    "            saver_spec=saver_spec,\n",
    "            summary_spec=summary_spec,\n",
    "            distributed_spec=distributed_spec,\n",
    "            optimizer=optimizer,\n",
    "            discount=discount,\n",
    "            variable_noise=variable_noise,\n",
    "            states_preprocessing_spec=states_preprocessing_spec,\n",
    "            explorations_spec=explorations_spec,\n",
    "            reward_preprocessing_spec=reward_preprocessing_spec,\n",
    "            distributions_spec=distributions_spec,\n",
    "            entropy_regularization=entropy_regularization,\n",
    "        )\n",
    "\n",
    "    def initialize(self, custom_getter):\n",
    "        super(QModel, self).initialize(custom_getter)\n",
    "\n",
    "        # TEMP: Random sampling fix\n",
    "        if self.random_sampling_fix:\n",
    "            self.next_states_input = dict()\n",
    "            for name, state in self.states_spec.items():\n",
    "                self.next_states_input[name] = tf.placeholder(\n",
    "                    dtype=util.tf_dtype(state['type']),\n",
    "                    shape=(None,) + tuple(state['shape']),\n",
    "                    name=('next-' + name)\n",
    "                )\n",
    "\n",
    "        # Target network\n",
    "        self.target_network = Network.from_spec(\n",
    "            spec=self.network_spec,\n",
    "            kwargs=dict(scope='target', summary_labels=self.summary_labels)\n",
    "        )\n",
    "\n",
    "        # Target network optimizer\n",
    "        self.target_optimizer = Synchronization(\n",
    "            sync_frequency=self.target_sync_frequency,\n",
    "            update_weight=self.target_update_weight\n",
    "        )\n",
    "\n",
    "        # Target network distributions\n",
    "        self.target_distributions = self.create_distributions()\n",
    "\n",
    "    def tf_q_value(self, embedding, distr_params, action, name):\n",
    "        # Mainly for NAF.\n",
    "        return self.distributions[name].state_action_value(distr_params=distr_params, action=action)\n",
    "\n",
    "    def tf_q_delta(self, q_value, next_q_value, terminal, reward):\n",
    "        \"\"\"\n",
    "        Creates the deltas (or advantage) of the Q values.\n",
    "        :return: A list of deltas per action\n",
    "        \"\"\"\n",
    "        for _ in range(util.rank(q_value) - 1):\n",
    "            terminal = tf.expand_dims(input=terminal, axis=1)\n",
    "            reward = tf.expand_dims(input=reward, axis=1)\n",
    "\n",
    "        multiples = (1,) + util.shape(q_value)[1:]\n",
    "        terminal = tf.tile(input=terminal, multiples=multiples)\n",
    "        reward = tf.tile(input=reward, multiples=multiples)\n",
    "\n",
    "        zeros = tf.zeros_like(tensor=next_q_value)\n",
    "        next_q_value = tf.where(condition=terminal, x=zeros, y=(self.discount * next_q_value))\n",
    "\n",
    "        return reward + next_q_value - q_value  # tf.stop_gradient(q_target)\n",
    "\n",
    "    def tf_loss_per_instance(self, states, internals, actions, terminal, reward, update):\n",
    "        # TEMP: Random sampling fix\n",
    "        if self.random_sampling_fix:\n",
    "            next_states = {name: tf.identity(input=state) for name, state in self.next_states_input.items()}\n",
    "            next_states = self.fn_preprocess_states(states=next_states)\n",
    "            next_states = {name: tf.stop_gradient(input=state) for name, state in next_states.items()}\n",
    "\n",
    "            embedding, next_internals = self.network.apply(\n",
    "                x=states,\n",
    "                internals=internals,\n",
    "                update=update,\n",
    "                return_internals=True\n",
    "            )\n",
    "\n",
    "            # Both networks can use the same internals, could that be a problem?\n",
    "            # Otherwise need to handle internals indices correctly everywhere\n",
    "            target_embedding = self.target_network.apply(\n",
    "                x=next_states,\n",
    "                internals=next_internals,\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            embedding = self.network.apply(\n",
    "                x={name: state[:-1] for name, state in states.items()},\n",
    "                internals=[internal[:-1] for internal in internals],\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "            # Both networks can use the same internals, could that be a problem?\n",
    "            # Otherwise need to handle internals indices correctly everywhere\n",
    "            target_embedding = self.target_network.apply(\n",
    "                x={name: state[1:] for name, state in states.items()},\n",
    "                internals=[internal[1:] for internal in internals],\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "            actions = {name: action[:-1] for name, action in actions.items()}\n",
    "            terminal = terminal[:-1]\n",
    "            reward = reward[:-1]\n",
    "\n",
    "        deltas = list()\n",
    "        for name, distribution in self.distributions.items():\n",
    "            target_distribution = self.target_distributions[name]\n",
    "\n",
    "            distr_params = distribution.parameterize(x=embedding)\n",
    "            target_distr_params = target_distribution.parameterize(x=target_embedding)\n",
    "\n",
    "            q_value = self.tf_q_value(embedding=embedding, distr_params=distr_params, action=actions[name], name=name)\n",
    "\n",
    "            if self.double_q_model:\n",
    "                action_taken = distribution.sample(distr_params=distr_params, deterministic=True)\n",
    "            else:\n",
    "                action_taken = target_distribution.sample(distr_params=target_distr_params, deterministic=True)\n",
    "\n",
    "            next_q_value = target_distribution.state_action_value(distr_params=target_distr_params, action=action_taken)\n",
    "\n",
    "            delta = self.tf_q_delta(q_value=q_value, next_q_value=next_q_value, terminal=terminal, reward=reward)\n",
    "\n",
    "            collapsed_size = util.prod(util.shape(delta)[1:])\n",
    "            delta = tf.reshape(tensor=delta, shape=(-1, collapsed_size))\n",
    "\n",
    "            deltas.append(delta)\n",
    "\n",
    "        # Surrogate loss as the mean squared error between actual observed rewards and expected rewards\n",
    "        loss_per_instance = tf.reduce_mean(input_tensor=tf.concat(values=deltas, axis=1), axis=1)\n",
    "\n",
    "        # Optional Huber loss\n",
    "        if self.huber_loss is not None and self.huber_loss > 0.0:\n",
    "            return tf.where(\n",
    "                condition=(tf.abs(x=loss_per_instance) <= self.huber_loss),\n",
    "                x=(0.5 * tf.square(x=loss_per_instance)),\n",
    "                y=(self.huber_loss * (tf.abs(x=loss_per_instance) - 0.5 * self.huber_loss))\n",
    "            )\n",
    "        else:\n",
    "            return tf.square(x=loss_per_instance)\n",
    "\n",
    "    def tf_optimization(self, states, internals, actions, terminal, reward, update):\n",
    "        optimization = super(QModel, self).tf_optimization(\n",
    "            states=states,\n",
    "            internals=internals,\n",
    "            actions=actions,\n",
    "            terminal=terminal,\n",
    "            reward=reward,\n",
    "            update=update\n",
    "        )\n",
    "\n",
    "        network_distributions_variables = self.get_distributions_variables(self.distributions)\n",
    "        target_distributions_variables = self.get_distributions_variables(self.target_distributions)\n",
    "\n",
    "        target_optimization = self.target_optimizer.minimize(\n",
    "            time=self.timestep,\n",
    "            variables=self.target_network.get_variables() + target_distributions_variables,\n",
    "            source_variables=self.network.get_variables() + network_distributions_variables\n",
    "        )\n",
    "\n",
    "        return tf.group(optimization, target_optimization)\n",
    "\n",
    "    def get_variables(self, include_non_trainable=False):\n",
    "        model_variables = super(QModel, self).get_variables(include_non_trainable=include_non_trainable)\n",
    "\n",
    "        if include_non_trainable:\n",
    "            # Target network and optimizer variables only included if 'include_non_trainable' set\n",
    "            target_variables = self.target_network.get_variables(include_non_trainable=include_non_trainable)\n",
    "            target_distributions_variables = self.get_distributions_variables(self.target_distributions)\n",
    "            target_optimizer_variables = self.target_optimizer.get_variables()\n",
    "\n",
    "            return model_variables + target_variables + target_optimizer_variables + target_distributions_variables\n",
    "\n",
    "        else:\n",
    "            return model_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import TensorForceError\n",
    "from tensorforce.agents import MemoryAgent\n",
    "from tensorforce.models import QModel\n",
    "\n",
    "\n",
    "class DDPGAgent(MemoryAgent):\n",
    "    \"\"\"\n",
    "    Deep-Q-Network agent (DQN). The piece de resistance of deep reinforcement learning as described by\n",
    "    [Minh et al. (2015)](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html). Includes\n",
    "    an option for double-DQN (DDQN; [van Hasselt et al., 2015](https://arxiv.org/abs/1509.06461))\n",
    "    DQN chooses from one of a number of discrete actions by taking the maximum Q-value\n",
    "    from the value function with one output neuron per available action. DQN uses a replay memory for experience\n",
    "    playback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        states_spec,\n",
    "        actions_spec,\n",
    "        network_spec,\n",
    "        device=None,\n",
    "        session_config=None,\n",
    "        scope='dqn',\n",
    "        saver_spec=None,\n",
    "        summary_spec=None,\n",
    "        distributed_spec=None,\n",
    "        optimizer=None,\n",
    "        discount=0.99,\n",
    "        variable_noise=None,\n",
    "        states_preprocessing_spec=None,\n",
    "        explorations_spec=None,\n",
    "        reward_preprocessing_spec=None,\n",
    "        distributions_spec=None,\n",
    "        entropy_regularization=None,\n",
    "        target_sync_frequency=10000,\n",
    "        target_update_weight=1.0,\n",
    "        double_q_model=False,\n",
    "        huber_loss=None,\n",
    "        batched_observe=None,\n",
    "        batch_size=32,\n",
    "        memory=None,\n",
    "        first_update=10000,\n",
    "        update_frequency=4,\n",
    "        repeat_update=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a Deep-Q agent.\n",
    "        Args:\n",
    "            states_spec: Dict containing at least one state definition. In the case of a single state,\n",
    "               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state\n",
    "               is a dict itself with a unique name as its key.\n",
    "            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`\n",
    "                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.\n",
    "            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments\n",
    "                such as activation or regularisation. Full examples are in the examples/configs folder.\n",
    "            device: Device string specifying model device.\n",
    "            session_config: optional tf.ConfigProto with additional desired session configurations\n",
    "            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).\n",
    "            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use\n",
    "                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies\n",
    "                if a model is initially loaded (set to True) from a file `file`.\n",
    "            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`\n",
    "                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values\n",
    "                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.\n",
    "            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`\n",
    "                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow\n",
    "                cluster spec.\n",
    "            optimizer: Dict specifying optimizer type and its optional parameters, typically a `learning_rate`.\n",
    "                Available optimizer types include standard TensorFlow optimizers, `natural_gradient`,\n",
    "                and `evolutionary`. Consult the optimizer test or example configurations for more.\n",
    "            discount: Float specifying reward discount factor.\n",
    "            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).\n",
    "            states_preprocessing_spec: Optional list of states preprocessors to apply to state  \n",
    "                (e.g. `image_resize`, `grayscale`).\n",
    "            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  \n",
    "                or Gaussian noise).\n",
    "            reward_preprocessing_spec: Optional dict specifying reward preprocessing.\n",
    "            distributions_spec: Optional dict specifying action distributions to override default distribution choices.\n",
    "                Must match action names.\n",
    "            entropy_regularization: Optional positive float specifying an entropy regularization value.\n",
    "            target_sync_frequency: Interval between optimization calls synchronizing the target network.\n",
    "            target_update_weight: Update weight, 1.0 meaning a full assignment to target network from training network.\n",
    "            huber_loss: Optional flat specifying Huber-loss clipping.\n",
    "            batched_observe: Optional int specifying how many observe calls are batched into one session run.\n",
    "                Without batching, throughput will be lower because every `observe` triggers a session invocation to\n",
    "                update rewards in the graph.\n",
    "            batch_size: Int specifying batch size used to sample from memory. Should be smaller than memory size.\n",
    "            memory: Dict describing memory via `type` (e.g. `replay`) and `capacity`.\n",
    "            first_update: Int describing at which time step the first update is performed. Should be larger\n",
    "                than batch size.\n",
    "            update_frequency: Int specifying number of observe steps to perform until an update is executed.\n",
    "            repeat_update: Int specifying how many update steps are performed per update, where each update step implies\n",
    "                sampling a batch from the memory and passing it to the model.\n",
    "        \"\"\"\n",
    "\n",
    "        if network_spec is None:\n",
    "            raise TensorForceError(\"No network_spec provided.\")\n",
    "\n",
    "        if optimizer is None:\n",
    "            self.optimizer = dict(\n",
    "                type='adam',\n",
    "                learning_rate=1e-3\n",
    "            )\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "        if memory is None:\n",
    "            memory = dict(\n",
    "                type='replay',\n",
    "                capacity=100000\n",
    "            )\n",
    "        else:\n",
    "            self.memory = memory\n",
    "\n",
    "        self.network_spec = network_spec\n",
    "        self.device = device\n",
    "        self.session_config = session_config\n",
    "        self.scope = scope\n",
    "        self.saver_spec = saver_spec\n",
    "        self.summary_spec = summary_spec\n",
    "        self.distributed_spec = distributed_spec\n",
    "        self.discount = discount\n",
    "        self.variable_noise = variable_noise\n",
    "        self.states_preprocessing_spec = states_preprocessing_spec\n",
    "        self.explorations_spec = explorations_spec\n",
    "        self.reward_preprocessing_spec = reward_preprocessing_spec\n",
    "        self.distributions_spec = distributions_spec\n",
    "        self.entropy_regularization = entropy_regularization\n",
    "        self.target_sync_frequency = target_sync_frequency\n",
    "        self.target_update_weight = target_update_weight\n",
    "        self.double_q_model = double_q_model\n",
    "        self.huber_loss = huber_loss\n",
    "\n",
    "        super(DQNAgent, self).__init__(\n",
    "            states_spec=states_spec,\n",
    "            actions_spec=actions_spec,\n",
    "            batched_observe=batched_observe,\n",
    "            batch_size=batch_size,\n",
    "            memory=memory,\n",
    "            first_update=first_update,\n",
    "            update_frequency=update_frequency,\n",
    "            repeat_update=repeat_update\n",
    "        )\n",
    "\n",
    "    def initialize_model(self):\n",
    "        return QModel(\n",
    "            states_spec=self.states_spec,\n",
    "            actions_spec=self.actions_spec,\n",
    "            network_spec=self.network_spec,\n",
    "            device=self.device,\n",
    "            session_config=self.session_config,\n",
    "            scope=self.scope,\n",
    "            saver_spec=self.saver_spec,\n",
    "            summary_spec=self.summary_spec,\n",
    "            distributed_spec=self.distributed_spec,\n",
    "            optimizer=self.optimizer,\n",
    "            discount=self.discount,\n",
    "            variable_noise=self.variable_noise,\n",
    "            states_preprocessing_spec=self.states_preprocessing_spec,\n",
    "            explorations_spec=self.explorations_spec,\n",
    "            reward_preprocessing_spec=self.reward_preprocessing_spec,\n",
    "            distributions_spec=self.distributions_spec,\n",
    "            entropy_regularization=self.entropy_regularization,\n",
    "            target_sync_frequency=self.target_sync_frequency,\n",
    "            target_update_weight=self.target_update_weight,\n",
    "            double_q_model=self.double_q_model,\n",
    "            huber_loss=self.huber_loss,\n",
    "            # TEMP: Random sampling fix\n",
    "            random_sampling_fix=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestClass(object):\n",
    "    capacity = 100\n",
    "    index = 120\n",
    "    beta = .001\n",
    "    def _sample_index(self, shape, maximum, length, start_index):\n",
    "        assert maximum > length\n",
    "        num_cand = maximum - length + 1\n",
    "        samples = (np.random.geometric(self.beta, shape) - 1) % num_cand\n",
    "        samples = (start_index - samples) % self.capacity\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = TestClass()\n",
    "samples = test._sample_index(1000, 99, 10, 98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TestClass()\n",
    "priority = x.get_priority()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\"\\\n",
    "          .format(ep=r.episode, ts=r.episode_timestep, reward=r.episode_rewards[-1]))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.agents import agent\n",
    "\n",
    "\n",
    "config = Configuration(\n",
    "    memory=dict(\n",
    "        type='replay',\n",
    "        capacity=1000\n",
    "    ),\n",
    "    batch_size=8,\n",
    "    first_update=100,\n",
    "    target_sync_frequency=10\n",
    ")\n",
    "\n",
    "T = 10\n",
    "num_stock = 5\n",
    "\n",
    "# Network is an ordered list of layers\n",
    "network_spec = [dict(type='dense', size=32), dict(type='dense', size=32)]\n",
    "\n",
    "# Define a state\n",
    "states = dict(shape=(T, num_stock), type='float')\n",
    "\n",
    "# Define an action\n",
    "actions = dict(type='float', shape=(num_stock,))\n",
    "\n",
    "agent = DQNAgent(\n",
    "    states_spec=states,\n",
    "    actions_spec=actions,\n",
    "    network_spec=network_spec,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "class Agent(agent.Agent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(12).reshape((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.take(1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.agents import TRPOAgent\n",
    "\n",
    "agent = TRPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=[\n",
    "        dict(type='dense', size=64),\n",
    "        dict(type='dense', size=64)\n",
    "    ],\n",
    "    batch_size=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=network_spec,\n",
    "    batch_size=4096,\n",
    "    # BatchAgent\n",
    "    keep_last_timestep=True,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    # DistributionModel\n",
    "    distributions_spec=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode=None,\n",
    "    baseline=None,\n",
    "    baseline_optimizer=None,\n",
    "    gae_lambda=None,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    summary_spec=None,\n",
    "    distributed_spec=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"2017-09-22\"][\"Open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1][\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = pd.read_csv(\"data/Top100Cryptos/100 List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(time_index[0], time_index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_data = []\n",
    "for name in data.keys():\n",
    "    num_data.append(data[name].shape[0])\n",
    "name_list = np.array(list(data.keys()))[np.argsort(num_data)[::-1]]\n",
    "big_names = name_list[:10]\n",
    "\n",
    "data_ = dict()\n",
    "for name in big_names:\n",
    "    data_[name] = data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ = dict()\n",
    "for name in big_names:\n",
    "    data_[name] = data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_index = set()\n",
    "\n",
    "for key, val in data_.items():\n",
    "    time_index = time_index.union(set(val[\"Date\"].values))\n",
    "time_index = list(time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time(t):\n",
    "    m = {\n",
    "        'Jan' : \"01\",\n",
    "        'Feb' : \"02\",\n",
    "        'Mar' : \"03\",\n",
    "        'Apr' : \"04\",\n",
    "        'May' : \"05\",\n",
    "        'Jun' : \"06\",\n",
    "        'Jul' : \"07\",\n",
    "        'Aug' : \"08\",\n",
    "        'Sep' : \"09\", \n",
    "        'Oct' : \"10\",\n",
    "        'Nov' : \"11\",\n",
    "        'Dec' : \"12\"\n",
    "    }\n",
    "    t_list = t.replace(\",\", \"\").split()\n",
    "    t_list[0] = m[t_list[0]]\n",
    "    return \"-\".join([t_list[2], t_list[0], t_list[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = sorted([convert_time(t) for t in time_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = data_[big_names[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce import TensorForceError\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "agent_spec = {\n",
    "    \"type\": \"dqn_agent\",\n",
    "    \"batch_size\": 64,\n",
    "    \"memory\": {\n",
    "        \"type\": \"replay\",\n",
    "        \"capacity\": 10000\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"learning_rate\": 1e-3\n",
    "    },\n",
    "\n",
    "    \"discount\": 0.97,\n",
    "\n",
    "    \"exploration\": {\n",
    "        \"type\": \"epsilon_decay\",\n",
    "        \"initial_epsilon\": 1.0,\n",
    "        \"final_epsilon\": 0.1,\n",
    "        \"timesteps\": 1e6\n",
    "    }\n",
    "}\n",
    "\n",
    "gym_id = 'CartPole-v0'\n",
    "max_episodes = 10000\n",
    "max_timesteps = 1000\n",
    "\n",
    "environment = OpenAIGym(gym_id)\n",
    "network_spec = [\n",
    "        dict(type='dense', size=32, activation='tanh'),\n",
    "        dict(type='dense', size=32, activation='tanh')\n",
    "]\n",
    "\n",
    "agent = Agent.from_spec(\n",
    "    spec=agent_spec,\n",
    "    kwargs=dict(\n",
    "        states_spec=environment.states,\n",
    "        actions_spec=environment.actions,\n",
    "        network_spec=network_spec\n",
    "    )\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    repeat_actions=1\n",
    ")\n",
    "\n",
    "report_episodes = 10\n",
    "\n",
    "def episode_finished(r):\n",
    "    if r.episode % report_episodes == 0:\n",
    "        print(\"Finished episode {ep} after {ts} timesteps\".format(ep=r.episode, ts=r.timestep))\n",
    "        print(\"Episode reward: {}\".format(r.episode_rewards[-1]))\n",
    "        print(\"Average of last 100 rewards: {}\".format(sum(r.episode_rewards[-100:]) / 100))\n",
    "    return True\n",
    "\n",
    "runner.run(max_episodes, max_timesteps, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorforce."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
