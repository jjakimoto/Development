{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_ = {}\n",
    "\n",
    "for filename in os.listdir(\"data/Top100Cryptos/\"):\n",
    "    path = os.path.join(\"data/Top100Cryptos/\", filename)\n",
    "    try:\n",
    "        name = filename.split(\".\")[0]\n",
    "        data_[name] = pd.read_csv(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "num_data = []\n",
    "for name in data_.keys():\n",
    "    num_data.append(data_[name].shape[0])\n",
    "name_list = np.array(list(data_.keys()))[np.argsort(num_data)[::-1]]\n",
    "big_names = name_list[:10]\n",
    "\n",
    "data = dict()\n",
    "for name in big_names:\n",
    "    data[name] = data_[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time(t):\n",
    "    m = {\n",
    "        'Jan' : \"01\",\n",
    "        'Feb' : \"02\",\n",
    "        'Mar' : \"03\",\n",
    "        'Apr' : \"04\",\n",
    "        'May' : \"05\",\n",
    "        'Jun' : \"06\",\n",
    "        'June' : \"06\",\n",
    "        'Jul' : \"07\",\n",
    "        'Aug' : \"08\",\n",
    "        'Sep' : \"09\", \n",
    "        'Oct' : \"10\",\n",
    "        'Nov' : \"11\",\n",
    "        'Dec' : \"12\"\n",
    "    }\n",
    "    t_list = t.replace(\",\", \"\").split()\n",
    "    t_list[0] = m[t_list[0]]\n",
    "    return \"-\".join([t_list[2], t_list[0], t_list[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.environments import Environment\n",
    "\n",
    "class TradeEnvironment(Environment):\n",
    "    \"\"\"Environment only for close prices\"\"\"\n",
    "    def __init__(self, data, start=None, end=None):\n",
    "        time_index = set()\n",
    "        impute_data = {}\n",
    "        for key, val in data.items():\n",
    "            dates = val[\"Date\"].values\n",
    "            dates = [convert_time(d) for d in dates]\n",
    "            impute_data[key] = dict(time_range=(dates[-1], dates[0]),\n",
    "                                    impute_val=(val.iloc[-1], val.iloc[0]))\n",
    "            data[key].index = dates\n",
    "            time_index = time_index.union(set(dates))\n",
    "        self.time_index = sorted(list(time_index))\n",
    "        self.impute_data  = impute_data\n",
    "        if start is None:\n",
    "            self.start = self.time_index[0]\n",
    "        else:\n",
    "            self.start = min(start, self.time_index[0])\n",
    "        if end is None:\n",
    "            self.end = self.time_index[-1]\n",
    "        else:\n",
    "            self.end = max(end, self.time_index[-1])\n",
    "        self.data = data\n",
    "        self.symbols = list(data.keys())\n",
    "        self.num_stocks = len(self.symbols)\n",
    "        self.current_time = self.start\n",
    "        self.current_step = 0\n",
    "        # Use for calculate return\n",
    "        self.prev_states = self._get_bar()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset environment and setup for new episode.\n",
    "        Returns:\n",
    "            initial state of resetted environment.\n",
    "        \"\"\"\n",
    "        self.current_time = self.start\n",
    "        self.cirrent_step = 0\n",
    "        current_states = self._get_bar()\n",
    "        states = np.array([current_states[symbol][\"Close\"] for symbol in self.symbols])\n",
    "        return states\n",
    "\n",
    "    def execute(self, actions):\n",
    "        \"\"\"\n",
    "        Executes action, observes next state(s) and reward.\n",
    "        Args:\n",
    "            actions: Actions to execute.\n",
    "        Returns:\n",
    "            (Dict of) next state(s), boolean indicating terminal, and reward signal.\n",
    "        \"\"\"\n",
    "        current_states = self._get_bar()\n",
    "        returns = []\n",
    "        for symbol in self.symbols:\n",
    "            returns.append(current_states[symbol][\"Close\"] / self.prev_states[symbol][\"Close\"] - 1)\n",
    "        self.prev_states = current_states\n",
    "        states = np.array([current_states[symbol][\"Close\"] for symbol in self.symbols])\n",
    "        terminal = True\n",
    "        reward = np.sum(np.array(returns) * actions)\n",
    "        return states, terminal, reward\n",
    "        \n",
    "            \n",
    "    def _update_time(self):\n",
    "        index = self.time_index.index(self.current_time)\n",
    "        self.current_time = self.time_index[index + 1]\n",
    "        self.current_step += 1\n",
    "        \n",
    "    def _get_bar(self):\n",
    "        bar = {}\n",
    "        for symbol in self.symbols:\n",
    "            min_t = self.impute_data[symbol][\"time_range\"][0]\n",
    "            max_t = self.impute_data[symbol][\"time_range\"][1]\n",
    "            if (min_t <= self.current_time) and (max_t >= self.current_time):\n",
    "                bar[symbol] = self.data[symbol].loc[self.current_time]\n",
    "            elif min_t > self.current_time:\n",
    "                bar[symbol] = self.impute_data[symbol][\"impute_val\"][0]\n",
    "            else:\n",
    "                bar[symbol] = self.impute_data[symbol][\"impute_val\"][1]\n",
    "        return bar\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        \"\"\"\n",
    "        Return the state space. Might include subdicts if multiple states are available simultaneously.\n",
    "        Returns: dict of state properties (shape and type).\n",
    "        \"\"\"\n",
    "        return {'shape': (self.num_stocks,), 'type': 'float'}\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        \"\"\"\n",
    "        Return the action space. Might include subdicts if multiple actions are available simultaneously.\n",
    "        Returns: dict of action properties (continuous, number of actions)\n",
    "        \"\"\"\n",
    "        return {\"shape\": (self.num_stocks,), \"min_value\": 0., \"max_value\": 1., \"type\": \"float\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.core.memories import Replay\n",
    "\n",
    "\n",
    "class SeqReplay(Replay):\n",
    "    \"\"\"\n",
    "    Replay memory to store observations and sample mini batches for training from.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, states_spec, actions_spec, capacity, random_sampling=True, beta=0.1):\n",
    "        self.beta = beta\n",
    "        super(SeqReplay, self).__init__(states_spec, actions_spec, capacity, random_sampling)\n",
    "        self.capacity = capacity\n",
    "        state_shape = list(state[\"shape\"])[1:]\n",
    "        self.length = state[\"shape\"][0]\n",
    "        self.states = {name: np.zeros((capacity,) + tuple(state_shape), dtype=util.np_dtype(state['type'])) for name, state in states_spec.items()}\n",
    "        self.internals = None\n",
    "        self.actions = {name: np.zeros((capacity,) + tuple(action['shape']), dtype=util.np_dtype(action['type'])) for name, action in actions_spec.items()}\n",
    "        self.terminal = np.zeros((capacity,), dtype=util.np_dtype('bool'))\n",
    "        self.reward = np.zeros((capacity,), dtype=util.np_dtype('float'))\n",
    "\n",
    "        self.size = 0\n",
    "        self.index = 0\n",
    "        self.random_sampling = random_sampling\n",
    "        \n",
    "    def _get_seq_index(self, X, index):\n",
    "        X_seq = []\n",
    "        for i in index:\n",
    "            idx = np.arange(i - self.length + 1, i + 1) % self.size\n",
    "            X_seq.append(X.take(idx, axis=0))\n",
    "        return np.array(X_seq)\n",
    "    \n",
    "    def _sample_index(self, shape, maximum, length, start_index):\n",
    "        assert maximum > length\n",
    "        num_cand = maximum - length + 1\n",
    "        samples = (np.random.geometric(self.beta, shape) - 1) % num_cand\n",
    "        samples = (start_index - samples) % self.capacity\n",
    "        return samples\n",
    "\n",
    "    def get_batch(self, batch_size, next_states=False, keep_terminal_states=True):\n",
    "        \"\"\"\n",
    "        Samples a batch of the specified size by selecting a random start/end point and returning\n",
    "        the contained sequence or random indices depending on the field 'random_sampling'.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: The batch size\n",
    "            next_states: A boolean flag indicating whether 'next_states' values should be included\n",
    "            keep_terminal_states: A boolean flag indicating whether to keep terminal states when\n",
    "                `next_states` are requested. In this case, the next state is not from the same episode\n",
    "                and should probably not be used to learn a model of the environment. However, if the\n",
    "                environment produces sparse rewards (i.e. only one reward at the end of the episode) we\n",
    "                cannot exclude terminal states, as otherwise there would never be a reward to learn from.\n",
    "        Returns: A dict containing states, actions, rewards, terminals, internal states (and next states)\n",
    "        \"\"\"\n",
    "        if self.random_sampling:\n",
    "            if next_states:\n",
    "                indices = self._sample_index(batch_size, self.size - 1, self.length, self.index - 1)\n",
    "                terminal = self.terminal.take(indices)\n",
    "                if not keep_terminal_states:\n",
    "                    while np.any(terminal):\n",
    "                        alternative = np.random.randint(self.size - 1, size=batch_size)\n",
    "                        indices = np.where(terminal, alternative, indices)\n",
    "                        terminal = self.terminal.take(indices)\n",
    "            else:\n",
    "                indices = self._sample_index(batch_size, self.size - 1, self.length, self.index - 1)\n",
    "\n",
    "            states = {name: self._get_seq_index(state, indices) for name, state in self.states.items()}\n",
    "            internals = [self._get_seq_index(internal, indices) for internal in self.internals]\n",
    "            actions = {name: self._get_seq_index(action, indices) for name, action in self.actions.items()}\n",
    "            terminal = self._get_seq_index(self.terminal, indices)\n",
    "            reward = self._get_seq_index(self.reward, indices)\n",
    "            if next_states:\n",
    "                indices = (indices + 1) % self.capacity\n",
    "                next_states = {name: self._get_seq_index(state, indices) for name, state in self.states.items()}\n",
    "                next_internals = [self._get_seq_index(internal, indices) for internal in self.internals]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        batch = dict(states=states, internals=internals, actions=actions, terminal=terminal, reward=reward)\n",
    "        if next_states:\n",
    "            batch['next_states'] = next_states\n",
    "            batch['next_internals'] = next_internals\n",
    "        return batch\n",
    "\n",
    "    def update_batch(self, loss_per_instance):\n",
    "        pass\n",
    "\n",
    "    def set_memory(self, states, internals, actions, terminal, reward):\n",
    "        \"\"\"\n",
    "        Convenience function to set whole batches as memory content to bypass\n",
    "        calling the insert function for every single experience.\n",
    "        Args:\n",
    "            states:\n",
    "            internals:\n",
    "            actions:\n",
    "            terminal:\n",
    "            reward:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.size = len(terminal)\n",
    "\n",
    "        if len(terminal) == self.capacity:\n",
    "            # Assign directly if capacity matches size.\n",
    "            for name, state in states.items():\n",
    "                self.states[name] = np.asarray(state)\n",
    "            self.internals = [np.asarray(internal) for internal in internals]\n",
    "            for name, action in actions.items():\n",
    "                self.actions[name] = np.asarray(action)\n",
    "            self.terminal = np.asarray(terminal)\n",
    "            self.reward = np.asarray(reward)\n",
    "            # Filled capacity to point of index wrap\n",
    "            self.index = 0\n",
    "\n",
    "        else:\n",
    "            # Otherwise partial assignment.\n",
    "            if self.internals is None and internals is not None:\n",
    "                self.internals = [np.zeros((self.capacity,) + internal.shape, internal.dtype) for internal\n",
    "                                  in internals]\n",
    "\n",
    "            for name, state in states.items():\n",
    "                self.states[name][:len(state)] = state\n",
    "            for n, internal in enumerate(internals):\n",
    "                self.internals[n][:len(internal)] = internal\n",
    "            for name, action in actions.items():\n",
    "                self.actions[name][:len(action)] = action\n",
    "            self.terminal[:len(terminal)] = terminal\n",
    "            self.reward[:len(reward)] = reward\n",
    "            self.index = len(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorforce import util, TensorForceError\n",
    "from tensorforce.models import DistributionModel\n",
    "from tensorforce.core.networks import Network\n",
    "from tensorforce.core.optimizers import Synchronization\n",
    "\n",
    "\n",
    "class DDPGModel(DistributionModel):\n",
    "    \"\"\"\n",
    "    Q-value model.\n",
    "    \"\"\"\n",
    "    def tf_actions_and_internals(self, states, internals, update, deterministic):\n",
    "        \"\"\"You have to implement to have connections between actions and input\"\"\"\n",
    "        embedding, internals = self.network.apply(x=states,\n",
    "                                                  internals=internals,\n",
    "                                                  update=update,\n",
    "                                                  return_internals=True)\n",
    "        actions = dict()\n",
    "        for name, distribution in self.distributions.items():\n",
    "            distr_params = distribution.parameterize(x=embedding)\n",
    "            actions[name] = distribution.sample(distr_params=distr_params, deterministic=deterministic)\n",
    "        return actions, internals\n",
    "    \n",
    "    def tf_discounted_cumulative_reward(self, terminal, reward, discount, final_reward=0.0):\n",
    "        \"\"\"\n",
    "        Creates the TensorFlow operations for calculating the discounted cumulative rewards\n",
    "        for a given sequence of rewards.\n",
    "        Args:\n",
    "            terminal: Terminal boolean tensor.\n",
    "            reward: Reward tensor.\n",
    "            discount: Discount factor.\n",
    "            final_reward: Last reward value in the sequence.\n",
    "        Returns:\n",
    "            Discounted cumulative reward tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: n-step cumulative reward (particularly for envs without terminal)\n",
    "\n",
    "        def cumulate(cumulative, reward_and_terminal):\n",
    "            rew, term = reward_and_terminal\n",
    "            return tf.where(\n",
    "                condition=term,\n",
    "                x=rew,\n",
    "                y=(rew + cumulative * discount)\n",
    "            )\n",
    "\n",
    "        # Reverse since reward cumulation is calculated right-to-left, but tf.scan only works left-to-right\n",
    "        reward = tf.reverse(tensor=reward, axis=(0,))\n",
    "        terminal = tf.reverse(tensor=terminal, axis=(0,))\n",
    "\n",
    "        reward = tf.scan(fn=cumulate, elems=(reward, terminal), initializer=final_reward)\n",
    "\n",
    "        return tf.reverse(tensor=reward, axis=(0,))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        states_spec,\n",
    "        actions_spec,\n",
    "        network_spec,\n",
    "        device,\n",
    "        session_config,\n",
    "        scope,\n",
    "        saver_spec,\n",
    "        summary_spec,\n",
    "        distributed_spec,\n",
    "        optimizer,\n",
    "        discount,\n",
    "        variable_noise,\n",
    "        states_preprocessing_spec,\n",
    "        explorations_spec,\n",
    "        reward_preprocessing_spec,\n",
    "        distributions_spec,\n",
    "        entropy_regularization,\n",
    "        target_sync_frequency,\n",
    "        target_update_weight,\n",
    "        double_q_model,\n",
    "        huber_loss,\n",
    "        # TEMP: Random sampling fix\n",
    "        random_sampling_fix\n",
    "    ):\n",
    "        self.target_sync_frequency = target_sync_frequency\n",
    "        self.target_update_weight = target_update_weight\n",
    "\n",
    "        self.double_q_model = double_q_model\n",
    "\n",
    "        assert huber_loss is None or huber_loss > 0.0\n",
    "        self.huber_loss = huber_loss\n",
    "\n",
    "        # TEMP: Random sampling fix\n",
    "        self.random_sampling_fix = random_sampling_fix\n",
    "\n",
    "        super(QModel, self).__init__(\n",
    "            states_spec=states_spec,\n",
    "            actions_spec=actions_spec,\n",
    "            network_spec=network_spec,\n",
    "            device=device,\n",
    "            session_config=session_config,\n",
    "            scope=scope,\n",
    "            saver_spec=saver_spec,\n",
    "            summary_spec=summary_spec,\n",
    "            distributed_spec=distributed_spec,\n",
    "            optimizer=optimizer,\n",
    "            discount=discount,\n",
    "            variable_noise=variable_noise,\n",
    "            states_preprocessing_spec=states_preprocessing_spec,\n",
    "            explorations_spec=explorations_spec,\n",
    "            reward_preprocessing_spec=reward_preprocessing_spec,\n",
    "            distributions_spec=distributions_spec,\n",
    "            entropy_regularization=entropy_regularization,\n",
    "        )\n",
    "\n",
    "    def initialize(self, custom_getter):\n",
    "        super(QModel, self).initialize(custom_getter)\n",
    "\n",
    "        # TEMP: Random sampling fix\n",
    "        if self.random_sampling_fix:\n",
    "            self.next_states_input = dict()\n",
    "            for name, state in self.states_spec.items():\n",
    "                self.next_states_input[name] = tf.placeholder(\n",
    "                    dtype=util.tf_dtype(state['type']),\n",
    "                    shape=(None,) + tuple(state['shape']),\n",
    "                    name=('next-' + name)\n",
    "                )\n",
    "\n",
    "        # Target network\n",
    "        self.target_network = Network.from_spec(\n",
    "            spec=self.network_spec,\n",
    "            kwargs=dict(scope='target', summary_labels=self.summary_labels)\n",
    "        )\n",
    "\n",
    "        # Target network optimizer\n",
    "        self.target_optimizer = Synchronization(\n",
    "            sync_frequency=self.target_sync_frequency,\n",
    "            update_weight=self.target_update_weight\n",
    "        )\n",
    "\n",
    "        # Target network distributions\n",
    "        self.target_distributions = self.create_distributions()\n",
    "\n",
    "    def tf_q_value(self, embedding, distr_params, action, name):\n",
    "        # Mainly for NAF.\n",
    "        return self.distributions[name].state_action_value(distr_params=distr_params, action=action)\n",
    "\n",
    "    def tf_q_delta(self, q_value, next_q_value, terminal, reward):\n",
    "        \"\"\"\n",
    "        Creates the deltas (or advantage) of the Q values.\n",
    "        :return: A list of deltas per action\n",
    "        \"\"\"\n",
    "        for _ in range(util.rank(q_value) - 1):\n",
    "            terminal = tf.expand_dims(input=terminal, axis=1)\n",
    "            reward = tf.expand_dims(input=reward, axis=1)\n",
    "\n",
    "        multiples = (1,) + util.shape(q_value)[1:]\n",
    "        terminal = tf.tile(input=terminal, multiples=multiples)\n",
    "        reward = tf.tile(input=reward, multiples=multiples)\n",
    "\n",
    "        zeros = tf.zeros_like(tensor=next_q_value)\n",
    "        next_q_value = tf.where(condition=terminal, x=zeros, y=(self.discount * next_q_value))\n",
    "\n",
    "        return reward + next_q_value - q_value  # tf.stop_gradient(q_target)\n",
    "\n",
    "    def tf_loss_per_instance(self, states, internals, actions, terminal, reward, update):\n",
    "        # TEMP: Random sampling fix\n",
    "        if self.random_sampling_fix:\n",
    "            next_states = {name: tf.identity(input=state) for name, state in self.next_states_input.items()}\n",
    "            next_states = self.fn_preprocess_states(states=next_states)\n",
    "            next_states = {name: tf.stop_gradient(input=state) for name, state in next_states.items()}\n",
    "\n",
    "            embedding, next_internals = self.network.apply(\n",
    "                x=states,\n",
    "                internals=internals,\n",
    "                update=update,\n",
    "                return_internals=True\n",
    "            )\n",
    "\n",
    "            # Both networks can use the same internals, could that be a problem?\n",
    "            # Otherwise need to handle internals indices correctly everywhere\n",
    "            target_embedding = self.target_network.apply(\n",
    "                x=next_states,\n",
    "                internals=next_internals,\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            embedding = self.network.apply(\n",
    "                x={name: state[:-1] for name, state in states.items()},\n",
    "                internals=[internal[:-1] for internal in internals],\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "            # Both networks can use the same internals, could that be a problem?\n",
    "            # Otherwise need to handle internals indices correctly everywhere\n",
    "            target_embedding = self.target_network.apply(\n",
    "                x={name: state[1:] for name, state in states.items()},\n",
    "                internals=[internal[1:] for internal in internals],\n",
    "                update=update\n",
    "            )\n",
    "\n",
    "            actions = {name: action[:-1] for name, action in actions.items()}\n",
    "            terminal = terminal[:-1]\n",
    "            reward = reward[:-1]\n",
    "\n",
    "        deltas = list()\n",
    "        for name, distribution in self.distributions.items():\n",
    "            target_distribution = self.target_distributions[name]\n",
    "\n",
    "            distr_params = distribution.parameterize(x=embedding)\n",
    "            target_distr_params = target_distribution.parameterize(x=target_embedding)\n",
    "\n",
    "            q_value = self.tf_q_value(embedding=embedding, distr_params=distr_params, action=actions[name], name=name)\n",
    "\n",
    "            if self.double_q_model:\n",
    "                action_taken = distribution.sample(distr_params=distr_params, deterministic=True)\n",
    "            else:\n",
    "                action_taken = target_distribution.sample(distr_params=target_distr_params, deterministic=True)\n",
    "\n",
    "            next_q_value = target_distribution.state_action_value(distr_params=target_distr_params, action=action_taken)\n",
    "\n",
    "            delta = self.tf_q_delta(q_value=q_value, next_q_value=next_q_value, terminal=terminal, reward=reward)\n",
    "\n",
    "            collapsed_size = util.prod(util.shape(delta)[1:])\n",
    "            delta = tf.reshape(tensor=delta, shape=(-1, collapsed_size))\n",
    "\n",
    "            deltas.append(delta)\n",
    "\n",
    "        # Surrogate loss as the mean squared error between actual observed rewards and expected rewards\n",
    "        loss_per_instance = tf.reduce_mean(input_tensor=tf.concat(values=deltas, axis=1), axis=1)\n",
    "\n",
    "        # Optional Huber loss\n",
    "        if self.huber_loss is not None and self.huber_loss > 0.0:\n",
    "            return tf.where(\n",
    "                condition=(tf.abs(x=loss_per_instance) <= self.huber_loss),\n",
    "                x=(0.5 * tf.square(x=loss_per_instance)),\n",
    "                y=(self.huber_loss * (tf.abs(x=loss_per_instance) - 0.5 * self.huber_loss))\n",
    "            )\n",
    "        else:\n",
    "            return tf.square(x=loss_per_instance)\n",
    "\n",
    "    def tf_optimization(self, states, internals, actions, terminal, reward, update):\n",
    "        optimization = super(QModel, self).tf_optimization(\n",
    "            states=states,\n",
    "            internals=internals,\n",
    "            actions=actions,\n",
    "            terminal=terminal,\n",
    "            reward=reward,\n",
    "            update=update\n",
    "        )\n",
    "\n",
    "        network_distributions_variables = self.get_distributions_variables(self.distributions)\n",
    "        target_distributions_variables = self.get_distributions_variables(self.target_distributions)\n",
    "\n",
    "        target_optimization = self.target_optimizer.minimize(\n",
    "            time=self.timestep,\n",
    "            variables=self.target_network.get_variables() + target_distributions_variables,\n",
    "            source_variables=self.network.get_variables() + network_distributions_variables\n",
    "        )\n",
    "\n",
    "        return tf.group(optimization, target_optimization)\n",
    "\n",
    "    def get_variables(self, include_non_trainable=False):\n",
    "        model_variables = super(QModel, self).get_variables(include_non_trainable=include_non_trainable)\n",
    "\n",
    "        if include_non_trainable:\n",
    "            # Target network and optimizer variables only included if 'include_non_trainable' set\n",
    "            target_variables = self.target_network.get_variables(include_non_trainable=include_non_trainable)\n",
    "            target_distributions_variables = self.get_distributions_variables(self.target_distributions)\n",
    "            target_optimizer_variables = self.target_optimizer.get_variables()\n",
    "\n",
    "            return model_variables + target_variables + target_optimizer_variables + target_distributions_variables\n",
    "\n",
    "        else:\n",
    "            return model_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce import TensorForceError\n",
    "from tensorforce.agents import MemoryAgent\n",
    "from tensorforce.models import QModel\n",
    "\n",
    "\n",
    "class DDPGAgent(MemoryAgent):\n",
    "    \"\"\"\n",
    "    Deep-Q-Network agent (DQN). The piece de resistance of deep reinforcement learning as described by\n",
    "    [Minh et al. (2015)](http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html). Includes\n",
    "    an option for double-DQN (DDQN; [van Hasselt et al., 2015](https://arxiv.org/abs/1509.06461))\n",
    "    DQN chooses from one of a number of discrete actions by taking the maximum Q-value\n",
    "    from the value function with one output neuron per available action. DQN uses a replay memory for experience\n",
    "    playback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        states_spec,\n",
    "        actions_spec,\n",
    "        network_spec,\n",
    "        device=None,\n",
    "        session_config=None,\n",
    "        scope='dqn',\n",
    "        saver_spec=None,\n",
    "        summary_spec=None,\n",
    "        distributed_spec=None,\n",
    "        optimizer=None,\n",
    "        discount=0.99,\n",
    "        variable_noise=None,\n",
    "        states_preprocessing_spec=None,\n",
    "        explorations_spec=None,\n",
    "        reward_preprocessing_spec=None,\n",
    "        distributions_spec=None,\n",
    "        entropy_regularization=None,\n",
    "        target_sync_frequency=10000,\n",
    "        target_update_weight=1.0,\n",
    "        double_q_model=False,\n",
    "        huber_loss=None,\n",
    "        batched_observe=None,\n",
    "        batch_size=32,\n",
    "        memory=None,\n",
    "        first_update=10000,\n",
    "        update_frequency=4,\n",
    "        repeat_update=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a Deep-Q agent.\n",
    "        Args:\n",
    "            states_spec: Dict containing at least one state definition. In the case of a single state,\n",
    "               keys `shape` and `type` are necessary. For multiple states, pass a dict of dicts where each state\n",
    "               is a dict itself with a unique name as its key.\n",
    "            actions_spec: Dict containing at least one action definition. Actions have types and either `num_actions`\n",
    "                for discrete actions or a `shape` for continuous actions. Consult documentation and tests for more.\n",
    "            network_spec: List of layers specifying a neural network via layer types, sizes and optional arguments\n",
    "                such as activation or regularisation. Full examples are in the examples/configs folder.\n",
    "            device: Device string specifying model device.\n",
    "            session_config: optional tf.ConfigProto with additional desired session configurations\n",
    "            scope: TensorFlow scope, defaults to agent name (e.g. `dqn`).\n",
    "            saver_spec: Dict specifying automated saving. Use `directory` to specify where checkpoints are saved. Use\n",
    "                either `seconds` or `steps` to specify how often the model should be saved. The `load` flag specifies\n",
    "                if a model is initially loaded (set to True) from a file `file`.\n",
    "            summary_spec: Dict specifying summaries for TensorBoard. Requires a 'directory' to store summaries, `steps`\n",
    "                or `seconds` to specify how often to save summaries, and a list of `labels` to indicate which values\n",
    "                to export, e.g. `losses`, `variables`. Consult neural network class and model for all available labels.\n",
    "            distributed_spec: Dict specifying distributed functionality. Use `parameter_server` and `replica_model`\n",
    "                Boolean flags to indicate workers and parameter servers. Use a `cluster_spec` key to pass a TensorFlow\n",
    "                cluster spec.\n",
    "            optimizer: Dict specifying optimizer type and its optional parameters, typically a `learning_rate`.\n",
    "                Available optimizer types include standard TensorFlow optimizers, `natural_gradient`,\n",
    "                and `evolutionary`. Consult the optimizer test or example configurations for more.\n",
    "            discount: Float specifying reward discount factor.\n",
    "            variable_noise: Experimental optional parameter specifying variable noise (NoisyNet).\n",
    "            states_preprocessing_spec: Optional list of states preprocessors to apply to state  \n",
    "                (e.g. `image_resize`, `grayscale`).\n",
    "            explorations_spec: Optional dict specifying action exploration type (epsilon greedy  \n",
    "                or Gaussian noise).\n",
    "            reward_preprocessing_spec: Optional dict specifying reward preprocessing.\n",
    "            distributions_spec: Optional dict specifying action distributions to override default distribution choices.\n",
    "                Must match action names.\n",
    "            entropy_regularization: Optional positive float specifying an entropy regularization value.\n",
    "            target_sync_frequency: Interval between optimization calls synchronizing the target network.\n",
    "            target_update_weight: Update weight, 1.0 meaning a full assignment to target network from training network.\n",
    "            huber_loss: Optional flat specifying Huber-loss clipping.\n",
    "            batched_observe: Optional int specifying how many observe calls are batched into one session run.\n",
    "                Without batching, throughput will be lower because every `observe` triggers a session invocation to\n",
    "                update rewards in the graph.\n",
    "            batch_size: Int specifying batch size used to sample from memory. Should be smaller than memory size.\n",
    "            memory: Dict describing memory via `type` (e.g. `replay`) and `capacity`.\n",
    "            first_update: Int describing at which time step the first update is performed. Should be larger\n",
    "                than batch size.\n",
    "            update_frequency: Int specifying number of observe steps to perform until an update is executed.\n",
    "            repeat_update: Int specifying how many update steps are performed per update, where each update step implies\n",
    "                sampling a batch from the memory and passing it to the model.\n",
    "        \"\"\"\n",
    "\n",
    "        if network_spec is None:\n",
    "            raise TensorForceError(\"No network_spec provided.\")\n",
    "\n",
    "        if optimizer is None:\n",
    "            self.optimizer = dict(\n",
    "                type='adam',\n",
    "                learning_rate=1e-3\n",
    "            )\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "        if memory is None:\n",
    "            memory = dict(\n",
    "                type='replay',\n",
    "                capacity=100000\n",
    "            )\n",
    "        else:\n",
    "            self.memory = memory\n",
    "\n",
    "        self.network_spec = network_spec\n",
    "        self.device = device\n",
    "        self.session_config = session_config\n",
    "        self.scope = scope\n",
    "        self.saver_spec = saver_spec\n",
    "        self.summary_spec = summary_spec\n",
    "        self.distributed_spec = distributed_spec\n",
    "        self.discount = discount\n",
    "        self.variable_noise = variable_noise\n",
    "        self.states_preprocessing_spec = states_preprocessing_spec\n",
    "        self.explorations_spec = explorations_spec\n",
    "        self.reward_preprocessing_spec = reward_preprocessing_spec\n",
    "        self.distributions_spec = distributions_spec\n",
    "        self.entropy_regularization = entropy_regularization\n",
    "        self.target_sync_frequency = target_sync_frequency\n",
    "        self.target_update_weight = target_update_weight\n",
    "        self.double_q_model = double_q_model\n",
    "        self.huber_loss = huber_loss\n",
    "\n",
    "        super(DQNAgent, self).__init__(\n",
    "            states_spec=states_spec,\n",
    "            actions_spec=actions_spec,\n",
    "            batched_observe=batched_observe,\n",
    "            batch_size=batch_size,\n",
    "            memory=memory,\n",
    "            first_update=first_update,\n",
    "            update_frequency=update_frequency,\n",
    "            repeat_update=repeat_update\n",
    "        )\n",
    "\n",
    "    def initialize_model(self):\n",
    "        return QModel(\n",
    "            states_spec=self.states_spec,\n",
    "            actions_spec=self.actions_spec,\n",
    "            network_spec=self.network_spec,\n",
    "            device=self.device,\n",
    "            session_config=self.session_config,\n",
    "            scope=self.scope,\n",
    "            saver_spec=self.saver_spec,\n",
    "            summary_spec=self.summary_spec,\n",
    "            distributed_spec=self.distributed_spec,\n",
    "            optimizer=self.optimizer,\n",
    "            discount=self.discount,\n",
    "            variable_noise=self.variable_noise,\n",
    "            states_preprocessing_spec=self.states_preprocessing_spec,\n",
    "            explorations_spec=self.explorations_spec,\n",
    "            reward_preprocessing_spec=self.reward_preprocessing_spec,\n",
    "            distributions_spec=self.distributions_spec,\n",
    "            entropy_regularization=self.entropy_regularization,\n",
    "            target_sync_frequency=self.target_sync_frequency,\n",
    "            target_update_weight=self.target_update_weight,\n",
    "            double_q_model=self.double_q_model,\n",
    "            huber_loss=self.huber_loss,\n",
    "            # TEMP: Random sampling fix\n",
    "            random_sampling_fix=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestClass(object):\n",
    "    capacity = 100\n",
    "    index = 120\n",
    "    beta = .001\n",
    "    def _sample_index(self, shape, maximum, length, start_index):\n",
    "        assert maximum > length\n",
    "        num_cand = maximum - length + 1\n",
    "        samples = (np.random.geometric(self.beta, shape) - 1) % num_cand\n",
    "        samples = (start_index - samples) % self.capacity\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TestClass()\n",
    "samples = test._sample_index(1000, 99, 10, 98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 8, 5, 5, 2, 4, 5, 7, 8, 2, 2, 1, 9, 8, 3, 0, 1, 5, 1, 2, 0, 8, 6,\n",
       "       4, 5, 6, 1, 4, 8, 0, 0, 9, 7, 6, 4, 4, 4, 8, 1, 7, 8, 2, 0, 7, 6, 9,\n",
       "       3, 2, 7, 1, 5, 7, 4, 2, 1, 1, 3, 3, 7, 9, 4, 2, 5, 8, 6, 4, 2, 0, 2,\n",
       "       1, 8, 9, 8, 8, 5, 2, 6, 3, 6, 2, 5, 7, 6, 0, 5, 0, 8, 2, 4, 8, 9, 4,\n",
       "       0, 4, 4, 9, 6, 1, 9, 2, 1, 9, 3, 5, 8, 3, 2, 1, 7, 1, 2, 4, 5, 9, 7,\n",
       "       3, 9, 7, 2, 1, 1, 3, 5, 8, 1, 5, 6, 6, 0, 8, 2, 4, 9, 4, 2, 0, 7, 6,\n",
       "       8, 4, 5, 5, 5, 7, 0, 0, 6, 6, 6, 9, 8, 5, 0, 5, 0, 1, 6, 3, 1, 4, 4,\n",
       "       2, 9, 4, 2, 8, 4, 7, 9, 6, 1, 9, 9, 3, 9, 9, 8, 5, 7, 6, 3, 1, 6, 8,\n",
       "       7, 1, 9, 3, 7, 7, 5, 7, 1, 9, 8, 7, 9, 5, 6, 8, 1, 6, 7, 4, 3, 6, 7,\n",
       "       5, 5, 5, 3, 4, 1, 2, 4, 3, 6, 9, 9, 1, 6, 6, 6, 3, 7, 3, 2, 6, 8, 8,\n",
       "       7, 9, 4, 6, 9, 0, 7, 8, 8, 9, 0, 0, 2, 5, 8, 6, 6, 1, 8, 3, 0, 2, 5,\n",
       "       8, 5, 5, 5, 1, 5, 6, 5, 0, 0, 9, 3, 1, 6, 8, 2, 0, 7, 0, 4, 7, 5, 2,\n",
       "       3, 0, 9, 2, 6, 1, 1, 8, 7, 6, 5, 0, 5, 3, 6, 6, 5, 8, 7, 5, 3, 2, 5,\n",
       "       4, 2, 2, 8, 6, 5, 5, 6, 3, 8, 3, 7, 9, 3, 3, 7, 7, 3, 1, 2, 7, 0, 1,\n",
       "       3, 7, 1, 4, 0, 8, 9, 5, 1, 5, 3, 4, 1, 7, 1, 5, 8, 8, 8, 3, 5, 0, 1,\n",
       "       3, 7, 2, 1, 5, 8, 2, 0, 1, 5, 0, 8, 2, 6, 8, 4, 7, 2, 5, 2, 7, 8, 1,\n",
       "       5, 3, 2, 0, 1, 2, 0, 0, 2, 8, 7, 0, 1, 3, 8, 0, 0, 5, 7, 8, 7, 0, 5,\n",
       "       1, 7, 1, 2, 8, 3, 8, 3, 7, 4, 7, 5, 6, 1, 5, 9, 0, 6, 5, 6, 3, 1, 9,\n",
       "       5, 2, 4, 5, 2, 0, 8, 6, 9, 0, 0, 1, 8, 8, 0, 4, 9, 8, 3, 9, 4, 1, 4,\n",
       "       6, 3, 4, 0, 9, 0, 1, 9, 4, 8, 1, 8, 2, 3, 2, 7, 7, 4, 1, 8, 5, 8, 7,\n",
       "       3, 1, 3, 4, 7, 0, 9, 4, 9, 4, 7, 3, 7, 3, 7, 2, 0, 1, 2, 6, 0, 1, 6,\n",
       "       3, 1, 5, 6, 7, 5, 4, 6, 3, 3, 1, 8, 2, 6, 5, 0, 4, 3, 2, 9, 2, 3, 8,\n",
       "       4, 4, 2, 3, 1, 7, 6, 8, 0, 8, 9, 5, 6, 0, 5, 4, 0, 6, 5, 9, 4, 3, 3,\n",
       "       6, 8, 0, 1, 5, 7, 2, 0, 5, 4, 8, 4, 3, 8, 4, 2, 3, 5, 6, 1, 3, 2, 8,\n",
       "       5, 0, 0, 7, 1, 2, 0, 1, 5, 1, 3, 7, 0, 9, 2, 4, 2, 0, 6, 9, 5, 4, 9,\n",
       "       0, 7, 9, 3, 1, 6, 5, 8, 0, 9, 4, 8, 8, 1, 7, 7, 2, 2, 6, 1, 6, 8, 5,\n",
       "       2, 8, 9, 7, 2, 9, 3, 2, 9, 7, 5, 4, 8, 5, 3, 7, 6, 0, 8, 2, 8, 1, 3,\n",
       "       1, 6, 1, 3, 4, 3, 4, 6, 4, 8, 2, 2, 1, 8, 9, 3, 1, 9, 0, 7, 2, 1, 4,\n",
       "       2, 5, 2, 4, 1, 6, 0, 9, 1, 1, 1, 8, 1, 6, 4, 6, 9, 4, 3, 7, 9, 8, 5,\n",
       "       3, 2, 2, 5, 3, 1, 5, 4, 2, 4, 3, 7, 2, 5, 6, 8, 0, 4, 2, 7, 9, 7, 2,\n",
       "       9, 9, 7, 4, 8, 0, 4, 1, 3, 4, 9, 9, 7, 4, 0, 6, 0, 6, 2, 0, 8, 2, 2,\n",
       "       6, 4, 7, 1, 3, 5, 0, 8, 7, 2, 6, 5, 4, 7, 4, 5, 6, 3, 6, 5, 4, 4, 7,\n",
       "       5, 6, 3, 3, 3, 9, 8, 4, 2, 7, 1, 2, 8, 5, 5, 8, 2, 3, 7, 5, 8, 0, 0,\n",
       "       7, 9, 6, 6, 6, 5, 1, 2, 1, 6, 5, 9, 8, 1, 5, 4, 8, 2, 6, 9, 5, 9, 0,\n",
       "       6, 6, 7, 2, 0, 5, 3, 0, 7, 8, 3, 2, 6, 4, 4, 0, 4, 7, 6, 3, 4, 7, 0,\n",
       "       5, 4, 8, 6, 4, 7, 5, 3, 4, 3, 1, 6, 1, 1, 7, 0, 2, 7, 3, 3, 4, 5, 0,\n",
       "       0, 4, 0, 0, 7, 4, 4, 4, 0, 2, 0, 4, 7, 1, 9, 9, 8, 4, 2, 7, 8, 3, 2,\n",
       "       7, 1, 6, 6, 5, 9, 8, 9, 0, 6, 6, 4, 8, 9, 3, 8, 6, 9, 3, 0, 6, 5, 9,\n",
       "       4, 9, 4, 6, 5, 4, 6, 5, 7, 6, 1, 9, 1, 0, 5, 0, 8, 0, 9, 3, 1, 4, 1,\n",
       "       4, 4, 5, 4, 5, 6, 7, 4, 9, 0, 9, 1, 9, 4, 2, 1, 2, 6, 7, 7, 1, 4, 7,\n",
       "       7, 1, 3, 3, 7, 0, 8, 6, 8, 9, 3, 6, 9, 5, 0, 5, 3, 2, 0, 0, 5, 9, 4,\n",
       "       6, 7, 9, 5, 7, 9, 4, 6, 3, 3, 8, 7, 4, 5, 9, 6, 6, 1, 4, 7, 4, 3, 1,\n",
       "       9, 9, 2, 1, 8, 5, 6, 5, 1, 1, 3, 5, 0, 1, 5, 4, 1, 3, 0, 4, 8, 0, 3,\n",
       "       4, 0, 5, 7, 0, 3, 1, 9, 5, 8, 8])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.\n",
      "  95.  96.  97.  98.  99.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.\n",
      "  10.  11.  12.  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.\n",
      "  25.  26.  27.  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.\n",
      "  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.\n",
      "  55.  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.]\n"
     ]
    }
   ],
   "source": [
    "x = TestClass()\n",
    "priority = x.get_priority()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb926dec780>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPzhPUfp9fsxgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb92ccccac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.18480304e-05,   1.96632273e-05,   1.76969046e-05,\n",
       "         1.59272141e-05,   1.43344927e-05,   1.29010435e-05,\n",
       "         1.16109391e-05,   1.04498452e-05,   9.40486068e-06,\n",
       "         8.46437461e-06,   7.61793715e-06,   6.85614343e-06,\n",
       "         6.17052909e-06,   5.55347618e-06,   4.99812856e-06,\n",
       "         4.49831571e-06,   4.04848414e-06,   3.64363572e-06,\n",
       "         3.27927215e-06,   2.95134493e-06,   1.00002656e-01,\n",
       "         9.00023906e-02,   8.10021515e-02,   7.29019364e-02,\n",
       "         6.56117427e-02,   5.90505685e-02,   5.31455116e-02,\n",
       "         4.78309605e-02,   4.30478644e-02,   3.87430780e-02,\n",
       "         3.48687702e-02,   3.13818932e-02,   2.82437038e-02,\n",
       "         2.54193335e-02,   2.28774001e-02,   2.05896601e-02,\n",
       "         1.85306941e-02,   1.66776247e-02,   1.50098622e-02,\n",
       "         1.35088760e-02,   1.21579884e-02,   1.09421896e-02,\n",
       "         9.84797060e-03,   8.86317354e-03,   7.97685618e-03,\n",
       "         7.17917057e-03,   6.46125351e-03,   5.81512816e-03,\n",
       "         5.23361534e-03,   4.71025381e-03,   4.23922843e-03,\n",
       "         3.81530558e-03,   3.43377503e-03,   3.09039752e-03,\n",
       "         2.78135777e-03,   2.50322199e-03,   2.25289979e-03,\n",
       "         2.02760982e-03,   1.82484883e-03,   1.64236395e-03,\n",
       "         1.47812756e-03,   1.33031480e-03,   1.19728332e-03,\n",
       "         1.07755499e-03,   9.69799489e-04,   8.72819540e-04,\n",
       "         7.85537586e-04,   7.06983827e-04,   6.36285445e-04,\n",
       "         5.72656900e-04,   5.15391210e-04,   4.63852089e-04,\n",
       "         4.17466880e-04,   3.75720192e-04,   3.38148173e-04,\n",
       "         3.04333356e-04,   2.73900020e-04,   2.46510018e-04,\n",
       "         2.21859016e-04,   1.99673115e-04,   1.79705803e-04,\n",
       "         1.61735223e-04,   1.45561701e-04,   1.31005531e-04,\n",
       "         1.17904977e-04,   1.06114480e-04,   9.55030318e-05,\n",
       "         8.59527286e-05,   7.73574557e-05,   6.96217102e-05,\n",
       "         6.26595391e-05,   5.63935852e-05,   5.07542267e-05,\n",
       "         4.56788040e-05,   4.11109236e-05,   3.69998313e-05,\n",
       "         3.32998481e-05,   2.99698633e-05,   2.69728770e-05,\n",
       "         2.42755893e-05])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\"\\\n",
    "          .format(ep=r.episode, ts=r.episode_timestep, reward=r.episode_rewards[-1]))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Configuration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-69a81a94e4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m config = Configuration(\n\u001b[0m\u001b[1;32m      5\u001b[0m     memory=dict(\n\u001b[1;32m      6\u001b[0m         \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replay'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Configuration' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorforce.agents import agent\n",
    "\n",
    "\n",
    "config = Configuration(\n",
    "    memory=dict(\n",
    "        type='replay',\n",
    "        capacity=1000\n",
    "    ),\n",
    "    batch_size=8,\n",
    "    first_update=100,\n",
    "    target_sync_frequency=10\n",
    ")\n",
    "\n",
    "T = 10\n",
    "num_stock = 5\n",
    "\n",
    "# Network is an ordered list of layers\n",
    "network_spec = [dict(type='dense', size=32), dict(type='dense', size=32)]\n",
    "\n",
    "# Define a state\n",
    "states = dict(shape=(T, num_stock), type='float')\n",
    "\n",
    "# Define an action\n",
    "actions = dict(type='float', shape=(num_stock,))\n",
    "\n",
    "agent = DQNAgent(\n",
    "    states_spec=states,\n",
    "    actions_spec=actions,\n",
    "    network_spec=network_spec,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "class Agent(agent.Agent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(12).reshape((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 6, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.take(1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorforce.agents import TRPOAgent\n",
    "\n",
    "agent = TRPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=[\n",
    "        dict(type='dense', size=64),\n",
    "        dict(type='dense', size=64)\n",
    "    ],\n",
    "    batch_size=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Do not use tf.reset_default_graph() to clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-249-469a83017b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tomoaki/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mreset_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4163\u001b[0m   \"\"\"\n\u001b[1;32m   4164\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cleared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4165\u001b[0;31m     raise AssertionError(\"Do not use tf.reset_default_graph() to clear \"\n\u001b[0m\u001b[1;32m   4166\u001b[0m                          \u001b[0;34m\"nested graphs. If you need a cleared graph, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4167\u001b[0m                          \"exit the nesting and create a new graph.\")\n",
      "\u001b[0;31mAssertionError\u001b[0m: Do not use tf.reset_default_graph() to clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph."
     ]
    }
   ],
   "source": [
    "agent = PPOAgent(\n",
    "    states_spec=env.states,\n",
    "    actions_spec=env.actions,\n",
    "    network_spec=network_spec,\n",
    "    batch_size=4096,\n",
    "    # BatchAgent\n",
    "    keep_last_timestep=True,\n",
    "    # PPOAgent\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-3\n",
    "    ),\n",
    "    optimization_steps=10,\n",
    "    # Model\n",
    "    scope='ppo',\n",
    "    discount=0.99,\n",
    "    # DistributionModel\n",
    "    distributions_spec=None,\n",
    "    entropy_regularization=0.01,\n",
    "    # PGModel\n",
    "    baseline_mode=None,\n",
    "    baseline=None,\n",
    "    baseline_optimizer=None,\n",
    "    gae_lambda=None,\n",
    "    # PGLRModel\n",
    "    likelihood_ratio_clipping=0.2,\n",
    "    summary_spec=None,\n",
    "    distributed_spec=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_value': 1.0, 'min_value': 0.0, 'shape': (10,), 'type': float}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_actions': 2, 'type': 'int'}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape': (4,), 'type': 'float'}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628.02"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[\"2017-09-22\"][\"Open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apr 28, 2013'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-1][\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = pd.read_csv(\"data/Top100Cryptos/100 List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2013-04-28'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(time_index[0], time_index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2013-04-28'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_data = []\n",
    "for name in data.keys():\n",
    "    num_data.append(data[name].shape[0])\n",
    "name_list = np.array(list(data.keys()))[np.argsort(num_data)[::-1]]\n",
    "big_names = name_list[:10]\n",
    "\n",
    "data_ = dict()\n",
    "for name in big_names:\n",
    "    data_[name] = data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ = dict()\n",
    "for name in big_names:\n",
    "    data_[name] = data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_index = set()\n",
    "\n",
    "for key, val in data_.items():\n",
    "    time_index = time_index.union(set(val[\"Date\"].values))\n",
    "time_index = list(time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time(t):\n",
    "    m = {\n",
    "        'Jan' : \"01\",\n",
    "        'Feb' : \"02\",\n",
    "        'Mar' : \"03\",\n",
    "        'Apr' : \"04\",\n",
    "        'May' : \"05\",\n",
    "        'Jun' : \"06\",\n",
    "        'Jul' : \"07\",\n",
    "        'Aug' : \"08\",\n",
    "        'Sep' : \"09\", \n",
    "        'Oct' : \"10\",\n",
    "        'Nov' : \"11\",\n",
    "        'Dec' : \"12\"\n",
    "    }\n",
    "    t_list = t.replace(\",\", \"\").split()\n",
    "    t_list[0] = m[t_list[0]]\n",
    "    return \"-\".join([t_list[2], t_list[0], t_list[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-21498ba7e2fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconvert_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtime_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'time_index' is not defined"
     ]
    }
   ],
   "source": [
    "time_index = sorted([convert_time(t) for t in time_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = data_[big_names[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sep 22, 2017</td>\n",
       "      <td>46.65</td>\n",
       "      <td>49.09</td>\n",
       "      <td>45.19</td>\n",
       "      <td>48.09</td>\n",
       "      <td>218,134,000</td>\n",
       "      <td>2,474,380,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sep 21, 2017</td>\n",
       "      <td>51.66</td>\n",
       "      <td>52.31</td>\n",
       "      <td>45.67</td>\n",
       "      <td>46.61</td>\n",
       "      <td>229,363,000</td>\n",
       "      <td>2,739,370,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sep 20, 2017</td>\n",
       "      <td>52.74</td>\n",
       "      <td>53.83</td>\n",
       "      <td>51.25</td>\n",
       "      <td>51.73</td>\n",
       "      <td>160,252,000</td>\n",
       "      <td>2,795,880,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sep 19, 2017</td>\n",
       "      <td>55.68</td>\n",
       "      <td>55.88</td>\n",
       "      <td>51.05</td>\n",
       "      <td>52.84</td>\n",
       "      <td>286,583,000</td>\n",
       "      <td>2,950,880,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sep 18, 2017</td>\n",
       "      <td>48.53</td>\n",
       "      <td>55.61</td>\n",
       "      <td>48.53</td>\n",
       "      <td>55.53</td>\n",
       "      <td>408,675,000</td>\n",
       "      <td>2,571,070,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sep 17, 2017</td>\n",
       "      <td>48.23</td>\n",
       "      <td>49.88</td>\n",
       "      <td>45.12</td>\n",
       "      <td>48.49</td>\n",
       "      <td>245,262,000</td>\n",
       "      <td>2,554,450,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sep 16, 2017</td>\n",
       "      <td>48.13</td>\n",
       "      <td>52.83</td>\n",
       "      <td>46.46</td>\n",
       "      <td>48.26</td>\n",
       "      <td>562,278,000</td>\n",
       "      <td>2,548,580,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sep 15, 2017</td>\n",
       "      <td>41.69</td>\n",
       "      <td>50.65</td>\n",
       "      <td>32.03</td>\n",
       "      <td>48.21</td>\n",
       "      <td>1,554,340,000</td>\n",
       "      <td>2,206,520,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sep 14, 2017</td>\n",
       "      <td>61.64</td>\n",
       "      <td>62.43</td>\n",
       "      <td>41.25</td>\n",
       "      <td>41.58</td>\n",
       "      <td>749,714,000</td>\n",
       "      <td>3,261,960,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sep 13, 2017</td>\n",
       "      <td>64.32</td>\n",
       "      <td>64.60</td>\n",
       "      <td>58.93</td>\n",
       "      <td>61.73</td>\n",
       "      <td>501,522,000</td>\n",
       "      <td>3,402,820,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sep 12, 2017</td>\n",
       "      <td>66.24</td>\n",
       "      <td>70.25</td>\n",
       "      <td>63.86</td>\n",
       "      <td>64.23</td>\n",
       "      <td>481,218,000</td>\n",
       "      <td>3,503,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sep 11, 2017</td>\n",
       "      <td>61.65</td>\n",
       "      <td>67.23</td>\n",
       "      <td>61.37</td>\n",
       "      <td>66.04</td>\n",
       "      <td>412,076,000</td>\n",
       "      <td>3,259,510,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sep 10, 2017</td>\n",
       "      <td>65.80</td>\n",
       "      <td>65.95</td>\n",
       "      <td>57.06</td>\n",
       "      <td>61.61</td>\n",
       "      <td>529,848,000</td>\n",
       "      <td>3,478,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sep 09, 2017</td>\n",
       "      <td>67.21</td>\n",
       "      <td>69.55</td>\n",
       "      <td>64.01</td>\n",
       "      <td>66.01</td>\n",
       "      <td>467,895,000</td>\n",
       "      <td>3,551,950,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sep 08, 2017</td>\n",
       "      <td>78.45</td>\n",
       "      <td>80.13</td>\n",
       "      <td>63.83</td>\n",
       "      <td>67.79</td>\n",
       "      <td>930,673,000</td>\n",
       "      <td>4,144,630,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sep 07, 2017</td>\n",
       "      <td>79.88</td>\n",
       "      <td>81.06</td>\n",
       "      <td>75.59</td>\n",
       "      <td>78.48</td>\n",
       "      <td>526,829,000</td>\n",
       "      <td>4,218,690,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sep 06, 2017</td>\n",
       "      <td>71.78</td>\n",
       "      <td>80.64</td>\n",
       "      <td>71.51</td>\n",
       "      <td>80.11</td>\n",
       "      <td>830,718,000</td>\n",
       "      <td>3,789,670,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sep 05, 2017</td>\n",
       "      <td>65.25</td>\n",
       "      <td>72.44</td>\n",
       "      <td>59.05</td>\n",
       "      <td>71.29</td>\n",
       "      <td>1,016,090,000</td>\n",
       "      <td>3,444,150,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sep 04, 2017</td>\n",
       "      <td>76.73</td>\n",
       "      <td>76.73</td>\n",
       "      <td>61.24</td>\n",
       "      <td>65.21</td>\n",
       "      <td>922,041,000</td>\n",
       "      <td>4,049,140,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sep 03, 2017</td>\n",
       "      <td>78.98</td>\n",
       "      <td>82.83</td>\n",
       "      <td>73.61</td>\n",
       "      <td>76.84</td>\n",
       "      <td>675,462,000</td>\n",
       "      <td>4,166,600,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sep 02, 2017</td>\n",
       "      <td>85.83</td>\n",
       "      <td>92.07</td>\n",
       "      <td>72.23</td>\n",
       "      <td>79.02</td>\n",
       "      <td>1,363,990,000</td>\n",
       "      <td>4,526,680,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sep 01, 2017</td>\n",
       "      <td>70.86</td>\n",
       "      <td>87.41</td>\n",
       "      <td>70.01</td>\n",
       "      <td>86.04</td>\n",
       "      <td>1,551,740,000</td>\n",
       "      <td>3,735,870,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Aug 31, 2017</td>\n",
       "      <td>64.16</td>\n",
       "      <td>72.28</td>\n",
       "      <td>64.04</td>\n",
       "      <td>71.06</td>\n",
       "      <td>633,147,000</td>\n",
       "      <td>3,381,750,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Aug 30, 2017</td>\n",
       "      <td>63.16</td>\n",
       "      <td>64.21</td>\n",
       "      <td>60.90</td>\n",
       "      <td>64.17</td>\n",
       "      <td>240,309,000</td>\n",
       "      <td>3,328,240,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Aug 29, 2017</td>\n",
       "      <td>62.39</td>\n",
       "      <td>63.29</td>\n",
       "      <td>61.26</td>\n",
       "      <td>63.17</td>\n",
       "      <td>294,799,000</td>\n",
       "      <td>3,286,610,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Aug 28, 2017</td>\n",
       "      <td>61.36</td>\n",
       "      <td>64.92</td>\n",
       "      <td>59.87</td>\n",
       "      <td>62.36</td>\n",
       "      <td>556,637,000</td>\n",
       "      <td>3,231,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Aug 27, 2017</td>\n",
       "      <td>51.80</td>\n",
       "      <td>64.44</td>\n",
       "      <td>51.64</td>\n",
       "      <td>61.16</td>\n",
       "      <td>890,546,000</td>\n",
       "      <td>2,726,990,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Aug 26, 2017</td>\n",
       "      <td>51.17</td>\n",
       "      <td>52.26</td>\n",
       "      <td>50.45</td>\n",
       "      <td>51.75</td>\n",
       "      <td>126,024,000</td>\n",
       "      <td>2,692,560,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Aug 25, 2017</td>\n",
       "      <td>50.17</td>\n",
       "      <td>52.13</td>\n",
       "      <td>49.84</td>\n",
       "      <td>51.18</td>\n",
       "      <td>157,619,000</td>\n",
       "      <td>2,639,490,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Aug 24, 2017</td>\n",
       "      <td>52.82</td>\n",
       "      <td>53.28</td>\n",
       "      <td>50.19</td>\n",
       "      <td>50.19</td>\n",
       "      <td>391,342,000</td>\n",
       "      <td>2,778,190,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>May 27, 2013</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-</td>\n",
       "      <td>58,130,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>May 26, 2013</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.25</td>\n",
       "      <td>-</td>\n",
       "      <td>56,123,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>May 25, 2013</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-</td>\n",
       "      <td>57,165,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>May 24, 2013</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3.18</td>\n",
       "      <td>-</td>\n",
       "      <td>56,747,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>May 23, 2013</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.18</td>\n",
       "      <td>-</td>\n",
       "      <td>55,727,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>May 22, 2013</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-</td>\n",
       "      <td>55,165,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>May 21, 2013</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3.09</td>\n",
       "      <td>-</td>\n",
       "      <td>56,887,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>May 20, 2013</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.18</td>\n",
       "      <td>-</td>\n",
       "      <td>58,962,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>May 19, 2013</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-</td>\n",
       "      <td>58,645,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>May 18, 2013</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.29</td>\n",
       "      <td>-</td>\n",
       "      <td>56,721,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>May 17, 2013</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.34</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.19</td>\n",
       "      <td>-</td>\n",
       "      <td>51,195,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>May 16, 2013</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.97</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.92</td>\n",
       "      <td>-</td>\n",
       "      <td>51,790,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>May 15, 2013</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.94</td>\n",
       "      <td>-</td>\n",
       "      <td>49,842,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>May 14, 2013</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-</td>\n",
       "      <td>57,812,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>May 13, 2013</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.28</td>\n",
       "      <td>-</td>\n",
       "      <td>57,363,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>May 12, 2013</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.27</td>\n",
       "      <td>-</td>\n",
       "      <td>59,052,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>May 11, 2013</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.35</td>\n",
       "      <td>-</td>\n",
       "      <td>60,658,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>May 10, 2013</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.44</td>\n",
       "      <td>-</td>\n",
       "      <td>59,588,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>May 09, 2013</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.42</td>\n",
       "      <td>-</td>\n",
       "      <td>59,337,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>May 08, 2013</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.41</td>\n",
       "      <td>-</td>\n",
       "      <td>57,196,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>May 07, 2013</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.41</td>\n",
       "      <td>2.94</td>\n",
       "      <td>3.33</td>\n",
       "      <td>-</td>\n",
       "      <td>58,588,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>May 06, 2013</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.37</td>\n",
       "      <td>-</td>\n",
       "      <td>62,356,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>May 05, 2013</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.59</td>\n",
       "      <td>-</td>\n",
       "      <td>60,525,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>May 04, 2013</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.64</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.48</td>\n",
       "      <td>-</td>\n",
       "      <td>52,476,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>May 03, 2013</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.04</td>\n",
       "      <td>-</td>\n",
       "      <td>58,607,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>May 02, 2013</td>\n",
       "      <td>3.78</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.01</td>\n",
       "      <td>3.37</td>\n",
       "      <td>-</td>\n",
       "      <td>65,242,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>May 01, 2013</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-</td>\n",
       "      <td>73,901,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>Apr 30, 2013</td>\n",
       "      <td>4.40</td>\n",
       "      <td>4.57</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.30</td>\n",
       "      <td>-</td>\n",
       "      <td>75,726,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>Apr 29, 2013</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.57</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.38</td>\n",
       "      <td>-</td>\n",
       "      <td>74,952,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>Apr 28, 2013</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.40</td>\n",
       "      <td>4.18</td>\n",
       "      <td>4.35</td>\n",
       "      <td>-</td>\n",
       "      <td>73,773,400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1609 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date   Open   High    Low  Close         Volume     Market Cap\n",
       "0     Sep 22, 2017  46.65  49.09  45.19  48.09    218,134,000  2,474,380,000\n",
       "1     Sep 21, 2017  51.66  52.31  45.67  46.61    229,363,000  2,739,370,000\n",
       "2     Sep 20, 2017  52.74  53.83  51.25  51.73    160,252,000  2,795,880,000\n",
       "3     Sep 19, 2017  55.68  55.88  51.05  52.84    286,583,000  2,950,880,000\n",
       "4     Sep 18, 2017  48.53  55.61  48.53  55.53    408,675,000  2,571,070,000\n",
       "5     Sep 17, 2017  48.23  49.88  45.12  48.49    245,262,000  2,554,450,000\n",
       "6     Sep 16, 2017  48.13  52.83  46.46  48.26    562,278,000  2,548,580,000\n",
       "7     Sep 15, 2017  41.69  50.65  32.03  48.21  1,554,340,000  2,206,520,000\n",
       "8     Sep 14, 2017  61.64  62.43  41.25  41.58    749,714,000  3,261,960,000\n",
       "9     Sep 13, 2017  64.32  64.60  58.93  61.73    501,522,000  3,402,820,000\n",
       "10    Sep 12, 2017  66.24  70.25  63.86  64.23    481,218,000  3,503,300,000\n",
       "11    Sep 11, 2017  61.65  67.23  61.37  66.04    412,076,000  3,259,510,000\n",
       "12    Sep 10, 2017  65.80  65.95  57.06  61.61    529,848,000  3,478,400,000\n",
       "13    Sep 09, 2017  67.21  69.55  64.01  66.01    467,895,000  3,551,950,000\n",
       "14    Sep 08, 2017  78.45  80.13  63.83  67.79    930,673,000  4,144,630,000\n",
       "15    Sep 07, 2017  79.88  81.06  75.59  78.48    526,829,000  4,218,690,000\n",
       "16    Sep 06, 2017  71.78  80.64  71.51  80.11    830,718,000  3,789,670,000\n",
       "17    Sep 05, 2017  65.25  72.44  59.05  71.29  1,016,090,000  3,444,150,000\n",
       "18    Sep 04, 2017  76.73  76.73  61.24  65.21    922,041,000  4,049,140,000\n",
       "19    Sep 03, 2017  78.98  82.83  73.61  76.84    675,462,000  4,166,600,000\n",
       "20    Sep 02, 2017  85.83  92.07  72.23  79.02  1,363,990,000  4,526,680,000\n",
       "21    Sep 01, 2017  70.86  87.41  70.01  86.04  1,551,740,000  3,735,870,000\n",
       "22    Aug 31, 2017  64.16  72.28  64.04  71.06    633,147,000  3,381,750,000\n",
       "23    Aug 30, 2017  63.16  64.21  60.90  64.17    240,309,000  3,328,240,000\n",
       "24    Aug 29, 2017  62.39  63.29  61.26  63.17    294,799,000  3,286,610,000\n",
       "25    Aug 28, 2017  61.36  64.92  59.87  62.36    556,637,000  3,231,200,000\n",
       "26    Aug 27, 2017  51.80  64.44  51.64  61.16    890,546,000  2,726,990,000\n",
       "27    Aug 26, 2017  51.17  52.26  50.45  51.75    126,024,000  2,692,560,000\n",
       "28    Aug 25, 2017  50.17  52.13  49.84  51.18    157,619,000  2,639,490,000\n",
       "29    Aug 24, 2017  52.82  53.28  50.19  50.19    391,342,000  2,778,190,000\n",
       "...            ...    ...    ...    ...    ...            ...            ...\n",
       "1579  May 27, 2013   3.23   3.26   3.02   3.10              -     58,130,800\n",
       "1580  May 26, 2013   3.12   3.33   3.09   3.25              -     56,123,900\n",
       "1581  May 25, 2013   3.19   3.20   3.05   3.12              -     57,165,800\n",
       "1582  May 24, 2013   3.17   3.19   3.04   3.18              -     56,747,200\n",
       "1583  May 23, 2013   3.12   3.19   3.09   3.18              -     55,727,000\n",
       "1584  May 22, 2013   3.09   3.14   3.03   3.12              -     55,165,300\n",
       "1585  May 21, 2013   3.19   3.22   3.04   3.09              -     56,887,600\n",
       "1586  May 20, 2013   3.31   3.32   3.15   3.18              -     58,962,900\n",
       "1587  May 19, 2013   3.30   3.41   3.19   3.30              -     58,645,400\n",
       "1588  May 18, 2013   3.20   3.36   3.13   3.29              -     56,721,100\n",
       "1589  May 17, 2013   2.89   3.34   2.87   3.19              -     51,195,900\n",
       "1590  May 16, 2013   2.93   2.97   2.69   2.92              -     51,790,900\n",
       "1591  May 15, 2013   2.82   3.04   2.64   2.94              -     49,842,500\n",
       "1592  May 14, 2013   3.28   3.33   2.78   2.82              -     57,812,000\n",
       "1593  May 13, 2013   3.26   3.37   3.20   3.28              -     57,363,200\n",
       "1594  May 12, 2013   3.36   3.42   3.22   3.27              -     59,052,900\n",
       "1595  May 11, 2013   3.46   3.48   3.30   3.35              -     60,658,000\n",
       "1596  May 10, 2013   3.41   3.63   3.37   3.44              -     59,588,000\n",
       "1597  May 09, 2013   3.40   3.44   3.29   3.42              -     59,337,100\n",
       "1598  May 08, 2013   3.28   3.49   3.28   3.41              -     57,196,300\n",
       "1599  May 07, 2013   3.37   3.41   2.94   3.33              -     58,588,300\n",
       "1600  May 06, 2013   3.59   3.78   3.12   3.37              -     62,356,000\n",
       "1601  May 05, 2013   3.49   3.69   3.35   3.59              -     60,525,000\n",
       "1602  May 04, 2013   3.03   3.64   2.90   3.48              -     52,476,400\n",
       "1603  May 03, 2013   3.39   3.45   2.40   3.04              -     58,607,400\n",
       "1604  May 02, 2013   3.78   4.04   3.01   3.37              -     65,242,700\n",
       "1605  May 01, 2013   4.29   4.36   3.52   3.80              -     73,901,200\n",
       "1606  Apr 30, 2013   4.40   4.57   4.17   4.30              -     75,726,800\n",
       "1607  Apr 29, 2013   4.37   4.57   4.23   4.38              -     74,952,700\n",
       "1608  Apr 28, 2013   4.30   4.40   4.18   4.35              -     73,773,400\n",
       "\n",
       "[1609 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-02 08:25:52,495] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 10 after 305 timesteps\n",
      "Episode reward: 99.0\n",
      "Average of last 100 rewards: 3.05\n",
      "Finished episode 20 after 479 timesteps\n",
      "Episode reward: 9.0\n",
      "Average of last 100 rewards: 4.79\n",
      "Finished episode 30 after 736 timesteps\n",
      "Episode reward: 52.0\n",
      "Average of last 100 rewards: 7.36\n",
      "Finished episode 40 after 949 timesteps\n",
      "Episode reward: 49.0\n",
      "Average of last 100 rewards: 9.49\n",
      "Finished episode 50 after 1204 timesteps\n",
      "Episode reward: 44.0\n",
      "Average of last 100 rewards: 12.04\n",
      "Finished episode 60 after 1453 timesteps\n",
      "Episode reward: 21.0\n",
      "Average of last 100 rewards: 14.53\n",
      "Finished episode 70 after 1675 timesteps\n",
      "Episode reward: 16.0\n",
      "Average of last 100 rewards: 16.75\n",
      "Finished episode 80 after 1888 timesteps\n",
      "Episode reward: 27.0\n",
      "Average of last 100 rewards: 18.88\n",
      "Finished episode 90 after 2079 timesteps\n",
      "Episode reward: 9.0\n",
      "Average of last 100 rewards: 20.79\n",
      "Finished episode 100 after 2340 timesteps\n",
      "Episode reward: 18.0\n",
      "Average of last 100 rewards: 23.4\n",
      "Finished episode 110 after 2564 timesteps\n",
      "Episode reward: 17.0\n",
      "Average of last 100 rewards: 22.59\n",
      "Finished episode 120 after 2804 timesteps\n",
      "Episode reward: 15.0\n",
      "Average of last 100 rewards: 23.25\n",
      "Finished episode 130 after 2956 timesteps\n",
      "Episode reward: 21.0\n",
      "Average of last 100 rewards: 22.2\n",
      "Finished episode 140 after 3234 timesteps\n",
      "Episode reward: 18.0\n",
      "Average of last 100 rewards: 22.85\n",
      "Finished episode 150 after 3481 timesteps\n",
      "Episode reward: 12.0\n",
      "Average of last 100 rewards: 22.77\n",
      "Finished episode 160 after 3692 timesteps\n",
      "Episode reward: 22.0\n",
      "Average of last 100 rewards: 22.39\n",
      "Finished episode 170 after 3997 timesteps\n",
      "Episode reward: 18.0\n",
      "Average of last 100 rewards: 23.22\n",
      "Finished episode 180 after 4185 timesteps\n",
      "Episode reward: 18.0\n",
      "Average of last 100 rewards: 22.97\n",
      "Finished episode 190 after 4396 timesteps\n",
      "Episode reward: 23.0\n",
      "Average of last 100 rewards: 23.17\n",
      "Finished episode 200 after 4616 timesteps\n",
      "Episode reward: 13.0\n",
      "Average of last 100 rewards: 22.76\n",
      "Finished episode 210 after 4824 timesteps\n",
      "Episode reward: 24.0\n",
      "Average of last 100 rewards: 22.6\n",
      "Finished episode 220 after 5019 timesteps\n",
      "Episode reward: 14.0\n",
      "Average of last 100 rewards: 22.15\n",
      "Finished episode 230 after 5285 timesteps\n",
      "Episode reward: 21.0\n",
      "Average of last 100 rewards: 23.29\n",
      "Finished episode 240 after 5549 timesteps\n",
      "Episode reward: 22.0\n",
      "Average of last 100 rewards: 23.15\n",
      "Finished episode 250 after 5722 timesteps\n",
      "Episode reward: 9.0\n",
      "Average of last 100 rewards: 22.41\n",
      "Finished episode 260 after 5979 timesteps\n",
      "Episode reward: 29.0\n",
      "Average of last 100 rewards: 22.87\n",
      "Finished episode 270 after 6169 timesteps\n",
      "Episode reward: 10.0\n",
      "Average of last 100 rewards: 21.72\n",
      "Finished episode 280 after 6445 timesteps\n",
      "Episode reward: 32.0\n",
      "Average of last 100 rewards: 22.6\n",
      "Finished episode 290 after 6699 timesteps\n",
      "Episode reward: 22.0\n",
      "Average of last 100 rewards: 23.03\n",
      "Finished episode 300 after 6882 timesteps\n",
      "Episode reward: 13.0\n",
      "Average of last 100 rewards: 22.66\n",
      "Finished episode 310 after 7111 timesteps\n",
      "Episode reward: 14.0\n",
      "Average of last 100 rewards: 22.87\n",
      "Finished episode 320 after 7298 timesteps\n",
      "Episode reward: 15.0\n",
      "Average of last 100 rewards: 22.79\n",
      "Finished episode 330 after 7550 timesteps\n",
      "Episode reward: 23.0\n",
      "Average of last 100 rewards: 22.65\n",
      "Finished episode 340 after 7796 timesteps\n",
      "Episode reward: 37.0\n",
      "Average of last 100 rewards: 22.47\n",
      "Finished episode 350 after 7978 timesteps\n",
      "Episode reward: 10.0\n",
      "Average of last 100 rewards: 22.56\n",
      "Finished episode 360 after 8224 timesteps\n",
      "Episode reward: 42.0\n",
      "Average of last 100 rewards: 22.45\n",
      "Finished episode 370 after 8435 timesteps\n",
      "Episode reward: 21.0\n",
      "Average of last 100 rewards: 22.66\n",
      "Finished episode 380 after 8663 timesteps\n",
      "Episode reward: 14.0\n",
      "Average of last 100 rewards: 22.18\n",
      "Finished episode 390 after 8870 timesteps\n",
      "Episode reward: 15.0\n",
      "Average of last 100 rewards: 21.71\n",
      "Finished episode 400 after 9054 timesteps\n",
      "Episode reward: 15.0\n",
      "Average of last 100 rewards: 21.72\n",
      "Finished episode 410 after 9284 timesteps\n",
      "Episode reward: 41.0\n",
      "Average of last 100 rewards: 21.73\n",
      "Finished episode 420 after 9521 timesteps\n",
      "Episode reward: 22.0\n",
      "Average of last 100 rewards: 22.23\n",
      "Finished episode 430 after 9733 timesteps\n",
      "Episode reward: 10.0\n",
      "Average of last 100 rewards: 21.83\n",
      "Finished episode 440 after 9927 timesteps\n",
      "Episode reward: 14.0\n",
      "Average of last 100 rewards: 21.31\n"
     ]
    }
   ],
   "source": [
    "from tensorforce import TensorForceError\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "agent_spec = {\n",
    "    \"type\": \"dqn_agent\",\n",
    "    \"batch_size\": 64,\n",
    "    \"memory\": {\n",
    "        \"type\": \"replay\",\n",
    "        \"capacity\": 10000\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"learning_rate\": 1e-3\n",
    "    },\n",
    "\n",
    "    \"discount\": 0.97,\n",
    "\n",
    "    \"exploration\": {\n",
    "        \"type\": \"epsilon_decay\",\n",
    "        \"initial_epsilon\": 1.0,\n",
    "        \"final_epsilon\": 0.1,\n",
    "        \"timesteps\": 1e6\n",
    "    }\n",
    "}\n",
    "\n",
    "gym_id = 'CartPole-v0'\n",
    "max_episodes = 10000\n",
    "max_timesteps = 1000\n",
    "\n",
    "environment = OpenAIGym(gym_id)\n",
    "network_spec = [\n",
    "        dict(type='dense', size=32, activation='tanh'),\n",
    "        dict(type='dense', size=32, activation='tanh')\n",
    "]\n",
    "\n",
    "agent = Agent.from_spec(\n",
    "    spec=agent_spec,\n",
    "    kwargs=dict(\n",
    "        states_spec=environment.states,\n",
    "        actions_spec=environment.actions,\n",
    "        network_spec=network_spec\n",
    "    )\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    repeat_actions=1\n",
    ")\n",
    "\n",
    "report_episodes = 10\n",
    "\n",
    "def episode_finished(r):\n",
    "    if r.episode % report_episodes == 0:\n",
    "        print(\"Finished episode {ep} after {ts} timesteps\".format(ep=r.episode, ts=r.timestep))\n",
    "        print(\"Episode reward: {}\".format(r.episode_rewards[-1]))\n",
    "        print(\"Average of last 100 rewards: {}\".format(sum(r.episode_rewards[-100:]) / 100))\n",
    "    return True\n",
    "\n",
    "runner.run(max_episodes, max_timesteps, episode_finished=episode_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorforce."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
