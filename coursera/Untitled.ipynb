{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                                           ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.posision]  = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADWCAYAAADBwHkCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e8a5b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                                     T.Resize(40, interpolation=Image.CUBIC),\n",
    "                                     T.ToTensor()])\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head): Linear(in_features=448, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.sample()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax(dim=1, keepdim=True)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimzie_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions  = memory.sample(BATCH_SIZE)\n",
    "    batch  = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                                                           device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function clamp:\n",
      "\n",
      "clamp(...)\n",
      "    clamp(input, min, max, out=None) -> Tensor\n",
      "    \n",
      "    Clamp all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]` and return\n",
      "    a resulting tensor:\n",
      "    \n",
      "    .. math::\n",
      "        y_i = \\begin{cases}\n",
      "            \\text{min} & \\text{if } x_i < \\text{min} \\\\\n",
      "            x_i & \\text{if } \\text{min} \\leq x_i \\leq \\text{max} \\\\\n",
      "            \\text{max} & \\text{if } x_i > \\text{max}\n",
      "        \\end{cases}\n",
      "    \n",
      "    If :attr:`input` is of type `FloatTensor` or `DoubleTensor`, args :attr:`min`\n",
      "    and :attr:`max` must be real numbers, otherwise they should be integers.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        min (Number): lower-bound of the range to be clamped to\n",
      "        max (Number): upper-bound of the range to be clamped to\n",
      "        out (Tensor, optional): the output tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4)\n",
      "        >>> a\n",
      "        tensor([-1.7120,  0.1734, -0.0478, -0.0922])\n",
      "        >>> torch.clamp(a, min=-0.5, max=0.5)\n",
      "        tensor([-0.5000,  0.1734, -0.0478, -0.0922])\n",
      "    \n",
      "    .. function:: clamp(input, *, min, out=None) -> Tensor\n",
      "    \n",
      "    Clamps all elements in :attr:`input` to be larger or equal :attr:`min`.\n",
      "    \n",
      "    If :attr:`input` is of type `FloatTensor` or `DoubleTensor`, :attr:`value`\n",
      "    should be a real number, otherwise it should be an integer.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        value (Number): minimal value of each element in the output\n",
      "        out (Tensor, optional): the output tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4)\n",
      "        >>> a\n",
      "        tensor([-0.0299, -2.3184,  2.1593, -0.8883])\n",
      "        >>> torch.clamp(a, min=0.5)\n",
      "        tensor([ 0.5000,  0.5000,  2.1593,  0.5000])\n",
      "    \n",
      "    .. function:: clamp(input, *, max, out=None) -> Tensor\n",
      "    \n",
      "    Clamps all elements in :attr:`input` to be smaller or equal :attr:`max`.\n",
      "    \n",
      "    If :attr:`input` is of type `FloatTensor` or `DoubleTensor`, :attr:`value`\n",
      "    should be a real number, otherwise it should be an integer.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        value (Number): maximal value of each element in the output\n",
      "        out (Tensor, optional): the output tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4)\n",
      "        >>> a\n",
      "        tensor([ 0.0753, -0.4702, -0.4599,  0.1899])\n",
      "        >>> torch.clamp(a, max=0.5)\n",
      "        tensor([ 0.0753, -0.4702, -0.4599,  0.1899])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.clamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "max(...)\n",
      "    max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      "    \n",
      "    See :func:`torch.max`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.Tensor.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function max:\n",
      "\n",
      "max(...)\n",
      "    .. function:: max(input) -> Tensor\n",
      "    \n",
      "    Returns the maximum value of all elements in the :attr:`input` tensor.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(1, 3)\n",
      "        >>> a\n",
      "        tensor([[ 0.6763,  0.7445, -2.2369]])\n",
      "        >>> torch.max(a)\n",
      "        tensor(0.7445)\n",
      "    \n",
      "    .. function:: max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)\n",
      "    \n",
      "    Returns the maximum value of each row of the :attr:`input` tensor in the given\n",
      "    dimension :attr:`dim`. The second return value is the index location of each\n",
      "    maximum value found (argmax).\n",
      "    \n",
      "    If :attr:`keepdim` is ``True``, the output tensors are of the same size\n",
      "    as :attr:`input` except in the dimension :attr:`dim` where they are of size 1.\n",
      "    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting\n",
      "    in the output tensors having 1 fewer dimension than :attr:`input`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        dim (int): the dimension to reduce\n",
      "        keepdim (bool): whether the output tensors have :attr:`dim` retained or not\n",
      "        out (tuple, optional): the result tuple of two output tensors (max, max_indices)\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4, 4)\n",
      "        >>> a\n",
      "        tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
      "                [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
      "                [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
      "                [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
      "        >>> torch.max(a, 1)\n",
      "        (tensor([ 0.8475,  1.1949,  1.5717,  1.0036]), tensor([ 3,  0,  0,  1]))\n",
      "    \n",
      "    .. function:: max(input, other, out=None) -> Tensor\n",
      "    \n",
      "    Each element of the tensor :attr:`input` is compared with the corresponding\n",
      "    element of the tensor :attr:`other` and an element-wise maximum is taken.\n",
      "    \n",
      "    The shapes of :attr:`input` and :attr:`other` don't need to match,\n",
      "    but they must be :ref:`broadcastable <broadcasting-semantics>`.\n",
      "    \n",
      "    .. math::\n",
      "        out_i = \\max(tensor_i, other_i)\n",
      "    \n",
      "    .. note:: When the shapes do not match, the shape of the returned output tensor\n",
      "              follows the :ref:`broadcasting rules <broadcasting-semantics>`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        other (Tensor): the second input tensor\n",
      "        out (Tensor, optional): the output tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4)\n",
      "        >>> a\n",
      "        tensor([ 0.2942, -0.7416,  0.2653, -0.1584])\n",
      "        >>> b = torch.randn(4)\n",
      "        >>> b\n",
      "        tensor([ 0.8722, -1.7421, -0.4141, -0.5055])\n",
      "        >>> torch.max(a, b)\n",
      "        tensor([ 0.8722, -0.7416,  0.2653, -0.1584])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 0],\n",
       "        [ 3],\n",
       "        [ 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
